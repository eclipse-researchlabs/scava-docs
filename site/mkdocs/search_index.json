{
    "docs": [
        {
            "location": "/",
            "text": "Scava Documentation\n\n\nThis web site is the main documentation place for the \nEclipse Scava\n project.\n\n\nSCAVA Installation Guide\n\n\nThe SCAVA installation guide provides instructions on how to install and configure the CORSSMINER Platform on a server and how to deploy the Eclipses Plugin in development environment. \n\n\nSCAVA User Guide\n\n\nThe SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.\n\n\nSCAVA Developers Guide\n\n\nThe developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.\n\n\nSCAVA Contributors Guide\n\n\nThe SCAVA Contributors guide containes all the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.\n\n\n\n\nUseful links:\n\n\n\n\nEclipse Scava home project: \nEclipse Scava @ Eclipse\n\n\nEclipse Scava code repository: \ngithub.com/crossminer/scava\n\n\nEclipse Scava deployment repository: \ngithub.com/crossminer/scava-deployment\n\n\nEclipse Scava documentation repository: \ngithub.com/crossminer/scava-docs\n\n\n\n\n\n\n\n\nOld Stuf\n : Documentation which must be migrated on one of the platfomrs guides",
            "title": "Home"
        },
        {
            "location": "/#scava-documentation",
            "text": "This web site is the main documentation place for the  Eclipse Scava  project.",
            "title": "Scava Documentation"
        },
        {
            "location": "/#scava-installation-guide",
            "text": "The SCAVA installation guide provides instructions on how to install and configure the CORSSMINER Platform on a server and how to deploy the Eclipses Plugin in development environment.",
            "title": "SCAVA Installation Guide"
        },
        {
            "location": "/#scava-user-guide",
            "text": "The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.",
            "title": "SCAVA User Guide"
        },
        {
            "location": "/#scava-developers-guide",
            "text": "The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.",
            "title": "SCAVA Developers Guide"
        },
        {
            "location": "/#scava-contributors-guide",
            "text": "The SCAVA Contributors guide containes all the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.",
            "title": "SCAVA Contributors Guide"
        },
        {
            "location": "/#useful-links",
            "text": "Eclipse Scava home project:  Eclipse Scava @ Eclipse  Eclipse Scava code repository:  github.com/crossminer/scava  Eclipse Scava deployment repository:  github.com/crossminer/scava-deployment  Eclipse Scava documentation repository:  github.com/crossminer/scava-docs     Old Stuf  : Documentation which must be migrated on one of the platfomrs guides",
            "title": "Useful links:"
        },
        {
            "location": "/contributors-guide/",
            "text": "SCAVA Contributors Guide\n\n\nThe Contributors Guide summarize the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.\n\n\nContributors Guidelines\n\n\nGuidelines related to the organisation ofthe SCAVA developement process.\n\n\n\n\nSCAVA Repository Organisation\n\n\nSCAVA Development Process\n\n\nSCAVA Testing recommendations\n\n\nSCAVA Naming recommendations\n\n\nSCAVA Licensing recommendations\n\n\n\n\nTechnical Guidelines\n\n\nTutorial and Guidelignes related to the various technologies used by the SCAVA Platform\n\n\n\n\nHow to configure the API Gateway in order to integrate a new REST service\n\n\nHow to consume a SCAVA REST services\n\n\nHow to implement Restlet services\n\n\nHow to extend the SCAVA data model\n\n\nHow to generate REST API Documentation\n\n\nHow to access to MongoDB using PONGO\n\n\n\n\nArchitecture Guidelines\n\n\nGuidelines related to the architecture of some components of the SCAVA Platform\n\n\n\n\nAPI Gateway Component\n\n\nAuthentication Component",
            "title": "Home"
        },
        {
            "location": "/contributors-guide/#scava-contributors-guide",
            "text": "The Contributors Guide summarize the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.",
            "title": "SCAVA Contributors Guide"
        },
        {
            "location": "/contributors-guide/#contributors-guidelines",
            "text": "Guidelines related to the organisation ofthe SCAVA developement process.   SCAVA Repository Organisation  SCAVA Development Process  SCAVA Testing recommendations  SCAVA Naming recommendations  SCAVA Licensing recommendations",
            "title": "Contributors Guidelines"
        },
        {
            "location": "/contributors-guide/#technical-guidelines",
            "text": "Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform   How to configure the API Gateway in order to integrate a new REST service  How to consume a SCAVA REST services  How to implement Restlet services  How to extend the SCAVA data model  How to generate REST API Documentation  How to access to MongoDB using PONGO",
            "title": "Technical Guidelines"
        },
        {
            "location": "/contributors-guide/#architecture-guidelines",
            "text": "Guidelines related to the architecture of some components of the SCAVA Platform   API Gateway Component  Authentication Component",
            "title": "Architecture Guidelines"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/",
            "text": "API Gateway component\n\n\nThe Scava API Gateway :\n\n\n\n\nProvide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application).\n\n\nProvide a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.\n\n\n\n\nAPI Gateway Architecture\n\n\nThe API Gateway  is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change.\n\n\nThe API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the  API Gateway will manage the authentication for all services of the platform.  \n\n\n\n\nAuthentication Mechanism\n\n\nJSON Web Tokens\n\n\nThe Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.\n\n\nIn authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)\n\n\n\n\nAuthentication Architecture\n\n\nIn Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.\n\n\n\n\nAuthentication Flow\n\n\n\n\nTo obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n\n\nWhen the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.\n\n\n\n\n\n\nImplementation\n\n\nThe implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures.\n\n\nhttps://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul\n\n\nAPI Gateway Configuration\n\n\nThe Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.\n\n\nServer Configuration\n\n\n\n\nid : \nserver.port\ndefault :\n 8086\n\n\nPort of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.\n\n\n\n\n\nJWT Security Configuration\n\n\n\n\nid : \napigateway.security.jwt.secret\ndefault :\n NA\n\n\nPrivate key pair which allow to sign jwt tokens using RSA.\n\n\n\n\n\n\n\nid : \napigateway.security.jwt.url\ndefault :\n /login\n\n\nURL Path of the authentication service.\n\n\n\n\n\n\n\nid : \napigateway.security.jwt.expiration\ndefault :\n 86400 (24H)\n\n\nPort of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.\n\n\n\n\n\nRouting : Authentication Service Configuration\n\n\n\n\nid : \nzuul.routes.auth-center.path\ndefault :\n /api/authentication/**\n\n\nRelative path of the authentication service.\n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.url\ndefault :\n NA\n\n\nURL of the authentification server. Example: http://127.0.0.1:8081/ \n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.sensitiveHeaders\ndefault :\n Cookie,Set-Cookie\n\n\nSpecify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers.\n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.stripPrefix\ndefault :\n false\n\n\nSwitch off the stripping of the service-specific prefix from individual routes\n\n\n\n\n\nRouting : Service Configuration\n\n\n\n\nid : \nzuul.routes.**servicename**.path\ndefault :\n NA\n\n\nRelative path of the incoming service which will be redirected. Example : /test1/**\n\n\n\n\n\n\n\nid : \nzuul.routes.**servicename**.url\ndefault :\n NA\n\n\nRedirection URL of the route. Example : http://127.0.0.1:8082/test1\n\n\n\n\n\nConfiguration file example\n\n\n#API Gateway Port\nserver.port=8086\n\n# JWT Configuration\napigateway.security.jwt.secret=otherpeopledontknowit\napigateway.security.jwt.url=/api/authentication\napigateway.security.jwt.expiration=86400\n\n# Rooting Configuration : Authentication Service\nzuul.routes.auth-center.path=/api/authentication/**\nzuul.routes.auth-center.url=http://127.0.0.1:8081/\nzuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie\nzuul.routes.auth-center.stripPrefix=false\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2\n\n\n\n\nControl access API\n\n\nThe Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including:\n\n \u201cROLE_ADMIN\u201d\n\n \u201cROLE_PROJECT_MANAGER\u201d\n* \u201cROLE_USER\u201d\n\n\nBy the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d.\n\n\n# Filtering private restApi\n\nscava.routes.config.adminAccessApi[0]=/api/users\nscava.routes.config.adminAccessApi[1]=/api/user/**\n\nscava.routes.config.projectManagerAccessApi[0]=/administration/projects/create\nscava.routes.config.projectManagerAccessApi[1]=/administration/projects/import\nscava.routes.config.projectManagerAccessApi[2]=/administration/analysis/**\n\nscava.routes.config.userAccessApi[0]=/administration/projects\nscava.routes.config.userAccessApi[1]=/administration/projects/p/**\nscava.routes.config.userAccessApi[2]=/api/users/**\nscava.routes.config.userAccessApi[3]=/api/account\n\n\n\n\nPackaging Form Sources\n\n\nMaven Packaging\n\n\nmvn -Pprod install\n\n\n\n\nAPI Gateway Execution\n\n\n\n\ncomplete an put the \"application.properties\" configuration file in the execute directory.\n\n\nExecute the crossmeter-api-gateway-1.0.0.jar Jar.\n\n\n\n\njava -jar scava-api-gateway-1.0.0.jar\n\n\n\n\nClient Implementation\n\n\nHow to consume a Scava REST services ?\n \\\nThis guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.",
            "title": "Api gateway"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-component",
            "text": "The Scava API Gateway :   Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application).  Provide a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.",
            "title": "API Gateway component"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-architecture",
            "text": "The API Gateway  is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change.  The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the  API Gateway will manage the authentication for all services of the platform.",
            "title": "API Gateway Architecture"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#authentication-mechanism",
            "text": "",
            "title": "Authentication Mechanism"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#json-web-tokens",
            "text": "The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.  In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)",
            "title": "JSON Web Tokens"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#authentication-architecture",
            "text": "In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.",
            "title": "Authentication Architecture"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#authentication-flow",
            "text": "To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.  When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.",
            "title": "Authentication Flow"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#implementation",
            "text": "The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures.  https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul",
            "title": "Implementation"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-configuration",
            "text": "The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.",
            "title": "API Gateway Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#server-configuration",
            "text": "id :  server.port default :  8086  Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.",
            "title": "Server Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#jwt-security-configuration",
            "text": "id :  apigateway.security.jwt.secret default :  NA  Private key pair which allow to sign jwt tokens using RSA.    id :  apigateway.security.jwt.url default :  /login  URL Path of the authentication service.    id :  apigateway.security.jwt.expiration default :  86400 (24H)  Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.",
            "title": "JWT Security Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#routing-authentication-service-configuration",
            "text": "id :  zuul.routes.auth-center.path default :  /api/authentication/**  Relative path of the authentication service.    id :  zuul.routes.auth-center.url default :  NA  URL of the authentification server. Example: http://127.0.0.1:8081/     id :  zuul.routes.auth-center.sensitiveHeaders default :  Cookie,Set-Cookie  Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers.    id :  zuul.routes.auth-center.stripPrefix default :  false  Switch off the stripping of the service-specific prefix from individual routes",
            "title": "Routing : Authentication Service Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#routing-service-configuration",
            "text": "id :  zuul.routes.**servicename**.path default :  NA  Relative path of the incoming service which will be redirected. Example : /test1/**    id :  zuul.routes.**servicename**.url default :  NA  Redirection URL of the route. Example : http://127.0.0.1:8082/test1",
            "title": "Routing : Service Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#configuration-file-example",
            "text": "#API Gateway Port\nserver.port=8086\n\n# JWT Configuration\napigateway.security.jwt.secret=otherpeopledontknowit\napigateway.security.jwt.url=/api/authentication\napigateway.security.jwt.expiration=86400\n\n# Rooting Configuration : Authentication Service\nzuul.routes.auth-center.path=/api/authentication/**\nzuul.routes.auth-center.url=http://127.0.0.1:8081/\nzuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie\nzuul.routes.auth-center.stripPrefix=false\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Configuration file example"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#control-access-api",
            "text": "The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including:  \u201cROLE_ADMIN\u201d  \u201cROLE_PROJECT_MANAGER\u201d\n* \u201cROLE_USER\u201d  By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d.  # Filtering private restApi\n\nscava.routes.config.adminAccessApi[0]=/api/users\nscava.routes.config.adminAccessApi[1]=/api/user/**\n\nscava.routes.config.projectManagerAccessApi[0]=/administration/projects/create\nscava.routes.config.projectManagerAccessApi[1]=/administration/projects/import\nscava.routes.config.projectManagerAccessApi[2]=/administration/analysis/**\n\nscava.routes.config.userAccessApi[0]=/administration/projects\nscava.routes.config.userAccessApi[1]=/administration/projects/p/**\nscava.routes.config.userAccessApi[2]=/api/users/**\nscava.routes.config.userAccessApi[3]=/api/account",
            "title": "Control access API"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#packaging-form-sources",
            "text": "Maven Packaging  mvn -Pprod install",
            "title": "Packaging Form Sources"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-execution",
            "text": "complete an put the \"application.properties\" configuration file in the execute directory.  Execute the crossmeter-api-gateway-1.0.0.jar Jar.   java -jar scava-api-gateway-1.0.0.jar",
            "title": "API Gateway Execution"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/api-gateway/#client-implementation",
            "text": "How to consume a Scava REST services ?  \\\nThis guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.",
            "title": "Client Implementation"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/",
            "text": "Authentication Component\n\n\nThe Scava Authentication service:\n\n\n\n\nProvides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform.\n\n\nProvides user management services, including user registration process, user profile editing and roles based authorization management.\n\n\n\n\nAuthentication API\n\n\nThe Authentication server is a component of The Scava platform which manages the authentication for all  services accessible behind the API Gateway.\n\n\n\n\nAuthenticate User\nPOST\n/api/authentication\n\n\nLogin as a registered user.\n\n\n\n\n\n\n\n### JSON Web Tokens (JWT)\n\nJSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are:\n* Header\n* Payload\n* Signature\n\nThis solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/).\n\n### JWT Authentication Implementation\n\n* Users have to login to the authentication service API using their credentials username and password.\n\n\u0002wzxhzdk:0\u0003\n\n* Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header.\n\n\n\n\n* The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway.\n\n\u0002wzxhzdk:1\u0003\n\n\n\n\n## User Management API\n\nThe Authentication component provides web services for CRUD user account.\n\n\n\n\nRegister User\nPOST\n/api/register\n\n\nRegister new user.\n\n\n\n\n\n\nActivate User\nGET\n/api/activate\n\n\nActivate the registered user.\n\n\n\n\n\n\nUpdate User\nPUT\n/api/users\n\n\nUpdate an existing user.\n\n\n\n\n\n\nRetrieve Users\nGET\n/api/users\n\n\nGet all registered users.\n\n\n\n\n\n\nRetrieve Login User\nGET\n/api/users/{login}\n\n\nGet the \"login\" user.\n\n\n\n\n\n\nDelete User\nDELETE\n/api/users/{login}\n\n\nDelete the \"login\" user.\n\n\n\n\n\nAuthentication Server Configuration\n\n\nThe Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.\n\n\nServer Configuration\n\n\n\n\nid : \nserver.port\ndefault :\n 8085\n\n\nPort of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.\n\n\n\n\n\nJWT Security Configuration\n\n\n\n\nid : \napigateway.security.jwt.secret\ndefault :\n NA\n\n\nPrivate key pair which allow to sign jwt tokens using RSA.\n\n\n\n\n\nDefault ADMIN configuration\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nscava.administration.username\n\n\nThe administrator username\n\n\nadmin\n\n\n\n\n\n\nscava.administration.password\n\n\nThe administrator password\n\n\nadmin\n\n\n\n\n\n\nscava.administration.admin-role\n\n\nThe admin role\n\n\nADMIN\n\n\n\n\n\n\nscava.administration.project-manager-role\n\n\nThe project manager role\n\n\nPROJECT_MANAGER\n\n\n\n\n\n\nscava.administration.project-user-role\n\n\nThe user role\n\n\nUSER\n\n\n\n\n\n\n\n\nMongodb Database Configuration\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nspring.data.mongodb.uri\n\n\nUrl of the MongoDB database server\n\n\nmongodb://localhost:27017\n\n\n\n\n\n\nspring.data.mongodb.database\n\n\nName of the MongoDB database\n\n\nscava\n\n\n\n\n\n\n\n\nMail Server configuration\n\n\nIn order to register new users, you have to configure a mail server.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nspring.mail.host\n\n\nUrl of the mail service\n\n\nsmtp.gmail.com\n\n\n\n\n\n\nspring.mail.port\n\n\nPort of the mail service\n\n\n587\n\n\n\n\n\n\nspring.mail.username\n\n\nLogin of the mail account\n\n\n\n\n\n\n\n\nspring.mail.password\n\n\nPassword of the mail account\n\n\n\n\n\n\n\n\nspring.mail.protocol\n\n\nmail protocole\n\n\nsmtp\n\n\n\n\n\n\nspring.mail.tls\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.auth\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.starttls.enable\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.ssl.trust=\n\n\n-\n\n\nsmtp.gmail.com\n\n\n\n\n\n\n\n\nAdministration Dashboard Setting\n\n\n\n\nid : \nscava.administration.base-url\ndefault :\n http://localhost:4200\n\n\nThe SCAVA administration base URL to generate the activation account URL.\n\n\n\n\n\nPackaging From Sources\n\n\nMaven Packaging\n\n\nmvn -Pprod install\n\n\n\n\nAuthentication Server Execution\n\n\n\n\ncomplete an put the \"application.properties\" configuration file in the execution directory.\n\n\nExecute the scava-auth-service-1.0.0.jar Jar.\n\n\n\n\njava -jar scava-auth-service-1.0.0.jar",
            "title": "Authentication"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#authentication-component",
            "text": "The Scava Authentication service:   Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform.  Provides user management services, including user registration process, user profile editing and roles based authorization management.",
            "title": "Authentication Component"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#authentication-api",
            "text": "The Authentication server is a component of The Scava platform which manages the authentication for all  services accessible behind the API Gateway.   Authenticate User POST /api/authentication  Login as a registered user.   \n\n### JSON Web Tokens (JWT)\n\nJSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are:\n* Header\n* Payload\n* Signature\n\nThis solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/).\n\n### JWT Authentication Implementation\n\n* Users have to login to the authentication service API using their credentials username and password.\n\n\u0002wzxhzdk:0\u0003\n\n* Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. \n\n* The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway.\n\n\u0002wzxhzdk:1\u0003 \n\n## User Management API\n\nThe Authentication component provides web services for CRUD user account.  Register User POST /api/register  Register new user.    Activate User GET /api/activate  Activate the registered user.    Update User PUT /api/users  Update an existing user.    Retrieve Users GET /api/users  Get all registered users.    Retrieve Login User GET /api/users/{login}  Get the \"login\" user.    Delete User DELETE /api/users/{login}  Delete the \"login\" user.",
            "title": "Authentication API"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#authentication-server-configuration",
            "text": "The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.",
            "title": "Authentication Server Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#server-configuration",
            "text": "id :  server.port default :  8085  Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.",
            "title": "Server Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#jwt-security-configuration",
            "text": "id :  apigateway.security.jwt.secret default :  NA  Private key pair which allow to sign jwt tokens using RSA.",
            "title": "JWT Security Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#default-admin-configuration",
            "text": "Property  Description  Default Value      scava.administration.username  The administrator username  admin    scava.administration.password  The administrator password  admin    scava.administration.admin-role  The admin role  ADMIN    scava.administration.project-manager-role  The project manager role  PROJECT_MANAGER    scava.administration.project-user-role  The user role  USER",
            "title": "Default ADMIN configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#mongodb-database-configuration",
            "text": "Property  Description  Default Value      spring.data.mongodb.uri  Url of the MongoDB database server  mongodb://localhost:27017    spring.data.mongodb.database  Name of the MongoDB database  scava",
            "title": "Mongodb Database Configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#mail-server-configuration",
            "text": "In order to register new users, you have to configure a mail server.     Property  Description  Default Value      spring.mail.host  Url of the mail service  smtp.gmail.com    spring.mail.port  Port of the mail service  587    spring.mail.username  Login of the mail account     spring.mail.password  Password of the mail account     spring.mail.protocol  mail protocole  smtp    spring.mail.tls  -  true    spring.mail.properties.mail.smtp.auth  -  true    spring.mail.properties.mail.smtp.starttls.enable  -  true    spring.mail.properties.mail.smtp.ssl.trust=  -  smtp.gmail.com",
            "title": "Mail Server configuration"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#administration-dashboard-setting",
            "text": "id :  scava.administration.base-url default :  http://localhost:4200  The SCAVA administration base URL to generate the activation account URL.",
            "title": "Administration Dashboard Setting"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#packaging-from-sources",
            "text": "Maven Packaging  mvn -Pprod install",
            "title": "Packaging From Sources"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/authentication/#authentication-server-execution",
            "text": "complete an put the \"application.properties\" configuration file in the execution directory.  Execute the scava-auth-service-1.0.0.jar Jar.   java -jar scava-auth-service-1.0.0.jar",
            "title": "Authentication Server Execution"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/",
            "text": "Architecture Guidelines\n\n\nGuidelines related to the architecture of some components of the SCAVA Platform\n\n\n\n\n\n\nAPI Gateway Component\n\nThe API Gateway Component provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application) and a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.\n\n\n\n\n\n\nAuthentication Component\n\n\n\n\n\n\nThe Scava Authentication service provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform and user management services, including user registration process, user profile editing and roles based authorization management.",
            "title": "Home"
        },
        {
            "location": "/contributors-guide/architecture-guidelignes/#architecture-guidelines",
            "text": "Guidelines related to the architecture of some components of the SCAVA Platform    API Gateway Component \nThe API Gateway Component provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application) and a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.    Authentication Component    The Scava Authentication service provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform and user management services, including user registration process, user profile editing and roles based authorization management.",
            "title": "Architecture Guidelines"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/",
            "text": "Contributors Guidelines\n\n\nGuidelines related to the organisation ofthe SCAVA developement process.\n\n\n\n\nSCAVA Repository Organisation\n\n\nSCAVA Development Process\n\n\nSCAVA Testing recommendations\n\n\nSCAVA Naming recommendations\n\n\nSCAVA Licensing recommendations",
            "title": "Home"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/#contributors-guidelines",
            "text": "Guidelines related to the organisation ofthe SCAVA developement process.   SCAVA Repository Organisation  SCAVA Development Process  SCAVA Testing recommendations  SCAVA Naming recommendations  SCAVA Licensing recommendations",
            "title": "Contributors Guidelines"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/",
            "text": "SCAVA Development Process\n\n\nIntroduction\n\n\nThis document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.\n\n\nProcess overview\n\n\nThe development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system.\n\n\nIt is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests.\n\n\nThe different elements shown below are individually described in the remaining of the section.\n\n\n\nFigure 1. Overview of the CROSSMINER development process and tools.\n\n\nSource Code Repository\n\n\nDifferent branches will be created in the repository. In particular, the \nmaster\n branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The \ndev\n branch contains the most recent code integrated by partners before release.\n\n\nOther branches will be created for the different features/components, which will be eventually merged in the \ndev\n branch. The name of each branch contains the id of the issue it is supposed to implement.\n\n\nAll source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository.\n\n\n\n\nCode: https://github.com/crossminer/scava\n\n\nProduct documentation: https://github.com/crossminer/scava-docs\n\n\nProduct deployment: https://github.com/crossminer/scava-deployment\n\n\n\n\nSince all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details).\n\n\nBy taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3.\n\n\n\nFigure 2: Branching model\n\n\nTests\n\n\nEach branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below.\n\n\n\nFigure 3: Explanatory master and branches\n\n\nBRANCH: Set of branches\nTEST: Set of tests\nINTGRATION_TEST < TEST\nUNIT_TEST < TEST\n\nfunction test: BRANCH -> TEST\n\ntest(master) -> test(cool-feature)\n    \u22c3 {itu, \u2026 itz}\n\n\n\n\nContinuous Integration Server\n\n\nContinuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus \nreduce cost and time\n.\n\n\nA complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all \nunit tests\n. \nIntegration tests\n are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities:\n\n\n\n\nAllow developers to write unit tests that can be executed automatically\n\n\nPerform automated tests against newly written code\n\n\nShow a list of tests that have passed and failed\n\n\nQuality analysis on source code\n\n\nPerform all the necessary actions to create a fully functioning build of the software when all tests have passed\n\n\n\n\nThe currently used CI server is available at http://ci5.castalia.camp:8080/\n\n\nDevelopment and Production environments\n\n\nAccording to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.\n\n\nNaming and change conventions\n\n\nThe repository will contain the code of the high-level components shown in Table 1\n\n\n\n\n\n\n\n\nComponents\n\n\nCorresponding folder in repository\n\n\nLeader\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\n\n\nBIT\n\n\n\n\n\n\nWorkflow Diagram Editor\n\n\n/crossflow\n\n\nYORK\n\n\n\n\n\n\nAdministration Web Application\n\n\n/administration\n\n\nSFT\n\n\n\n\n\n\nIDE\n\n\n/eclipse-based-ide\n\n\nFEA\n\n\n\n\n\n\nAPI Gateway\n\n\n/api-gateway\n\n\nSFT\n\n\n\n\n\n\nDevOps Backend\n\n\n\n\nBIT\n\n\n\n\n\n\nKnowledge Base\n\n\n/knowledge-base\n\n\nUDA\n\n\n\n\n\n\nProject Analyser\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nData Collector\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nProject Scheduler\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nMetric Providers\n\n\n/metric-platform\n\n\nYORK, CWI, AUEB, EHU\n\n\n\n\n\n\nData Storage\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nWeb-based dashboards\n\n\n/web-dashboards\n\n\nBIT\n\n\n\n\n\n\n\n\nTable 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes\n\n\n\nFigure 4: Scava components architecture\n\n\nFor each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus:\n\n\n\n\nwhen developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons;\n\n\nwhen a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR.\n\n\n\n\nThe partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder:\n\n\n\n\nreadme.md\n : this \ufb01le will contain the entry-point to all documentation for the component.\n\n\nJenkinsfile\n: in order to compile the component, run tests, etc on the CI server.\n\n\ntest\n: this is a folder containing the unit and/or integration tests.\n\n\n\n\nAs a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base\n\n\nThe following items are typical practices when applying CI:\n\n Maintain a code repository\n\n Automate the build\n\n Make the build self-testing\n\n Everyone commits to the baseline every day\n\n Every commit (to baseline) should be built\n\n Keep the build fast\n\n Test in a clone of the production environment\n\n Make it easy to get the latest deliverables\n\n Everyone can see the results of the latest build\n\n Automate deployment\n\n\nCommunication and collaboration means\n\n\nThe development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of:\n\n\n\n\nSlack channel: http://crossminer.slack.com\n\n\nEclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev\n\n\nGitHub repository for documentation: https://github.com/crossminer/scava-docs/",
            "title": "Scava developement process"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#scava-development-process",
            "text": "",
            "title": "SCAVA Development Process"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#introduction",
            "text": "This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.",
            "title": "Introduction"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#process-overview",
            "text": "The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system.  It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests.  The different elements shown below are individually described in the remaining of the section.  \nFigure 1. Overview of the CROSSMINER development process and tools.",
            "title": "Process overview"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#source-code-repository",
            "text": "Different branches will be created in the repository. In particular, the  master  branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The  dev  branch contains the most recent code integrated by partners before release.  Other branches will be created for the different features/components, which will be eventually merged in the  dev  branch. The name of each branch contains the id of the issue it is supposed to implement.  All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository.   Code: https://github.com/crossminer/scava  Product documentation: https://github.com/crossminer/scava-docs  Product deployment: https://github.com/crossminer/scava-deployment   Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details).  By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3.  \nFigure 2: Branching model",
            "title": "Source Code Repository"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#tests",
            "text": "Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below.  \nFigure 3: Explanatory master and branches  BRANCH: Set of branches\nTEST: Set of tests\nINTGRATION_TEST < TEST\nUNIT_TEST < TEST\n\nfunction test: BRANCH -> TEST\n\ntest(master) -> test(cool-feature)\n    \u22c3 {itu, \u2026 itz}",
            "title": "Tests"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#continuous-integration-server",
            "text": "Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus  reduce cost and time .  A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all  unit tests .  Integration tests  are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities:   Allow developers to write unit tests that can be executed automatically  Perform automated tests against newly written code  Show a list of tests that have passed and failed  Quality analysis on source code  Perform all the necessary actions to create a fully functioning build of the software when all tests have passed   The currently used CI server is available at http://ci5.castalia.camp:8080/",
            "title": "Continuous Integration Server"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#development-and-production-environments",
            "text": "According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.",
            "title": "Development and Production environments"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#naming-and-change-conventions",
            "text": "The repository will contain the code of the high-level components shown in Table 1     Components  Corresponding folder in repository  Leader      DevOps Dashboard   BIT    Workflow Diagram Editor  /crossflow  YORK    Administration Web Application  /administration  SFT    IDE  /eclipse-based-ide  FEA    API Gateway  /api-gateway  SFT    DevOps Backend   BIT    Knowledge Base  /knowledge-base  UDA    Project Analyser  /metric-platform  SFT    Data Collector  /metric-platform  SFT    Project Scheduler  /metric-platform  SFT    Metric Providers  /metric-platform  YORK, CWI, AUEB, EHU    Data Storage  /metric-platform  SFT    Web-based dashboards  /web-dashboards  BIT     Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes  \nFigure 4: Scava components architecture  For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus:   when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons;  when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR.   The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder:   readme.md  : this \ufb01le will contain the entry-point to all documentation for the component.  Jenkinsfile : in order to compile the component, run tests, etc on the CI server.  test : this is a folder containing the unit and/or integration tests.   As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base  The following items are typical practices when applying CI:  Maintain a code repository  Automate the build  Make the build self-testing  Everyone commits to the baseline every day  Every commit (to baseline) should be built  Keep the build fast  Test in a clone of the production environment  Make it easy to get the latest deliverables  Everyone can see the results of the latest build  Automate deployment",
            "title": "Naming and change conventions"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-developement-process/#communication-and-collaboration-means",
            "text": "The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of:   Slack channel: http://crossminer.slack.com  Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev  GitHub repository for documentation: https://github.com/crossminer/scava-docs/",
            "title": "Communication and collaboration means"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-licensing-recomendation/",
            "text": "SCAVA Licensing recommendations\n\n\nContent\n\n\nThe Scava project is licensed under \nEclipse Public License - v 2.0\n license.\n\n\nAs consequence of our status of project hosted by the eclipse foundation, all Scava components  must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.\n\n\n\"Eclipse Public License\" licensing file\n\n\nThe text below must be integrated to the root folder of your project on a text file name \"LICENSE\".\n\n\nEclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.\n\n\n\n\nSource File header\n\n\nAll sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...).\n\n\nOptionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization.\n\n\nExample of Java license header file\n\n\n/*******************************************************************************\n * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"}\n * This program and the accompanying materials are made\n * available under the terms of the Eclipse Public License 2.0\n * which is available at https://www.eclipse.org/legal/epl-2.0/\n *\n * SPDX-License-Identifier: EPL-2.0\n ******************************************************************************/\n\n\n\n\nComment\n\n\nn Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files.\n\n\nThis plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.",
            "title": "Scava licensing recomendation"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#scava-licensing-recommendations",
            "text": "",
            "title": "SCAVA Licensing recommendations"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#content",
            "text": "The Scava project is licensed under  Eclipse Public License - v 2.0  license.  As consequence of our status of project hosted by the eclipse foundation, all Scava components  must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.",
            "title": "Content"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#eclipse-public-license-licensing-file",
            "text": "The text below must be integrated to the root folder of your project on a text file name \"LICENSE\".  Eclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.",
            "title": "\"Eclipse Public License\" licensing file"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#source-file-header",
            "text": "All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...).  Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization.  Example of Java license header file  /*******************************************************************************\n * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"}\n * This program and the accompanying materials are made\n * available under the terms of the Eclipse Public License 2.0\n * which is available at https://www.eclipse.org/legal/epl-2.0/\n *\n * SPDX-License-Identifier: EPL-2.0\n ******************************************************************************/",
            "title": "Source File header"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#comment",
            "text": "n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files.  This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.",
            "title": "Comment"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/",
            "text": "SCAVA Naming recommendations\n\n\nComponent Naming\n\n\nAs consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project.\n\n\nIn this section, \"component\" means big funcional componen of the SCAVA project.\n\n\n\n\n\n\n\n\nComponent\n\n\nComponentId\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\ndashboard\n\n\n\n\n\n\nWorkflow Execution Engine\n\n\nworkflow\n\n\n\n\n\n\nKnowledge Base\n\n\nknowledgebase\n\n\n\n\n\n\nMetric Provider\n\n\nmetricprovider\n\n\n\n\n\n\nAdministration\n\n\nadministration\n\n\n\n\n\n\n\n\nProject Naming\n\n\n\n\nFor Eclipse Plugin \n\n\n\n\norg.eclipse.scava.{componentname}\n\n\n\n\n\n\nFor Maven Project : the name of the project if the ArtifactId\n\n\n\n\nIf your component is composed of one project :\n\n\n{component-name}\n\n\n\n\nIf your component is composed of several sub projects :\n\n\n{sub-component-name}\n ```\n\n## Source Code Namsespace\nAll sources must be nemspaces by : \n\n\n\n\norg.eclipse.scava.{componentname}\n\n\n\n## Maven Ids\nFor  the Maven projects:\n\n* If your component is composed of one project : \n\n\n\n\nGroup Id : org.eclipse.scava\nArtifactId : {component-name}\n\n\n\n* If your component is composed of several sub projects : \n\n\n\n\nGroup Id : org.eclipse.scava.{component-name}\nArtifactId : {sub-component-name}\n```\n\n\nREST services Naming\n\n\nThe REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of  platform services , we need to used a common naming schema for all REST services provided by the platform.\n\n\nHow to name a REST service ?\n\n\n\n\n/{\ncomponentid\n}/{\ncategoryname\n}/{\nservicename\n}\n\n\n\n\n\n\n/\ncomponentid\n/\n: Name of the Architectural component which provide the service\n\n\n/\ncategoryname\n/ (Optional)\n : optional category of the service\n\n\n/\nservicename\n/\n : Name of the rest service\n\n\n\n\nComponent\n\n\n\n\n\n\n\n\nComponent\n\n\nComponentId\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\ndashboard\n\n\n\n\n\n\nWorkflow Execution Engine\n\n\nworkflow\n\n\n\n\n\n\nKnowledge Base\n\n\nknowledgebase\n\n\n\n\n\n\nMetric Provider\n\n\nmetricprovider\n\n\n\n\n\n\nAdministration\n\n\nadministration",
            "title": "Scava naming recomendations"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/#scava-naming-recommendations",
            "text": "",
            "title": "SCAVA Naming recommendations"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/#component-naming",
            "text": "As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project.  In this section, \"component\" means big funcional componen of the SCAVA project.     Component  ComponentId      DevOps Dashboard  dashboard    Workflow Execution Engine  workflow    Knowledge Base  knowledgebase    Metric Provider  metricprovider    Administration  administration",
            "title": "Component Naming"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/#project-naming",
            "text": "For Eclipse Plugin    org.eclipse.scava.{componentname}   For Maven Project : the name of the project if the ArtifactId   If your component is composed of one project :  {component-name}  If your component is composed of several sub projects :  {sub-component-name}\n ```\n\n## Source Code Namsespace\nAll sources must be nemspaces by :   org.eclipse.scava.{componentname}  \n## Maven Ids\nFor  the Maven projects:\n\n* If your component is composed of one project :   Group Id : org.eclipse.scava\nArtifactId : {component-name}  \n* If your component is composed of several sub projects :   Group Id : org.eclipse.scava.{component-name}\nArtifactId : {sub-component-name}\n```",
            "title": "Project Naming"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/#rest-services-naming",
            "text": "The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of  platform services , we need to used a common naming schema for all REST services provided by the platform.",
            "title": "REST services Naming"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/#how-to-name-a-rest-service",
            "text": "/{ componentid }/{ categoryname }/{ servicename }    / componentid / : Name of the Architectural component which provide the service  / categoryname / (Optional)  : optional category of the service  / servicename /  : Name of the rest service",
            "title": "How to name a REST service ?"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-naming-recomendations/#component",
            "text": "Component  ComponentId      DevOps Dashboard  dashboard    Workflow Execution Engine  workflow    Knowledge Base  knowledgebase    Metric Provider  metricprovider    Administration  administration",
            "title": "Component"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-repository-organisation/",
            "text": "SCAVA Repository Organisation\n\n\nThe SCAVA code repository  is organized by functional components with one package for each of this components. \n\n\nGeneral organisation\n\n\n\n\nmetric-platform \n\n\nplatform : Core projects of the metric platform\n\n\nplatform-extensions : Extensions of the metric platform\n\n\nmetric-providers : Metric Providers implementations projects\n\n\nfactoids : Factoids implementations projects\n\n\ntests : Test projects related to the metric-platform\n\n\nweb-dashboards : DevOps Dashboard and DevOpsDashboard components \n\n\nknowledge-base : Knowledge Base implementation\n\n\nWorkflow Execution Engine : Workflow Execution Engine implementation\n\n\neclipse-based-ide : SCAVA Eclipse plugin\n\n\napi-gateway :  API Gateway implementation.\n\n\nadministration : Administration component implementation\n\n\nmingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.",
            "title": "Scava repository organisation"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-repository-organisation/#scava-repository-organisation",
            "text": "The SCAVA code repository  is organized by functional components with one package for each of this components.",
            "title": "SCAVA Repository Organisation"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-repository-organisation/#general-organisation",
            "text": "metric-platform   platform : Core projects of the metric platform  platform-extensions : Extensions of the metric platform  metric-providers : Metric Providers implementations projects  factoids : Factoids implementations projects  tests : Test projects related to the metric-platform  web-dashboards : DevOps Dashboard and DevOpsDashboard components   knowledge-base : Knowledge Base implementation  Workflow Execution Engine : Workflow Execution Engine implementation  eclipse-based-ide : SCAVA Eclipse plugin  api-gateway :  API Gateway implementation.  administration : Administration component implementation  mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.",
            "title": "General organisation"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-testing-organisation/",
            "text": "SCAVA Testing recommendations\n\n\nKnowledge Base\n\n\nThis builds needs some configuration to run successfully. \n\n\nIn the application.properties files:\n\n \n/knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties\n\n\n \n/knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties\n\n\nEdit the following parameters:\n\n\nlucene.index.folder=/tmp/scava_lucene/\negit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57\n\n\n\n\nTo generate the GitHub access token you need to go to \nyour own GitHub account\n and create a new one. Once it's done simply restart the tests, they should pass.",
            "title": "Scava testing organisation"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-testing-organisation/#scava-testing-recommendations",
            "text": "",
            "title": "SCAVA Testing recommendations"
        },
        {
            "location": "/contributors-guide/contributors-guidelignes/scava-testing-organisation/#knowledge-base",
            "text": "This builds needs some configuration to run successfully.   In the application.properties files:   /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties    /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties  Edit the following parameters:  lucene.index.folder=/tmp/scava_lucene/\negit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57  To generate the GitHub access token you need to go to  your own GitHub account  and create a new one. Once it's done simply restart the tests, they should pass.",
            "title": "Knowledge Base"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/",
            "text": "How to access to MongoDB using PONGO\n\n\nWhen to use ?\n\n\nThis guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.\n\n\nContext\n\n\nThe Scava platform use a MongoDb data base to store his data. We go through \nPONGO\n, a template based \nJava POJO generator\n to access \nMongoDB\n database. With Pongo we can define the data model which generates strongly-typed Java classes.\n\n\nIn this guideligne, we will present  : \n\n The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform.\n\n The access to MongoDb Document on from an external Java application.\n* How to preform basic CRUD operation with a PONGO Java data model.\n\n\nWe consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : \nExtend MongoDB Data Model\n.\n\n\nYou want to access to MongoDB Document from an Eclipse Plugin ?\n\n\n1. Add a dependency to the Java Data Model\n\n\n\n\nEdit the plugin.xml file of your plugin.\n\n\nIn Dependency section, add a dependency to  the plugin which contained the data model you went to access.\n\n\nTo \norg.ossmeter.repository.model\n to access data related to project administration , metric execution process and authentification system.\n\n\nTo \norg.ossmeter.repository.model.'project delta manager'\n to access  configuration informations related to source codes managemeny tools.\n\n\nTo  \nspecific metric provider plugins\n  to access data related to a specific metric provider implementation contains his once data model.\n\n\n... others plugin which contained the data model\n\n\nIn Dependency section, add a dependency to \ncom.googlecode.pongo.runtime\n plugin\n\n\n\n\n\n\n2. Initiate a Connection to the MongoDb\n\n\nIn context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base.\n\n\n\n\nIn Dependency section of plugin.xml file , add a dependency to the \norg.ossmeter.platform\n plugin.\n\n\nYou can now create a new connection to the database using the Configuration service.\n\n\n\n\nMongo mongo = Configuration.getInstance().getMongoConnection(); \n\n\n\n\nYou want to access to MongoDB Document on from an External Java Application ?\n\n\n1. Add a dependency to the Java Data Model\n\n\n\n\nAdd to your java project a dependency to the jar which contained  the data model you went to access. You will have to deliver this jar with your application.\n\n\n\n\n\n\n\n\nAdd o your java project a dependency to the \npongo.jar\n jar file which can be download at this url : https://github.com/kolovos/pongo/releases\n\n\n\n\n2. Initiate a Connection to MongoDb\n\n\n// Define ServerAddress of the MongoDb database\nList<ServerAddress> mongoHostAddresses = new ArrayList<>();\nmongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1])));\n\n// Create Connection\nMongo mongo = new Mongo(mongoHostAddresses);\n\n\n\n\nOnce the connection to MongoDb has been created, you have to make the link  between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document.\n\n\nDB db =  mongo.getDB(\"databasename\");\n\n// Initiate the Project Java model\nProjectRepository = new ProjectRepository(db);\n\n\n\n\nBasic CRUD with a PONGO Java data model\n\n\n1. CREATE\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);\n\n\n\n\n2. READ\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to access object properties\nmetricprovider.getname();\n......\n\n\n\n\n3. UPDATE\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);\n\n\n\n\n4. DELETE\n\n\nMongo mongo = new Mongo();\nmongo.dropDatabase(\"databasename\");",
            "title": "Access mongodb pongo"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#how-to-access-to-mongodb-using-pongo",
            "text": "",
            "title": "How to access to MongoDB using PONGO"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#when-to-use",
            "text": "This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.",
            "title": "When to use ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#context",
            "text": "The Scava platform use a MongoDb data base to store his data. We go through  PONGO , a template based  Java POJO generator  to access  MongoDB  database. With Pongo we can define the data model which generates strongly-typed Java classes.  In this guideligne, we will present  :   The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform.  The access to MongoDb Document on from an external Java application.\n* How to preform basic CRUD operation with a PONGO Java data model.  We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model :  Extend MongoDB Data Model .",
            "title": "Context"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin",
            "text": "",
            "title": "You want to access to MongoDB Document from an Eclipse Plugin ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-add-a-dependency-to-the-java-data-model",
            "text": "Edit the plugin.xml file of your plugin.  In Dependency section, add a dependency to  the plugin which contained the data model you went to access.  To  org.ossmeter.repository.model  to access data related to project administration , metric execution process and authentification system.  To  org.ossmeter.repository.model.'project delta manager'  to access  configuration informations related to source codes managemeny tools.  To   specific metric provider plugins   to access data related to a specific metric provider implementation contains his once data model.  ... others plugin which contained the data model  In Dependency section, add a dependency to  com.googlecode.pongo.runtime  plugin",
            "title": "1. Add a dependency to the Java Data Model"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-initiate-a-connection-to-the-mongodb",
            "text": "In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base.   In Dependency section of plugin.xml file , add a dependency to the  org.ossmeter.platform  plugin.  You can now create a new connection to the database using the Configuration service.   Mongo mongo = Configuration.getInstance().getMongoConnection();",
            "title": "2. Initiate a Connection to the MongoDb"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application",
            "text": "",
            "title": "You want to access to MongoDB Document on from an External Java Application ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-add-a-dependency-to-the-java-data-model_1",
            "text": "Add to your java project a dependency to the jar which contained  the data model you went to access. You will have to deliver this jar with your application.     Add o your java project a dependency to the  pongo.jar  jar file which can be download at this url : https://github.com/kolovos/pongo/releases",
            "title": "1. Add a dependency to the Java Data Model"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-initiate-a-connection-to-mongodb",
            "text": "// Define ServerAddress of the MongoDb database\nList<ServerAddress> mongoHostAddresses = new ArrayList<>();\nmongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1])));\n\n// Create Connection\nMongo mongo = new Mongo(mongoHostAddresses);  Once the connection to MongoDb has been created, you have to make the link  between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document.  DB db =  mongo.getDB(\"databasename\");\n\n// Initiate the Project Java model\nProjectRepository = new ProjectRepository(db);",
            "title": "2. Initiate a Connection to MongoDb"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#basic-crud-with-a-pongo-java-data-model",
            "text": "",
            "title": "Basic CRUD with a PONGO Java data model"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-create",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);",
            "title": "1. CREATE"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-read",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to access object properties\nmetricprovider.getname();\n......",
            "title": "2. READ"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#3-update",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);",
            "title": "3. UPDATE"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/access-mongodb-pongo/#4-delete",
            "text": "Mongo mongo = new Mongo();\nmongo.dropDatabase(\"databasename\");",
            "title": "4. DELETE"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/api-gateway-configuration/",
            "text": "How to configure the API Gateway in order to integrate a new REST service\n\n\nWhen to use this guideline ?\n\n\nThis guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.\n\n\nContext\n\n\nThe Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.\n\n\nRouting : Service Configuration\n\n\nTo reference a new remote REST API in the gateway, you have to add  2 new properties in the application.properties configuration file : the relative path of services  which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process.\n\n\nExamples :\n\n api gateway url = http://85.36.10.13:8080\n\n path = /administration/*\n\n\n url = http://85.36.10.12:8082/administration\n\n\nThe request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create\n\n\n\n\nid : \nzuul.routes.**servicename**.path\ndefault :\n NA\n\n\nRelative path of the incoming service which will be redirected. Example : /test1/**\n\n\n\n\n\n\n\nid : \nzuul.routes.**servicename**.url\ndefault :\n NA\n\n\nRedirection URL of the route. Example : http://127.0.0.1:8082/test1\n\n\n\n\n\nConfiguration file example\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Api gateway configuration"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/api-gateway-configuration/#how-to-configure-the-api-gateway-in-order-to-integrate-a-new-rest-service",
            "text": "",
            "title": "How to configure the API Gateway in order to integrate a new REST service"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/api-gateway-configuration/#when-to-use-this-guideline",
            "text": "This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/api-gateway-configuration/#context",
            "text": "The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.",
            "title": "Context"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/api-gateway-configuration/#routing-service-configuration",
            "text": "To reference a new remote REST API in the gateway, you have to add  2 new properties in the application.properties configuration file : the relative path of services  which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process.  Examples :  api gateway url = http://85.36.10.13:8080  path = /administration/*   url = http://85.36.10.12:8082/administration  The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create   id :  zuul.routes.**servicename**.path default :  NA  Relative path of the incoming service which will be redirected. Example : /test1/**    id :  zuul.routes.**servicename**.url default :  NA  Redirection URL of the route. Example : http://127.0.0.1:8082/test1",
            "title": "Routing : Service Configuration"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/api-gateway-configuration/#configuration-file-example",
            "text": "# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Configuration file example"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/",
            "text": "How to consume a SCAVA REST services\n\n\nWhen to use ?\n\n\nThis guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.\n\n\nREST API Reference\n\n\nThe reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].\n\n\nAPI Gateway\n\n\nThe Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway.\n\n\nAll web service request form clients have to go through the gateway.\n\n\n\n\nThe api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.\n\n\nPlatform Authentication\n\n\nThe CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io).\n1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n1. When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.\n\n\nAuthentication in Java\n\n\nRetrieve a Web Tokens from authentication service\n\n\nprivate String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException {\n  // Authentication Service URI\n  URL url = new URL(\"http://localhost:8086/api/authentication\");\n\n  // AUthentication Request\n  HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n  connection.setDoOutput(true);\n  connection.setRequestMethod(\"POST\");\n  connection.setRequestProperty(\"Content-Type\", \"application/json\");\n\n  String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\";\n  OutputStream os = connection.getOutputStream();\n  os.write(input.getBytes());\n  os.flush();\n\n  if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n    throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode());\n  }\n\n  connection.disconnect();\n\n  // A JWT Token is return in the Header of the response\n  return connection.getHeaderField(\"Authorization\");\n}\n\n\n\n\nREST Service Call\n\n\ncurl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication\n\n\n\n\nService Consumption\n\n\nTo consume a REST service provided by the integrated platform, the client must include the Token in the header of his request.\n\n\n```java\n// Service URL\nURL url = new URL(\"http://localhost:8086/api/users\");\nHttpURLConnection connection = (HttpURLConnection) url.openConnection();\nconnection.setRequestMethod(\"GET\");\n\n\n// Add Token to the request header\nconnection.setRequestProperty(\"Authorization\",token);",
            "title": "Consume scava rest services"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/#how-to-consume-a-scava-rest-services",
            "text": "",
            "title": "How to consume a SCAVA REST services"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/#when-to-use",
            "text": "This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.",
            "title": "When to use ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/#rest-api-reference",
            "text": "The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].",
            "title": "REST API Reference"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/#api-gateway",
            "text": "The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway.  All web service request form clients have to go through the gateway.   The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.",
            "title": "API Gateway"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/#platform-authentication",
            "text": "The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io).\n1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n1. When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.  Authentication in Java  Retrieve a Web Tokens from authentication service  private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException {\n  // Authentication Service URI\n  URL url = new URL(\"http://localhost:8086/api/authentication\");\n\n  // AUthentication Request\n  HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n  connection.setDoOutput(true);\n  connection.setRequestMethod(\"POST\");\n  connection.setRequestProperty(\"Content-Type\", \"application/json\");\n\n  String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\";\n  OutputStream os = connection.getOutputStream();\n  os.write(input.getBytes());\n  os.flush();\n\n  if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n    throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode());\n  }\n\n  connection.disconnect();\n\n  // A JWT Token is return in the Header of the response\n  return connection.getHeaderField(\"Authorization\");\n}  REST Service Call  curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication",
            "title": "Platform Authentication"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/consume-scava-rest-services/#service-consumption",
            "text": "To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request.  ```java\n// Service URL\nURL url = new URL(\"http://localhost:8086/api/users\");\nHttpURLConnection connection = (HttpURLConnection) url.openConnection();\nconnection.setRequestMethod(\"GET\");  // Add Token to the request header\nconnection.setRequestProperty(\"Authorization\",token);",
            "title": "Service Consumption"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/",
            "text": "Technical Guidelignes\n\n\nTutorial and Guidelignes related to the various technologies used by the SCAVA Platform\n\n\n\n\nHow to configure the API Gateway in order to integrate a new REST service\n\n\nHow to consume a SCAVA REST services\n\n\nHow to implement Restlet services\n\n\nHow to extend the SCAVA data model\n\n\nHow to generate REST API Documentation\n\n\nHow to access to MongoDB using PONGO",
            "title": "Home"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/#technical-guidelignes",
            "text": "Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform   How to configure the API Gateway in order to integrate a new REST service  How to consume a SCAVA REST services  How to implement Restlet services  How to extend the SCAVA data model  How to generate REST API Documentation  How to access to MongoDB using PONGO",
            "title": "Technical Guidelignes"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/rest-api-doc-generation/",
            "text": "How to generate REST API Documentation\n\n\nREST API Tutorial files\n\n\nInstall\n\n\nFirst of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install.\n\n\nExecute the following steps.\n\n\n\n\nClone repository: \nhttps://github.com/patrickneubauer/crossminer-workflow\n\n\nImport projects from the cloned repo to empty Eclipse workspace:\n\n\nImport root via \nMaven > Existing Maven Projects\n,\n\n\nthen the rest via \nGeneral > Existing projects into Workspace\n \nwithout\n checking \nSearch for nested projects\n.\n\n\nInstall new software packages:\n\n\nInstall Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling \n[web page]\n\n\nInstall Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ \n[update site]\n\n\nUpdate Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ \n[update site]\n\n\nInstall Emfatic: http://download.eclipse.org/emfatic/update/ \n[update site]\n\n\nNote:\n I didn't see the the feature until \nGroup items by category\n was unchecked.\n\n\nInstall GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ \n[update site]\n\n\nImport the given json projects (\norg.eclipse.epsilon.emc.json\n and \norg.eclipse.epsilon.emc.json.dt\n) into the workspace via \nGeneral > Existing projects into Workspace\n.\n\n\nNow right-click on \norg.eclipse.crossmeter.workflow project\n and select \nMaven > Update Project....\n\n\nCopy the give \nM2M_Environment_oxygen.launch\n (for Windows 10) to \norg.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator\n and overwrite the old one.\n\n\nRefresh project \norg.eclipse.crossmeter.workflow.restmule.generator\n in Eclipse.\n\n\nBefore running the \n.launch\n file, right-click on it, \nRun as > Run configurations....\n Here go to \nPlug-ins\n tab and click on \nAdd Required Plug-ins\n.\n\n\nNow you can run the \n.launch\n file TBA:How?\n\n\nIn the Runtime Eclipse import the \norg.eclipse.crossmeter.workflow.restmule.generator\n project as Maven Project.\n\n\nRun \ngenerateFromOAS.launch\n as you can see in the videos https://youtu.be/BJXuozHJPeg.\n\n\nNow, you should see it generating the project.\n\n\nFor further steps, watch Patrick's videos .\n\n\ngithub client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg\n\n\ngithub client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI\n\n\ngithub client generation example \u2014 https://youtu.be/ltSNnSZRETA\n\n\nkafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc\n\n\nkafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4\n\n\nkafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA\n\n\nMDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk\n\n\n\n\nExample\n\n\nI've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: \n/square/{number}\n. You can replace the \n{number}\n with any number. The repsonse for this request is a \nJSON\n object, which looks like this:\n\n\n{\n    \"squared\": {squaredValue}\n}\n\n\n\n\nIt contains only one field, named \nsquared\n, which has the value of the square of the given number. From the specification the generator will provide us a \nJava Project\n which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines:\n\n\nITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings).\nint number = 7; //The number to be squared.\nIData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server.\nNumberValue numberValue; //NumberValue is the class which represents the object received from the server\nnumberValue = squared.observe().blockingSingle(); //Get the actually received object from the response.\nSystem.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer.\n\n\n\n\nEverything, including the \nIEntityApi\n interface and the \nNumberValue\n class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through \nJava\n to the objects/field inside of the received \nJSON\n object. And once again, they are all generated from the OpenAPI specification.\n\n\nSo, to generate the mentioned \nJava Project\n, you have to put your OpenAPI specification file into the \nschemas\n folder inside the \norg.eclipse.crossmeter.workflow.restmule.generator\n project. I'll call mine as \nTestAPI.json\n, and it's made up of the following:\n\n\n{\n  \"swagger\": \"2.0\",\n  \"schemes\": [\n    \"http\"\n  ],\n  \"host\": \"localhost:8080\",\n  \"basePath\": \"/\",\n  \"info\": {\n    \"description\": \"This is a test API, only for demonstration of the generator.\",\n    \"termsOfService\": \"\",\n    \"title\": \"TestAPI\",\n    \"version\": \"v1\"\n  },\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"securityDefinitions\": {\n\n  },\n  \"paths\": {\n    \"/square/{number}\": {\n      \"get\": {\n        \"description\": \"Square a number.\",\n        \"parameters\": [\n          {\n            \"description\": \"The number to be squared\",\n            \"in\": \"path\",\n            \"name\": \"number\",\n            \"required\": true,\n            \"type\": \"integer\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/NumberValue\"\n            }\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"NumberValue\": {\n      \"properties\": {\n        \"squared\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"type\": \"object\"\n    }\n  }\n}\n\n\n\n\nYou can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own.\nThen we have to set the generator to use our new schema. To do this, open the \nbuild.xml\n in the generator project and modify the \napi\n and \njson.model.file\n property to \ntestapi\n and \nschemas/TestAPI.json\n, respectively.\n\n\n<!--API Variables -->\n<property name=\"api\" value=\"testapi\" />\n<property name=\"json.model.file\" value=\"schemas/TestAPI.json\" />\n\n\n\n\nWe are almost done, but we have to take one more small step. We have to provide an \n.eol\n file in the \nepsilon/util/fix/\n folder in the generator project. Its name must be the same as the value of the \napi\n property in the previous step, so for this example we use the \ntestapi.eol\n. The content of this file:\n\n\nimport \"../restmule.eol\";\n\nvar api = RestMule!API.all.first();\n\n// RATE LIMITS\n\nvar search = new RestMule!RatePolicyScope;\nsearch.scope = \"Search\";\nvar entity = new RestMule!RatePolicyScope;\nentity.scope = \"Entity\";\n\n// RATE POLICY\n\nvar policy = new RestMule!RatePolicy;\npolicy.scopes.add(entity);\npolicy.scopes.add(search);\n\nvar reset = new RestMule!ResponseHeader;\nvar resetInt = new RestMule!TInteger;\nresetInt.label = \"X-RateLimit-Reset\";\nreset.type = resetInt;\npolicy.reset = reset;\n\nvar limit = new RestMule!ResponseHeader;\nvar limitInt = new RestMule!TInteger;\nlimitInt.label = \"X-RateLimit-Limit\";\nlimit.type = limitInt;\npolicy.limit = limit;\n\nvar remaining = new RestMule!ResponseHeader;\nvar remainingInt = new RestMule!TInteger;\nremainingInt.label = \"X-RateLimit-Remaining\";\nremaining.type = remainingInt;\npolicy.remaining = remaining;\n\napi.ratePolicy = policy;\n\n// PAGINATION\n\nvar pagination= new RestMule!PaginationPolicy;\npagination.start = 1;\npagination.max = 10;\npagination.increment = 1;\npagination.maxPerIteration = 100;\n\nvar perIteration = new RestMule!Query;\nperIteration.description = \"Items per page\";\nperIteration.required = false;\nvar type = new RestMule!TInteger;\ntype.label = \"per_page\";\ntype.name = type.label;\nperIteration.type = type;\npagination.perIteration = perIteration;\n\nvar page = new RestMule!Query;\npage.description = \"Page identifier\";\npage.required = false;\nvar type1 = new RestMule!TInteger;\ntype1.label = \"page\";\ntype1.name = type1.label;\npage.type = type1;\npagination.page = page;\n\nvar link = new RestMule!ResponseHeader;\nlink.description = \"Page links\";\nvar format = new RestMule!TFormattedString;\nformat.label = \"Link\";\nformat.name = format.label;\nlink.type = format;\npagination.links = link;\n\napi.pagination = pagination;\n\n// WRAPPER\n\nvar wrapper = new RestMule!Wrapper;\nwrapper.name = \"Wrapper\";\nvar items = new RestMule!ListType;\nitems.label = \"items\";\nwrapper.items = items;\nwrapper.totalLabel= \"total_count\";\nwrapper.incompleteLabel = \"incomplete_results\";\napi.pageWrapper = wrapper;\n\n// ADD RATE & WRAPPER TO REQUESTS (FIXME)\n\nfor (r in RestMule!Request.all){\n    if (r.parent.path.startsWith(\"/search\")){\n        r.scope = search;\n    } else {\n        r.scope = entity;\n    }\n    r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader)));\n    for (resp in r.responses.select(s|s.responseType <> null)){\n        resp.unwrap();\n    }\n}\n\n/*  //////////\n * OPERATIONS\n *//////////\noperation RestMule!ObjectType hasWrapper() : Boolean {\n    var wrapper = RestMule!Wrapper.all.first;\n    var lists = self.listFields.collect(a|a.label);\n    return (not lists.isEmpty()) and lists\n        .includes(wrapper.items.label);\n}\n\noperation RestMule!Response unwrap() : RestMule!ObjectType{\n    if (self.responseType.instanceOf(ObjectType)){\n        if (self.responseType.hasWrapper()){\n            (\"Unwrapping : \"+ self.responseType.name).println;\n            var wrapper = RestMule!Wrapper.all.first;\n            var name = self.responseType.name.println;\n            self.responseType = self.responseType.println.listFields\n                .select(b| b.label == wrapper.items.label).first.elements.first;            \n            self.responseType.name = name;\n            self.pageWrapped = true;\n            self.responseType.description = \"UNWRAPPED: \" + self.responseType.description;\n        }\n    }\n}\n\n\n\n\nAfter this, we can run the generator and it will generate our \nJava Project\n from the specification.\n\n\nIn the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.",
            "title": "Rest api doc generation"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/rest-api-doc-generation/#how-to-generate-rest-api-documentation",
            "text": "REST API Tutorial files",
            "title": "How to generate REST API Documentation"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/rest-api-doc-generation/#install",
            "text": "First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install.  Execute the following steps.   Clone repository:  https://github.com/patrickneubauer/crossminer-workflow  Import projects from the cloned repo to empty Eclipse workspace:  Import root via  Maven > Existing Maven Projects ,  then the rest via  General > Existing projects into Workspace   without  checking  Search for nested projects .  Install new software packages:  Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling  [web page]  Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/  [update site]  Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/  [update site]  Install Emfatic: http://download.eclipse.org/emfatic/update/  [update site]  Note:  I didn't see the the feature until  Group items by category  was unchecked.  Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/  [update site]  Import the given json projects ( org.eclipse.epsilon.emc.json  and  org.eclipse.epsilon.emc.json.dt ) into the workspace via  General > Existing projects into Workspace .  Now right-click on  org.eclipse.crossmeter.workflow project  and select  Maven > Update Project....  Copy the give  M2M_Environment_oxygen.launch  (for Windows 10) to  org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator  and overwrite the old one.  Refresh project  org.eclipse.crossmeter.workflow.restmule.generator  in Eclipse.  Before running the  .launch  file, right-click on it,  Run as > Run configurations....  Here go to  Plug-ins  tab and click on  Add Required Plug-ins .  Now you can run the  .launch  file TBA:How?  In the Runtime Eclipse import the  org.eclipse.crossmeter.workflow.restmule.generator  project as Maven Project.  Run  generateFromOAS.launch  as you can see in the videos https://youtu.be/BJXuozHJPeg.  Now, you should see it generating the project.  For further steps, watch Patrick's videos .  github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg  github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI  github client generation example \u2014 https://youtu.be/ltSNnSZRETA  kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc  kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4  kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA  MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk",
            "title": "Install"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/rest-api-doc-generation/#example",
            "text": "I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following:  /square/{number} . You can replace the  {number}  with any number. The repsonse for this request is a  JSON  object, which looks like this:  {\n    \"squared\": {squaredValue}\n}  It contains only one field, named  squared , which has the value of the square of the given number. From the specification the generator will provide us a  Java Project  which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines:  ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings).\nint number = 7; //The number to be squared.\nIData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server.\nNumberValue numberValue; //NumberValue is the class which represents the object received from the server\nnumberValue = squared.observe().blockingSingle(); //Get the actually received object from the response.\nSystem.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer.  Everything, including the  IEntityApi  interface and the  NumberValue  class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through  Java  to the objects/field inside of the received  JSON  object. And once again, they are all generated from the OpenAPI specification.  So, to generate the mentioned  Java Project , you have to put your OpenAPI specification file into the  schemas  folder inside the  org.eclipse.crossmeter.workflow.restmule.generator  project. I'll call mine as  TestAPI.json , and it's made up of the following:  {\n  \"swagger\": \"2.0\",\n  \"schemes\": [\n    \"http\"\n  ],\n  \"host\": \"localhost:8080\",\n  \"basePath\": \"/\",\n  \"info\": {\n    \"description\": \"This is a test API, only for demonstration of the generator.\",\n    \"termsOfService\": \"\",\n    \"title\": \"TestAPI\",\n    \"version\": \"v1\"\n  },\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"securityDefinitions\": {\n\n  },\n  \"paths\": {\n    \"/square/{number}\": {\n      \"get\": {\n        \"description\": \"Square a number.\",\n        \"parameters\": [\n          {\n            \"description\": \"The number to be squared\",\n            \"in\": \"path\",\n            \"name\": \"number\",\n            \"required\": true,\n            \"type\": \"integer\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/NumberValue\"\n            }\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"NumberValue\": {\n      \"properties\": {\n        \"squared\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"type\": \"object\"\n    }\n  }\n}  You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own.\nThen we have to set the generator to use our new schema. To do this, open the  build.xml  in the generator project and modify the  api  and  json.model.file  property to  testapi  and  schemas/TestAPI.json , respectively.  <!--API Variables -->\n<property name=\"api\" value=\"testapi\" />\n<property name=\"json.model.file\" value=\"schemas/TestAPI.json\" />  We are almost done, but we have to take one more small step. We have to provide an  .eol  file in the  epsilon/util/fix/  folder in the generator project. Its name must be the same as the value of the  api  property in the previous step, so for this example we use the  testapi.eol . The content of this file:  import \"../restmule.eol\";\n\nvar api = RestMule!API.all.first();\n\n// RATE LIMITS\n\nvar search = new RestMule!RatePolicyScope;\nsearch.scope = \"Search\";\nvar entity = new RestMule!RatePolicyScope;\nentity.scope = \"Entity\";\n\n// RATE POLICY\n\nvar policy = new RestMule!RatePolicy;\npolicy.scopes.add(entity);\npolicy.scopes.add(search);\n\nvar reset = new RestMule!ResponseHeader;\nvar resetInt = new RestMule!TInteger;\nresetInt.label = \"X-RateLimit-Reset\";\nreset.type = resetInt;\npolicy.reset = reset;\n\nvar limit = new RestMule!ResponseHeader;\nvar limitInt = new RestMule!TInteger;\nlimitInt.label = \"X-RateLimit-Limit\";\nlimit.type = limitInt;\npolicy.limit = limit;\n\nvar remaining = new RestMule!ResponseHeader;\nvar remainingInt = new RestMule!TInteger;\nremainingInt.label = \"X-RateLimit-Remaining\";\nremaining.type = remainingInt;\npolicy.remaining = remaining;\n\napi.ratePolicy = policy;\n\n// PAGINATION\n\nvar pagination= new RestMule!PaginationPolicy;\npagination.start = 1;\npagination.max = 10;\npagination.increment = 1;\npagination.maxPerIteration = 100;\n\nvar perIteration = new RestMule!Query;\nperIteration.description = \"Items per page\";\nperIteration.required = false;\nvar type = new RestMule!TInteger;\ntype.label = \"per_page\";\ntype.name = type.label;\nperIteration.type = type;\npagination.perIteration = perIteration;\n\nvar page = new RestMule!Query;\npage.description = \"Page identifier\";\npage.required = false;\nvar type1 = new RestMule!TInteger;\ntype1.label = \"page\";\ntype1.name = type1.label;\npage.type = type1;\npagination.page = page;\n\nvar link = new RestMule!ResponseHeader;\nlink.description = \"Page links\";\nvar format = new RestMule!TFormattedString;\nformat.label = \"Link\";\nformat.name = format.label;\nlink.type = format;\npagination.links = link;\n\napi.pagination = pagination;\n\n// WRAPPER\n\nvar wrapper = new RestMule!Wrapper;\nwrapper.name = \"Wrapper\";\nvar items = new RestMule!ListType;\nitems.label = \"items\";\nwrapper.items = items;\nwrapper.totalLabel= \"total_count\";\nwrapper.incompleteLabel = \"incomplete_results\";\napi.pageWrapper = wrapper;\n\n// ADD RATE & WRAPPER TO REQUESTS (FIXME)\n\nfor (r in RestMule!Request.all){\n    if (r.parent.path.startsWith(\"/search\")){\n        r.scope = search;\n    } else {\n        r.scope = entity;\n    }\n    r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader)));\n    for (resp in r.responses.select(s|s.responseType <> null)){\n        resp.unwrap();\n    }\n}\n\n/*  //////////\n * OPERATIONS\n *//////////\noperation RestMule!ObjectType hasWrapper() : Boolean {\n    var wrapper = RestMule!Wrapper.all.first;\n    var lists = self.listFields.collect(a|a.label);\n    return (not lists.isEmpty()) and lists\n        .includes(wrapper.items.label);\n}\n\noperation RestMule!Response unwrap() : RestMule!ObjectType{\n    if (self.responseType.instanceOf(ObjectType)){\n        if (self.responseType.hasWrapper()){\n            (\"Unwrapping : \"+ self.responseType.name).println;\n            var wrapper = RestMule!Wrapper.all.first;\n            var name = self.responseType.name.println;\n            self.responseType = self.responseType.println.listFields\n                .select(b| b.label == wrapper.items.label).first.elements.first;            \n            self.responseType.name = name;\n            self.pageWrapped = true;\n            self.responseType.description = \"UNWRAPPED: \" + self.responseType.description;\n        }\n    }\n}  After this, we can run the generator and it will generate our  Java Project  from the specification.  In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.",
            "title": "Example"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/",
            "text": "How to implement Restlet services\n\n\nWhen to use this guideline ?\n\n\nThis guideline present how to create a new REST service using the RESTLET framework in the Scava platform.\n\n\nContext\n\n\nScava project manages REST services with the RESTLET framework.\n\n\nThe usage of Restlet framework  has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin :\n\n\n\n\norg.eclipse.crossmeter.platform.client.api.  \n\n\n\n\nThe RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.\n\n\nYou want to access to create a new REST Service ?\n\n\n1. Create a new Route\n\n\nTo  register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.\n\n\nNaming the Route\n\n\nThe routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : \nNaming-Scava-REST-Services.html\n\n\nRegister the Route\n\n\nThe \norg.scava.platform.services\n plug-in contained the class \nPlatformRoute.java\n  responsible for declaring routes.\n\n\npackage org.scava.platform.services;\nimport org.restlet.Application;\nimport org.restlet.Restlet;\nimport org.restlet.routing.Router;\n\npublic class PlatformRoute extends Application {\n    @Override\n    public Restlet createInboundRoot() {\n        Router router = new Router(getContext());\n\n        router.attach(\"/\", PingResource.class);     \n        router.attach(\"/search\", SearchProjectResource.class);\n                ...\n                router.attach(\"/projects/p/{projectid}\", ProjectResource.class);\n        router.attach(\"/raw/metrics\", RawMetricListResource.class);\n        ...\n        return router;\n    }\n}\n\n\n\n\nRoute Example :\n\n\nrouter.attach(\"/raw/metrics\", RawMetricListResource.class);\n\n\n\n\n\n\n\"/raw/metrics\"\n: Represent the route URL.\n\n\n\"RawMetricListResource.class\"\n : Represent the class where the service to be implemented for this path.\n\n\n\n\nA route can contained some parameters. In this case, parameters are identified by a name with curly brackets \n{}\n.\n\n\n2. Implement the Service\n\n\nA service implementation is a Java class which extend the \nServerResource\n class provided by the RESTLET framework.\nTo create a new service create a new Class :\n\n Named \"\nServiceName\n\" + Resource.  Ex : ProjectCreationResource.java\n\n On a namespace based on the  route. Ex : org.scava.platform.services.administration for platform administration services.\n* Who extend the org.restlet.resource.ServerResource class.\n\n\nGET Service\n\n\nTo implement a service of type GET, create a new method :\n\n Based on the following signature : public final Representation represent()\n\n Add the @Get(\"json\") annotation\n\n\n\n@Get(\"json\")\npublic final Representation represent() {\n  // Initialise Response Header\n  Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\");\n   if (responseHeaders == null) {\n    responseHeaders = new Series(Header.class);\n     getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders);\n  }\n  responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\"));\n  responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\"));\n\n  // Get Route parameter if required {projectid}\n  String projectId = (String) getRequest().getAttributes().get(\"projectid\");\n\n\n  try {\n    ....\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_OK);\n    return new StringRepresentation(...);\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n    return rep;\n  }\n}\n\n\n\n\n\nYou can also extend the \nAbstractApiResource\n , a service class  provided by the platform and dedicate to services who request a connection to MongoDb database instead of the \nServerResource\n. In this case you will have to implement the doRepresent() method.\n\n\npublic class RawMetricListResource extends AbstractApiResource {\n    public Representation doRepresent() {\n        ObjectNode res = mapper.createObjectNode();\n\n        ArrayNode metrics = mapper.createArrayNode();\n        res.put(\"metrics\", metrics);\n        ...\n        return Util.createJsonRepresentation(res);\n    }\n}\n\n\n\n\nPOST Service\n\n\nTo implement a service of type POST, crate a new method :\n\n Based on the following signature : public Representation \nmyServiceName\n (Representation entity)\n\n Add the @Post annotation\n\n\n@Post\npublic Representation myServiceName(Representation entity) {\n  try {\n    // Read Json Datas\n    JsonNode json = mapper.readTree(entity.getText());\n\n    ...\n\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_CREATED);\n    return new StringRepresentation(...);\n\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n   return rep;\n  }\n}\n\n\n\n\nDELETE Service\n\n\nTo do ....\n\n\n3. Document the Service\n\n\nThe REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to  allow an easy integration, this contract must be documented :",
            "title": "Restlet service implementation"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#how-to-implement-restlet-services",
            "text": "",
            "title": "How to implement Restlet services"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#when-to-use-this-guideline",
            "text": "This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#context",
            "text": "Scava project manages REST services with the RESTLET framework.  The usage of Restlet framework  has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin :   org.eclipse.crossmeter.platform.client.api.     The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.",
            "title": "Context"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#you-want-to-access-to-create-a-new-rest-service",
            "text": "",
            "title": "You want to access to create a new REST Service ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#1-create-a-new-route",
            "text": "To  register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.",
            "title": "1. Create a new Route"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#naming-the-route",
            "text": "The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service :  Naming-Scava-REST-Services.html",
            "title": "Naming the Route"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#register-the-route",
            "text": "The  org.scava.platform.services  plug-in contained the class  PlatformRoute.java   responsible for declaring routes.  package org.scava.platform.services;\nimport org.restlet.Application;\nimport org.restlet.Restlet;\nimport org.restlet.routing.Router;\n\npublic class PlatformRoute extends Application {\n    @Override\n    public Restlet createInboundRoot() {\n        Router router = new Router(getContext());\n\n        router.attach(\"/\", PingResource.class);     \n        router.attach(\"/search\", SearchProjectResource.class);\n                ...\n                router.attach(\"/projects/p/{projectid}\", ProjectResource.class);\n        router.attach(\"/raw/metrics\", RawMetricListResource.class);\n        ...\n        return router;\n    }\n}  Route Example :  router.attach(\"/raw/metrics\", RawMetricListResource.class);   \"/raw/metrics\" : Represent the route URL.  \"RawMetricListResource.class\"  : Represent the class where the service to be implemented for this path.   A route can contained some parameters. In this case, parameters are identified by a name with curly brackets  {} .",
            "title": "Register the Route"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#2-implement-the-service",
            "text": "A service implementation is a Java class which extend the  ServerResource  class provided by the RESTLET framework.\nTo create a new service create a new Class :  Named \" ServiceName \" + Resource.  Ex : ProjectCreationResource.java  On a namespace based on the  route. Ex : org.scava.platform.services.administration for platform administration services.\n* Who extend the org.restlet.resource.ServerResource class.",
            "title": "2. Implement the Service"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#get-service",
            "text": "To implement a service of type GET, create a new method :  Based on the following signature : public final Representation represent()  Add the @Get(\"json\") annotation  \n@Get(\"json\")\npublic final Representation represent() {\n  // Initialise Response Header\n  Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\");\n   if (responseHeaders == null) {\n    responseHeaders = new Series(Header.class);\n     getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders);\n  }\n  responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\"));\n  responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\"));\n\n  // Get Route parameter if required {projectid}\n  String projectId = (String) getRequest().getAttributes().get(\"projectid\");\n\n\n  try {\n    ....\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_OK);\n    return new StringRepresentation(...);\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n    return rep;\n  }\n}  You can also extend the  AbstractApiResource  , a service class  provided by the platform and dedicate to services who request a connection to MongoDb database instead of the  ServerResource . In this case you will have to implement the doRepresent() method.  public class RawMetricListResource extends AbstractApiResource {\n    public Representation doRepresent() {\n        ObjectNode res = mapper.createObjectNode();\n\n        ArrayNode metrics = mapper.createArrayNode();\n        res.put(\"metrics\", metrics);\n        ...\n        return Util.createJsonRepresentation(res);\n    }\n}",
            "title": "GET Service"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#post-service",
            "text": "To implement a service of type POST, crate a new method :  Based on the following signature : public Representation  myServiceName  (Representation entity)  Add the @Post annotation  @Post\npublic Representation myServiceName(Representation entity) {\n  try {\n    // Read Json Datas\n    JsonNode json = mapper.readTree(entity.getText());\n\n    ...\n\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_CREATED);\n    return new StringRepresentation(...);\n\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n   return rep;\n  }\n}",
            "title": "POST Service"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#delete-service",
            "text": "To do ....",
            "title": "DELETE Service"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/restlet-service-implementation/#3-document-the-service",
            "text": "The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to  allow an easy integration, this contract must be documented :",
            "title": "3. Document the Service"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/",
            "text": "How to extend the SCAVA data model\n\n\nWhen to use ?\n\n\nIn this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order  the data layer of Scava platform during the evolution.\n\n\nContext\n\n\nThe Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class.\n\n\nEach MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model.\n\n\nThe current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins :\n\n\n\n\nThe Platform data model (org.ossmeter.repository.model)\n : Contains data related to project administration ,  metric execution process and authentification system.\n\n\nSource Code Repositroy managers (org.ossmeter.repository.model.'project delta manager')\n : Contains configuration informations related to source codes managemeny tools.\n\n\nMetric Providers data models (in each metric provider plugins)\n :  Each metric provider implementation contains his once data model.\n\n\n\n\nYou need to Extend an Existing Data Model ?\n\n\nThe first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.\n\n\n1. Locate the *.emf file of this data model\n\n\nA presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model.\n\n\nEx : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)\n\n\n2. Update the Data  Model description\n\n\nA data model description file contains a statical description of a MongoDb document.\n\n\n@db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n    attr String stackTrace;\n}\n...\n\n\n\n\nIf we would like to add one more attribute to the element \nProjectError\n, we could add it this way :\n\n\n@db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n        .\n        .\n    attr String stackTrace;\n+       attr String TestAttribute;\n}\n...\n\n\n\n\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines\n\n\n3. Generate the Java Class using the Pongo Tool.\n\n\n\n\nDownload the Pongo tool : https://github.com/kolovos/pongo/releases\n\n\nRun the Pongo generator from the command line as follows: \njava -jar pongo.jar  youremffile.emf\n\n\nReplace the existing Java class by the new generated java class.\n\n\n\n\nMore information about Pongo  : https://github.com/kolovos/pongo/wiki\n\n\nYou need to Create a new Data Model ?\n\n\nThe second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model.\n\n\nIn this case, we invite you to create a new plugin which will contain your data model.\n\n\n1. Create a new Eclipse Plug-In\n\n\n\n\nCreate a new Eclipse Plug-In Project (\n\n\nIn Eclipse Toolbar : File > New > Plug-In Project\n\n\nName of the project : org.scava.\nmycomponent\n.repository.model\n\n\nDisable the generation of an Activator Class / contribution to the ui\n\n\nEdit the MANIFEST.MF file\n\n\nIn Dependency : add a dependency to the \norg.eclipse.core.runtime\n plugin\n\n\nIn Dependency : add a dependency to the \ncom.googlecode.pongo.runtime\n plugin\n\n\nIn Dependency : add a dependency to the \norg.apache.commons.lang3\n plugin\n\n\nIn Extentions : reference an extension point named \ncom.googlecode.pongo.runtime.osgi\n\n\nIn source directory\n\n\nCreate a package named org.scava.\nmycomponent\n.repository.model\n\n\nIn this package create an emf file named \nmycomponent.emf\n\n\n\n\nA presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file :\n\n\n```package org.scava.mycomponent.repository.model;\n\n\n@db\nclass MyComponent {\n     ....\n}\n```\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines\n\n\n2. Generate the Java Class using the Pongo Tool.\n\n\n\n\nDownload the Pongo tool : https://github.com/kolovos/pongo/releases\n\n\nRun the Pongo generator from the command line as follows: \njava -jar pongo.jar  youremffile.emf\n\n\nAdd this class in your org.scava.\nmycomponent\n.repository.model package\n\n\n\n\nMore information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "Scava data model extention"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#how-to-extend-the-scava-data-model",
            "text": "",
            "title": "How to extend the SCAVA data model"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#when-to-use",
            "text": "In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order  the data layer of Scava platform during the evolution.",
            "title": "When to use ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#context",
            "text": "The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class.  Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model.  The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins :   The Platform data model (org.ossmeter.repository.model)  : Contains data related to project administration ,  metric execution process and authentification system.  Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager')  : Contains configuration informations related to source codes managemeny tools.  Metric Providers data models (in each metric provider plugins)  :  Each metric provider implementation contains his once data model.",
            "title": "Context"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#you-need-to-extend-an-existing-data-model",
            "text": "The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.",
            "title": "You need to Extend an Existing Data Model ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#1-locate-the-emf-file-of-this-data-model",
            "text": "A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model.  Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)",
            "title": "1. Locate the *.emf file of this data model"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#2-update-the-data-model-description",
            "text": "A data model description file contains a statical description of a MongoDb document.  @db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n    attr String stackTrace;\n}\n...  If we would like to add one more attribute to the element  ProjectError , we could add it this way :  @db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n        .\n        .\n    attr String stackTrace;\n+       attr String TestAttribute;\n}\n...  You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines",
            "title": "2. Update the Data  Model description"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#3-generate-the-java-class-using-the-pongo-tool",
            "text": "Download the Pongo tool : https://github.com/kolovos/pongo/releases  Run the Pongo generator from the command line as follows:  java -jar pongo.jar  youremffile.emf  Replace the existing Java class by the new generated java class.   More information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "3. Generate the Java Class using the Pongo Tool."
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#you-need-to-create-a-new-data-model",
            "text": "The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model.  In this case, we invite you to create a new plugin which will contain your data model.",
            "title": "You need to Create a new Data Model ?"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#1-create-a-new-eclipse-plug-in",
            "text": "Create a new Eclipse Plug-In Project (  In Eclipse Toolbar : File > New > Plug-In Project  Name of the project : org.scava. mycomponent .repository.model  Disable the generation of an Activator Class / contribution to the ui  Edit the MANIFEST.MF file  In Dependency : add a dependency to the  org.eclipse.core.runtime  plugin  In Dependency : add a dependency to the  com.googlecode.pongo.runtime  plugin  In Dependency : add a dependency to the  org.apache.commons.lang3  plugin  In Extentions : reference an extension point named  com.googlecode.pongo.runtime.osgi  In source directory  Create a package named org.scava. mycomponent .repository.model  In this package create an emf file named  mycomponent.emf   A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file :  ```package org.scava.mycomponent.repository.model;  @db\nclass MyComponent {\n     ....\n}\n```\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines",
            "title": "1. Create a new Eclipse Plug-In"
        },
        {
            "location": "/contributors-guide/technical-guidelignes/scava-data-model-extention/#2-generate-the-java-class-using-the-pongo-tool",
            "text": "Download the Pongo tool : https://github.com/kolovos/pongo/releases  Run the Pongo generator from the command line as follows:  java -jar pongo.jar  youremffile.emf  Add this class in your org.scava. mycomponent .repository.model package   More information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "2. Generate the Java Class using the Pongo Tool."
        },
        {
            "location": "/developers-guide/",
            "text": "SCAVA Developers Guide\n\n\nThe developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.\n\n\nRunning SCAVA Platform form Sources\n\n\nThe following section provide informations related to how the main platform components can be run from sources in developers mode\n\n\n\n\nAnalysis Platform\n \n\n\nAdministration Application\n \n\n\nVisualisation Dashboard\n \n\n\nEclipse Plugin\n\n\n\n\nMetric Provider Development Guide\n\n\nThe following section provide the key informations required to develop a new metric provider and to integrate it on the platforme\n\n\nREST API Reference Guide\n\n\nReference Guide of REST API exposed by the main platform components\n\n\n\n\nAccess to the REST API via the API Gateway\n\n\nAnalysis Platform\n \n\n\nKnowledge Base\n \n\n\nVisualisation Dashboard\n \n\n\nWorkflow Engine",
            "title": "Home"
        },
        {
            "location": "/developers-guide/#scava-developers-guide",
            "text": "The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.",
            "title": "SCAVA Developers Guide"
        },
        {
            "location": "/developers-guide/#running-scava-platform-form-sources",
            "text": "The following section provide informations related to how the main platform components can be run from sources in developers mode   Analysis Platform    Administration Application    Visualisation Dashboard    Eclipse Plugin",
            "title": "Running SCAVA Platform form Sources"
        },
        {
            "location": "/developers-guide/#metric-provider-development-guide",
            "text": "The following section provide the key informations required to develop a new metric provider and to integrate it on the platforme",
            "title": "Metric Provider Development Guide"
        },
        {
            "location": "/developers-guide/#rest-api-reference-guide",
            "text": "Reference Guide of REST API exposed by the main platform components   Access to the REST API via the API Gateway  Analysis Platform    Knowledge Base    Visualisation Dashboard    Workflow Engine",
            "title": "REST API Reference Guide"
        },
        {
            "location": "/developers-guide/api-reference-guide/analysis-platform/",
            "text": "Analysis Platform REST API reference Guide\n\n\nThe MetricPlatform APIs documentation generated with swagger UI is available : \n\nSCAVA MetricPlatform APIs",
            "title": "Analysis platform"
        },
        {
            "location": "/developers-guide/api-reference-guide/analysis-platform/#analysis-platform-rest-api-reference-guide",
            "text": "The MetricPlatform APIs documentation generated with swagger UI is available :  SCAVA MetricPlatform APIs",
            "title": "Analysis Platform REST API reference Guide"
        },
        {
            "location": "/developers-guide/api-reference-guide/api-gateway/",
            "text": "Access to the REST API via the API Gateway",
            "title": "Api gateway"
        },
        {
            "location": "/developers-guide/api-reference-guide/api-gateway/#access-to-the-rest-api-via-the-api-gateway",
            "text": "",
            "title": "Access to the REST API via the API Gateway"
        },
        {
            "location": "/developers-guide/api-reference-guide/",
            "text": "REST API Reference Guide\n\n\nReference Guide of REST API exposed by the main platform components\n\n\n\n\nAccess to the REST API via the API Gateway\n (Softeam)\n\n\nAnalysis Platform\n (Softeam)\n\n\nKnowledge Base\n (UDA)\n\n\nVisualisation Dashboard\n (Bitergia)\n\n\nWorkflow Engine\n (York)",
            "title": "Home"
        },
        {
            "location": "/developers-guide/api-reference-guide/#rest-api-reference-guide",
            "text": "Reference Guide of REST API exposed by the main platform components   Access to the REST API via the API Gateway  (Softeam)  Analysis Platform  (Softeam)  Knowledge Base  (UDA)  Visualisation Dashboard  (Bitergia)  Workflow Engine  (York)",
            "title": "REST API Reference Guide"
        },
        {
            "location": "/developers-guide/api-reference-guide/knowledge-base/",
            "text": "Knowledge Base REST API reference Guide\n\n\nThe MetricPlatform APIs documentation generated with swagger UI is available : \nSCAVA KB APIs",
            "title": "Knowledge base"
        },
        {
            "location": "/developers-guide/api-reference-guide/knowledge-base/#knowledge-base-rest-api-reference-guide",
            "text": "The MetricPlatform APIs documentation generated with swagger UI is available :  SCAVA KB APIs",
            "title": "Knowledge Base REST API reference Guide"
        },
        {
            "location": "/developers-guide/api-reference-guide/visualisation-dashboard/",
            "text": "Visualisation Platform REST API reference Guide",
            "title": "Visualisation dashboard"
        },
        {
            "location": "/developers-guide/api-reference-guide/visualisation-dashboard/#visualisation-platform-rest-api-reference-guide",
            "text": "",
            "title": "Visualisation Platform REST API reference Guide"
        },
        {
            "location": "/developers-guide/api-reference-guide/workflow-engine/",
            "text": "Workflow Engine REST API reference Guide",
            "title": "Workflow engine"
        },
        {
            "location": "/developers-guide/api-reference-guide/workflow-engine/#workflow-engine-rest-api-reference-guide",
            "text": "",
            "title": "Workflow Engine REST API reference Guide"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/",
            "text": "Metric Provider Developement Guide\n\n\nIn this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a \ntransient\n metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a \nhistorical\n metric provider, will count the total number of commits over time. \n\n\nWe'll go through the steps above for each metric provider.\n\n\nPre-requisites\n\n\n\n\nEclipse \n\n\nThe \nEmfatic\n plug-in should be installed in your Eclipse\n\n\nThe \nPongo\n plug-in should be installed in your Eclipse\n\n\nThe OSSMETER source code should be in your workspace\n\n\n\n\nThe Transient Metric Provider\n\n\nThis metric provider will store a complete history of the commits in the version control system(s) used by a project. \n\n\n0. Setup\n\n\nCreate a new Plugin project in Eclipse.\n\n\n\n\nGo to File > New > Project... and select 'Plug-in project'\n\n\nGive the project an appropriate name. The OSSMETER naming convention is:\n\n\nTransient metrics: org.ossmeter.metricprovider.trans.(metric name)\n\n\nHistorical metrics: org.ossmeter.metricprovider.historical.(metric name)\n\n\n\n\n\n\nClick Next\n\n\nIf the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it\n\n\nClick Finish\n\n\nOpen up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.\n\n\n\n\n1. The data model\n\n\nWe define the data model using the Emfatic language. In your newly created plug-in, create a package called \norg.ossmeter.metricprovider.trans.commits.model\n. In that package create an empty file called \ncommits.emf\n. In this file, we will define our data model.\n\n\nFirst of all, we need to state the name of the package.\n\n\npackage org.ossmeter.metricprovider.trans.commits.model;\n\n\n\n\nThis is used by the Pongo code generator - the generated classes will be put in this package.\n\n\nWe then define the database for our model:\n\n\n@db(qualifiedCollectionNames=\"true\")\nclass Commits {\n    val Repository[*] repositories;\n    val Commit[*] commits;\n}\n\n\n\n\nThe \n@db\n annotation tells Pongo that this will be the container database for the data. Adding the \nqualifiedCollectionNames=true\n property will prepend the database name to all Mongo collections. \n\n\nThe \nCommits\n class above says that we want a database with two collections, name \nrepositories\n and \ncommits\n. If \nqualifiedCollectionNames\n is set to \ntrue\n, the collections will be named \nCommits.repositories\n and \nCommits.commits\n. \n\n\nWe now define the schema of the \nCommits.repositories\n collection:\n\n\n~~~java\nclass Repository {\n    @searchable\n    attr String url;\n    attr String repoType;\n    attr String revision;\n    attr int totalNumberOfCommits;\n    ref CommitData[*] commits;\n}\n\n\n\nThis class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection.\n\nThe `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time.\n\nEach commit is represented in the `Commits.commits` collection by the following model:\n\n~~~~java\nclass Commit {\n    @searchable\n    attr Date date;\n    attr String identifier;\n    attr String message;\n    attr String author;\n}\n\n\n\n\nFor each commit, we store its \ndate\n, \nidentifier\n (revision ID), the commit \nmessage\n, and the \nauthor\n. We also create an index on the \ndate\n to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated \nfindByDate(String date)\n or \nfindOneByDate(String date)\n methods).\n\n\nNow we need to use Pongo to generate code from this model. Right-click on the \ncommits.emf\n file and select \nPongo > Generate Pongos and plugin.xml\n. You should see some Java classes appear in your package.\n\n\nNow that we have our data model, we can implement the metric provider.\n\n\n2. The metric provider\n\n\nCreate a Java class called \norg.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider\n\n\nThis class should extend \nAbstractTransientMetricProvider\n and specify \nCommits\n for the generic argument:\n\n\npublic class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits>\n\n\n\n\nThe generic argument states that the metric provider stores objects conforming to the \nCommits\n data model. Note: You do not need to extend \nAbstractTransientMetricProvider\n and can implement \nITransientMetricProvider\n instead should you wish to.\n\n\nThere are a number of methods that need implementing. We will discuss each in turn.\n\n\nadapt(DB db)\n\n\nThis method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows:\n\n\n    @Override\n    public Commits adapt(DB db) {\n        return new Commits(db);\n    }\n\n\n\n\n\nThe next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider:\n\n\n    @Override\n    public String getShortIdentifier() { // This may be deprecated very soon\n        return \"transient-commits\"; \n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Commit History\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"The commit history of the project.\";\n    }\n\n\n\n\nThe next method allows you to declare whether the metric provider is applicable to a given project:\n\n\n@Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }\n\n\n\n\nOur metric applies to any project that has at least one VCS repository.\n\n\nFinally, we have the \nmeasure(...)\n method that performs the actual metric calculation:\n\n\n    @Override\n    public void measure(Project project, ProjectDelta delta, Commits db) {\n\n        for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) {\n            Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl());\n\n            if (repo == null) {\n                repo = new Repository();\n                repo.setUrl(repoDelta.getRepository().getUrl());\n                db.getRepositories().add(repo);\n            }\n\n            for (VcsCommit commit : repoDelta.getCommits()) {\n                Commit c = new Commit();\n                c.setDate(commit.getJavaDate());\n                c.setMessage(commit.getMessage());\n                c.setAuthor(commit.getAuthor());\n                c.setIdentifier(commit.getRevision());\n\n                repo.getCommits().add(c);\n                db.getCommits().add(c);\n            }\n        }\n        db.getCommits().sync();\n        db.getRepositories().sync();\n    }\n\n\n\n\nThe above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new \nCommit\n object is created for each commit in the delta.\n\n\n3. Make the metric provider discoverable\n\n\nMetric providers are registered with the OSSMETER platform using \nextension points\n:\n\n\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.metricprovider\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'CommitsTransientMetricProvider' class\n\n\n\n\nNow everything is ready for the metric to be executed :)\n\n\nThe Historic Metric Provider\n\n\nThis metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.  \n\n\n0. Setup\n\n\nCreate a new Plugin project in Eclipse.  \n\n\n\n\nGo to File > New > Project... and select 'Plug-in project'\n\n\nGive the project an appropriate name. The OSSMETER naming convention is:\n\n\nTransient metrics: org.ossmeter.metricprovider.trans.(metric name)\n\n\nHistorical metrics: org.ossmeter.metricprovider.historical.(metric name)\n\n\n\n\n\n\nClick Next\n\n\nIf the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it\n\n\nClick Finish\n\n\nOpen up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.\n\n\n\n\n1. The data model\n\n\nIn your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model:\n\n\npackage org.ossmeter.metricprovider.historic.commits.model;\n\nclass HistoricCommits {\n    attr int numberOfCommits;\n}\n\n\n\n\nThe data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.\n\n\n2. The metric provider\n\n\nCreate a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider\n\n\nThis class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)):\n\n\npublic class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider\n\nThe generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to.\n\n\nThere are a number of methods that need implementing. We will discuss each in turn.\n\n\nFirst of all, complete the typical information-related methods:\n\n\n    @Override\n    public String getShortIdentifier() {\n        return \"historicalcommits\";\n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Historical commits\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"...\";\n    }\n\n\n\n\nNow complete the standard \nappliesTo\n method:\n\n\n    @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }\n\n\n\n\nWe now need to specify a dependency on the transient metric provider that we just implemented.\n\n\n    @Override\n    public List<String> getIdentifiersOfUses() {\n        return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName());\n    }   \n\n\n\n\nThis tells the platform that we need access to the \nCommitsTransientMetricProvider\n database. The platform will assign this to the \nuses\n field that is available to the historical metric provider, as you'll see in the \nmeasure\n method:\n\n\n    @Override\n    public Pongo measure(Project project) {\n        Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0));\n\n        int commits = (int) transDb.getCommits().size();\n\n        HistoricCommits hist = new HistoricCommits();\n        hist.setNumberOfCommits(commits);\n\n        return hist;\n    }\n\n\n\n\nFirst of all, we get hold of the database of the transient commits metric provider, and simply count the size of the \ncommits\n collection. We save this in an instance of the \nHistoricCommits\n Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.\n\n\n3. Make the metric provider discoverable\n\n\nThis process is the same as the transient metric provider:\n\n\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.metricprovider\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class\n\n\n\n\nNow everything is ready for both metrics to be executed :)\n\n\nBut first. Let's specify how we want this historical metric to be visualised.\n\n\n4. Define a MetVis visualisation specification\n\n\nMetVis\n is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis \nweb page\n has numerous examples of this.\n\n\nCreate a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider:\n\n\n{\n    \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\",\n    \"vis\" : [\n        {\n            \"id\" : \"historicalcommits\",\n            \"name\" : \"Commits over time\",\n            \"description\" : \"This metric shows when the projects commits occurred\",\n            \"type\" : \"LineChart\",\n            \"datatable\" : {\n                \"cols\" : [\n                    { \"name\" : \"Date\", \"field\" : \"$__date\" },\n                    { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" }\n                ]\n            },\n            \"x\" : \"Date\",\n            \"y\" : \"Commits\"\n        }\n    ]\n}\n\n\n\n\nThe \nmetricId\n field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use \n$__date\n. The final two fields (\nx\n and \ny\n) are references to column names in the datatable specification. The \ntype\n states that the data should be plotted as a line chart. You can test your MetVis specifications on the \nMetVis playpen\n.\n\n\n5. Make the visualisation specification discoverable\n\n\nAs with metric providers, visualisation specifications are registered using extension points.\n\n\n\n\nAdd the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.visualisation.metric\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'historicalcommits.json' file\n\n\n\n\nGood job.\n\n\nRunning the metric providers\n\n\nSee \nRunning from Source\n\n\nHomework\n\n\nAdapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.",
            "title": "Home"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#metric-provider-developement-guide",
            "text": "In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a  transient  metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a  historical  metric provider, will count the total number of commits over time.   We'll go through the steps above for each metric provider.",
            "title": "Metric Provider Developement Guide"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#pre-requisites",
            "text": "Eclipse   The  Emfatic  plug-in should be installed in your Eclipse  The  Pongo  plug-in should be installed in your Eclipse  The OSSMETER source code should be in your workspace",
            "title": "Pre-requisites"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#the-transient-metric-provider",
            "text": "This metric provider will store a complete history of the commits in the version control system(s) used by a project.",
            "title": "The Transient Metric Provider"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#0-setup",
            "text": "Create a new Plugin project in Eclipse.   Go to File > New > Project... and select 'Plug-in project'  Give the project an appropriate name. The OSSMETER naming convention is:  Transient metrics: org.ossmeter.metricprovider.trans.(metric name)  Historical metrics: org.ossmeter.metricprovider.historical.(metric name)    Click Next  If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it  Click Finish  Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.",
            "title": "0. Setup"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#1-the-data-model",
            "text": "We define the data model using the Emfatic language. In your newly created plug-in, create a package called  org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called  commits.emf . In this file, we will define our data model.  First of all, we need to state the name of the package.  package org.ossmeter.metricprovider.trans.commits.model;  This is used by the Pongo code generator - the generated classes will be put in this package.  We then define the database for our model:  @db(qualifiedCollectionNames=\"true\")\nclass Commits {\n    val Repository[*] repositories;\n    val Commit[*] commits;\n}  The  @db  annotation tells Pongo that this will be the container database for the data. Adding the  qualifiedCollectionNames=true  property will prepend the database name to all Mongo collections.   The  Commits  class above says that we want a database with two collections, name  repositories  and  commits . If  qualifiedCollectionNames  is set to  true , the collections will be named  Commits.repositories  and  Commits.commits .   We now define the schema of the  Commits.repositories  collection:  ~~~java\nclass Repository {\n    @searchable\n    attr String url;\n    attr String repoType;\n    attr String revision;\n    attr int totalNumberOfCommits;\n    ref CommitData[*] commits;\n}  \nThis class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection.\n\nThe `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time.\n\nEach commit is represented in the `Commits.commits` collection by the following model:\n\n~~~~java\nclass Commit {\n    @searchable\n    attr Date date;\n    attr String identifier;\n    attr String message;\n    attr String author;\n}  For each commit, we store its  date ,  identifier  (revision ID), the commit  message , and the  author . We also create an index on the  date  to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated  findByDate(String date)  or  findOneByDate(String date)  methods).  Now we need to use Pongo to generate code from this model. Right-click on the  commits.emf  file and select  Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package.  Now that we have our data model, we can implement the metric provider.",
            "title": "1. The data model"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#2-the-metric-provider",
            "text": "Create a Java class called  org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider  This class should extend  AbstractTransientMetricProvider  and specify  Commits  for the generic argument:  public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits>  The generic argument states that the metric provider stores objects conforming to the  Commits  data model. Note: You do not need to extend  AbstractTransientMetricProvider  and can implement  ITransientMetricProvider  instead should you wish to.  There are a number of methods that need implementing. We will discuss each in turn.",
            "title": "2. The metric provider"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#adaptdb-db",
            "text": "This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows:      @Override\n    public Commits adapt(DB db) {\n        return new Commits(db);\n    }  The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider:      @Override\n    public String getShortIdentifier() { // This may be deprecated very soon\n        return \"transient-commits\"; \n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Commit History\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"The commit history of the project.\";\n    }  The next method allows you to declare whether the metric provider is applicable to a given project:  @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }  Our metric applies to any project that has at least one VCS repository.  Finally, we have the  measure(...)  method that performs the actual metric calculation:      @Override\n    public void measure(Project project, ProjectDelta delta, Commits db) {\n\n        for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) {\n            Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl());\n\n            if (repo == null) {\n                repo = new Repository();\n                repo.setUrl(repoDelta.getRepository().getUrl());\n                db.getRepositories().add(repo);\n            }\n\n            for (VcsCommit commit : repoDelta.getCommits()) {\n                Commit c = new Commit();\n                c.setDate(commit.getJavaDate());\n                c.setMessage(commit.getMessage());\n                c.setAuthor(commit.getAuthor());\n                c.setIdentifier(commit.getRevision());\n\n                repo.getCommits().add(c);\n                db.getCommits().add(c);\n            }\n        }\n        db.getCommits().sync();\n        db.getRepositories().sync();\n    }  The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new  Commit  object is created for each commit in the delta.",
            "title": "adapt(DB db)"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#3-make-the-metric-provider-discoverable",
            "text": "Metric providers are registered with the OSSMETER platform using  extension points :   Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.metricprovider  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class   Now everything is ready for the metric to be executed :)",
            "title": "3. Make the metric provider discoverable"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#the-historic-metric-provider",
            "text": "This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.",
            "title": "The Historic Metric Provider"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#0-setup_1",
            "text": "Create a new Plugin project in Eclipse.     Go to File > New > Project... and select 'Plug-in project'  Give the project an appropriate name. The OSSMETER naming convention is:  Transient metrics: org.ossmeter.metricprovider.trans.(metric name)  Historical metrics: org.ossmeter.metricprovider.historical.(metric name)    Click Next  If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it  Click Finish  Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.",
            "title": "0. Setup"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#1-the-data-model_1",
            "text": "In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model:  package org.ossmeter.metricprovider.historic.commits.model;\n\nclass HistoricCommits {\n    attr int numberOfCommits;\n}  The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.",
            "title": "1. The data model"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#2-the-metric-provider_1",
            "text": "Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider  This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)):  public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider \nThe generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to.  There are a number of methods that need implementing. We will discuss each in turn.  First of all, complete the typical information-related methods:      @Override\n    public String getShortIdentifier() {\n        return \"historicalcommits\";\n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Historical commits\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"...\";\n    }  Now complete the standard  appliesTo  method:      @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }  We now need to specify a dependency on the transient metric provider that we just implemented.      @Override\n    public List<String> getIdentifiersOfUses() {\n        return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName());\n    }     This tells the platform that we need access to the  CommitsTransientMetricProvider  database. The platform will assign this to the  uses  field that is available to the historical metric provider, as you'll see in the  measure  method:      @Override\n    public Pongo measure(Project project) {\n        Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0));\n\n        int commits = (int) transDb.getCommits().size();\n\n        HistoricCommits hist = new HistoricCommits();\n        hist.setNumberOfCommits(commits);\n\n        return hist;\n    }  First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the  commits  collection. We save this in an instance of the  HistoricCommits  Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.",
            "title": "2. The metric provider"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#3-make-the-metric-provider-discoverable_1",
            "text": "This process is the same as the transient metric provider:   Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.metricprovider  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class   Now everything is ready for both metrics to be executed :)  But first. Let's specify how we want this historical metric to be visualised.",
            "title": "3. Make the metric provider discoverable"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#4-define-a-metvis-visualisation-specification",
            "text": "MetVis  is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis  web page  has numerous examples of this.  Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider:  {\n    \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\",\n    \"vis\" : [\n        {\n            \"id\" : \"historicalcommits\",\n            \"name\" : \"Commits over time\",\n            \"description\" : \"This metric shows when the projects commits occurred\",\n            \"type\" : \"LineChart\",\n            \"datatable\" : {\n                \"cols\" : [\n                    { \"name\" : \"Date\", \"field\" : \"$__date\" },\n                    { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" }\n                ]\n            },\n            \"x\" : \"Date\",\n            \"y\" : \"Commits\"\n        }\n    ]\n}  The  metricId  field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use  $__date . The final two fields ( x  and  y ) are references to column names in the datatable specification. The  type  states that the data should be plotted as a line chart. You can test your MetVis specifications on the  MetVis playpen .",
            "title": "4. Define a MetVis visualisation specification"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#5-make-the-visualisation-specification-discoverable",
            "text": "As with metric providers, visualisation specifications are registered using extension points.   Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project  Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.visualisation.metric  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'historicalcommits.json' file   Good job.",
            "title": "5. Make the visualisation specification discoverable"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#running-the-metric-providers",
            "text": "See  Running from Source",
            "title": "Running the metric providers"
        },
        {
            "location": "/developers-guide/metric-provider-developement-guide/#homework",
            "text": "Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.",
            "title": "Homework"
        },
        {
            "location": "/developers-guide/runing-from-sources/administration-application/",
            "text": "Running the Administration Application form Sources\n\n\nPrerequisites:\n\n\n\n\nStart running the \nMetric Platform\n.\n\n\n\n\nDevelopment Toolkits\n\n\nScava Administration is a based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI 6.1.4 tool to make application development more quicker and easier (Find more here: \nhttps://angular.io/guide/quickstart\n).\n\n\nsudo npm install @angular/cli@6.1.4\n\n\n\n\nGet the Code\n\n\nGet the latest version of the code, and checkout the \ndev\n branch. Please don't commit to the \nmaster\n branch: see the \nDevelopment Guidelines\n:\n\n\nIf you are using \nLinux / OS X\n:\n\n\ngit clone https://github.com/crossminer/scava.git scava\ncd scava\ngit checkout dev\n\n\n\n\nIf you are using \nWindows\n you need to do things differently due to Windows' long file name limit. In the Git shell:\n\n\nmkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/crossminer/scava.git\ngit fetch\ngit checkout dev\n\n\n\n\nRun the Administration webapp\n\n\nThe following instructions show how to run the dashboard web app from source:\n  * Enter the \nadministration/scava-administration/\n directory within the scava repository.\n  * Install Angular dependency using \nnpm install\n\n  * Run the web app on port 4200 using angular-cli: \nng serve\n\n  * Navigate to \nhttp://localhost:4200/",
            "title": "Administration application"
        },
        {
            "location": "/developers-guide/runing-from-sources/administration-application/#running-the-administration-application-form-sources",
            "text": "",
            "title": "Running the Administration Application form Sources"
        },
        {
            "location": "/developers-guide/runing-from-sources/administration-application/#prerequisites",
            "text": "Start running the  Metric Platform .",
            "title": "Prerequisites:"
        },
        {
            "location": "/developers-guide/runing-from-sources/administration-application/#development-toolkits",
            "text": "Scava Administration is a based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI 6.1.4 tool to make application development more quicker and easier (Find more here:  https://angular.io/guide/quickstart ).  sudo npm install @angular/cli@6.1.4",
            "title": "Development Toolkits"
        },
        {
            "location": "/developers-guide/runing-from-sources/administration-application/#get-the-code",
            "text": "Get the latest version of the code, and checkout the  dev  branch. Please don't commit to the  master  branch: see the  Development Guidelines :  If you are using  Linux / OS X :  git clone https://github.com/crossminer/scava.git scava\ncd scava\ngit checkout dev  If you are using  Windows  you need to do things differently due to Windows' long file name limit. In the Git shell:  mkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/crossminer/scava.git\ngit fetch\ngit checkout dev",
            "title": "Get the Code"
        },
        {
            "location": "/developers-guide/runing-from-sources/administration-application/#run-the-administration-webapp",
            "text": "The following instructions show how to run the dashboard web app from source:\n  * Enter the  administration/scava-administration/  directory within the scava repository.\n  * Install Angular dependency using  npm install \n  * Run the web app on port 4200 using angular-cli:  ng serve \n  * Navigate to  http://localhost:4200/",
            "title": "Run the Administration webapp"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/",
            "text": "Running the Analysis Platform form Sources\n\n\nThis is a quick start guide to get the SCAVA platform running from source with Eclipse.\n\n\nPrerequisite\n\n\nInstall MongoDB\n\n\nYou can download MongoDB from the \nMongoDb website\n.\n\n\nInstall EclipseIDE\n\n\nAlthough these instructions may apply to other versions of Eclipse IDE, they were tested under Eclipse Mars.2 with plug-in development support (Eclipse IDE for RCP Developers package).\n\n\nGet the Code\n\n\nGet the latest version of the code, and checkout the \ndev\n branch. Please don't commit to the \nmaster\n branch: see the \nDevelopment Guidelines\n.\n\n\nIf you are using \nLinux / OS X\n:\n\n\ngit clone https://github.com/eclipse-researchlabs/scava.git scava\ncd scava\ngit checkout dev\n\n\n\n\nIf you are using \nWindows\n you need to do things differently due to Windows' long file name limit. In the Git shell:\n\n\nmkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/eclipse-researchlabs/scava.git\ngit fetch\ngit checkout dev\n\n\n\n\nConfiguration\n\n\nConfigure The Eclipse IDE\n\n\nConfigure the Target Platform\n\n\nThe Scava Analysis Platform is based on the Mars version of the RCP Eclipse. In order to run the platform in a newer eclipse environment, you will have to download the Eclipse Mars platform and configure it as Target Platform.\n\n\n\n\nDownload Eclipse Mars platform\n\n\n\n\nDownload the complete Eclipse Mars platform.\n\n\nIn command ligne :\n\n\n./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.metadata.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder}\n\n\n\n\n./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.artifact.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder}\n\n\n\n\nThen, extract its context somewehere on your machine.\n\n\n\n\nDownload Rascal dependencies\n\n\n\n\nDownload two external libraries required to run the metric providers based on Rascal.\n\n\nRepository URL : \nRascal Dependencies\n\n\n\n\nimpulse_0.3.0.xxx.jar\n\n\n\n\nrascal_eclipse_0.1x.x.xxx.jar\n\n\n\n\n\n\nConfigure the Target Platform\n\n\n\n\n\n\nOpen the Eclipse preferences (\nWindow -> Preferences\n), then choose the Target Platform (\nPlug-in Development -> Target Platform\n).\n\n\n\nFigure-01: Eclipse Preferences.\n\n\nAdd a new Target Platform configuration for the project, choose the Default initialization then click on (\nNext\n).\n\n\n\nFigure-02: Eclipse Target Platforms Initialization.\n\n\nGive a name to the new target platform, Click on (\nAdd\n) button to add the Eclipse Mars dependencies and the external rascal libraries content to the target, then click on (\nFinish\n).\n\n\n\nFigure-03: Eclipse Target Platforms Dependencies.\n\n\nFinally, check the new target configuration, then click on (\nApply and Close\n) to save the changes.\n\n\n\n\nFigure-04: Eclipse Target Platforms.\n\n\nImport Projects into the Eclipse Workspace\n\n\nOpen Eclipse and import all projects from the top level directory of the Scava code (\nFile -> Import -> Maven -> Existing Maven Projects\n), and wait for all the projects to compile without errors.\n\n\nMeanwhile, the Eclipse IDE would suggest you to install a set of m2e-connectors including \n\nTycho Plugin\n.\n\n\n\n\n\nFigure-05: Install m2e-connectors plugins.\n\n\nConfigure the Analysis Platform\n\n\nidentifier=<your name>\n\n\nThe identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. \n\n\nIf you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.\n\n\nlog.type=console|file|rolling\n\n\nYou can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify \nfile\n or \nrolling\n, you must complete the \nlog.file.path\n or \nlog.rolling.path\n property as well. \n\n\nIf the property is not specified, it will default to the console logger.\n\n\nlog.file.path=<path>\n\n\nThe path to the file to store the log. E.g. \nlog.file.path=/tmp/lovelylog.log\n\n\nlog.rolling.path=<path>\n\n\nThe path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify \nlog.rolling.path=/tmp/mylovelylog.log\n, it will store files like so: \n/tmp/mylovelylog.log.2014-12-17-00\n and \n/tmp/mylovelylog.log.2014-12-17-12\n.\n\n\nmaven_executable=<path>\n\n\nThe path to where Maven is installed. E.g. \nmaven_executable=/usr/bin/mvn\n\n\nstorage_path=<path>\n\n\nThe path to where files should be stored. E.g. \nstorage_path=/mnt/ossmeter/\n\n\nmongo_hosts\n\n\nA comma-separated list of the hosts and ports in a replica set. E.g. \nua002:27017,ua009:27017,ua019:27017,ua020:27017\n\n\nRun the Analysis Platform\n\n\nStart MongoDB\n\n\nInstructions for starting mongo can be found in the MongoDB \nmanual\n. For example:\n\n\nsudo systemctl start mongod\n\n\n\n\nor\n\n\nsudo service mongod start\n\n\n\n\nValidate and Run the Platform\n\n\nOpen \nreleng/org.eclipse.scava.product/scava.product\n\n  * Click the \nValidate...\n icon in the top right of the product configuration editor (the icon is a piece of paper with a tick)\n  * If things do not validate, there's something wrong -- get in touch :) Problems related to \norg.eclipse.e4.core.di\n aren't critical.\n  * Then, click the \nExport an Eclipse product\n on the left of the \nValidate...\n button. Uncheck the \nGenerate p2 repository\n checkbox, select a destination directory and validate. After a while, the SCAVA platform will be generated in the selected directory.\n  * The platform can then be run using the generated \neclipse\n binary; it accepts the following arguments:\n    * \n-apiServer\n: Starts up the client API on localhost:8182\n    * \n-worker ${id-worker}\n: Spawns a thread that analyses registered projects\n  * To get a full platform running, first launch a worker thread, then the API server.\n\n\nWhen starting the platform, you can pass a configuration file to control the behaviour of the platform:\n\n\n./eclipse -worker ${id-worker} -config myconfiguration.properties\n\n\n\n\nWorkarounds to manage the runtime missing dependencies\n\n\nIn case that the previous steps doesn't launch your RCP application (mainly due to a runtime missing dependencies), the \"Add Required Plug-ins\" option allows you to validate (i.e. select only the right amount of plugins you actually need) your runtime configuration.\nTo do so, go to \nRun menu\n and select \nRun Configurations...\n. \nFrom the list of the left hand side, select the run configuration for your Eclipse application, then choose \nPlug-ins\n tab at the right dialog box. To resolve the runtime dependencies effectively required to run the RCP application, click on \nAdd Required Plug-ins\n button the \nApply\n to save the changes and re-run your configuration.\n\n\n\n\nFigure-06: Add Required Plug-ins configuration.",
            "title": "Analysis platform"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#running-the-analysis-platform-form-sources",
            "text": "This is a quick start guide to get the SCAVA platform running from source with Eclipse.",
            "title": "Running the Analysis Platform form Sources"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#prerequisite",
            "text": "",
            "title": "Prerequisite"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#install-mongodb",
            "text": "You can download MongoDB from the  MongoDb website .",
            "title": "Install MongoDB"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#install-eclipseide",
            "text": "Although these instructions may apply to other versions of Eclipse IDE, they were tested under Eclipse Mars.2 with plug-in development support (Eclipse IDE for RCP Developers package).",
            "title": "Install EclipseIDE"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#get-the-code",
            "text": "Get the latest version of the code, and checkout the  dev  branch. Please don't commit to the  master  branch: see the  Development Guidelines .  If you are using  Linux / OS X :  git clone https://github.com/eclipse-researchlabs/scava.git scava\ncd scava\ngit checkout dev  If you are using  Windows  you need to do things differently due to Windows' long file name limit. In the Git shell:  mkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/eclipse-researchlabs/scava.git\ngit fetch\ngit checkout dev",
            "title": "Get the Code"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#configuration",
            "text": "",
            "title": "Configuration"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#configure-the-eclipse-ide",
            "text": "",
            "title": "Configure The Eclipse IDE"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#configure-the-target-platform",
            "text": "The Scava Analysis Platform is based on the Mars version of the RCP Eclipse. In order to run the platform in a newer eclipse environment, you will have to download the Eclipse Mars platform and configure it as Target Platform.   Download Eclipse Mars platform   Download the complete Eclipse Mars platform.  In command ligne :  ./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.metadata.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder}  ./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.artifact.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder}  Then, extract its context somewehere on your machine.   Download Rascal dependencies   Download two external libraries required to run the metric providers based on Rascal.  Repository URL :  Rascal Dependencies   impulse_0.3.0.xxx.jar   rascal_eclipse_0.1x.x.xxx.jar    Configure the Target Platform    Open the Eclipse preferences ( Window -> Preferences ), then choose the Target Platform ( Plug-in Development -> Target Platform ).  \nFigure-01: Eclipse Preferences.  Add a new Target Platform configuration for the project, choose the Default initialization then click on ( Next ).  \nFigure-02: Eclipse Target Platforms Initialization.  Give a name to the new target platform, Click on ( Add ) button to add the Eclipse Mars dependencies and the external rascal libraries content to the target, then click on ( Finish ).  \nFigure-03: Eclipse Target Platforms Dependencies.  Finally, check the new target configuration, then click on ( Apply and Close ) to save the changes.   Figure-04: Eclipse Target Platforms.",
            "title": "Configure the Target Platform"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#import-projects-into-the-eclipse-workspace",
            "text": "Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Maven -> Existing Maven Projects ), and wait for all the projects to compile without errors.  Meanwhile, the Eclipse IDE would suggest you to install a set of m2e-connectors including  Tycho Plugin .   Figure-05: Install m2e-connectors plugins.",
            "title": "Import Projects into the Eclipse Workspace"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#configure-the-analysis-platform",
            "text": "",
            "title": "Configure the Analysis Platform"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#identifieryour-name",
            "text": "The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID.   If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.",
            "title": "identifier=&lt;your name&gt;"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#logtypeconsolefilerolling",
            "text": "You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify  file  or  rolling , you must complete the  log.file.path  or  log.rolling.path  property as well.   If the property is not specified, it will default to the console logger.",
            "title": "log.type=console|file|rolling"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#logfilepathpath",
            "text": "The path to the file to store the log. E.g.  log.file.path=/tmp/lovelylog.log",
            "title": "log.file.path=&lt;path&gt;"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#logrollingpathpath",
            "text": "The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify  log.rolling.path=/tmp/mylovelylog.log , it will store files like so:  /tmp/mylovelylog.log.2014-12-17-00  and  /tmp/mylovelylog.log.2014-12-17-12 .",
            "title": "log.rolling.path=&lt;path&gt;"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#maven_executablepath",
            "text": "The path to where Maven is installed. E.g.  maven_executable=/usr/bin/mvn",
            "title": "maven_executable=&lt;path&gt;"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#storage_pathpath",
            "text": "The path to where files should be stored. E.g.  storage_path=/mnt/ossmeter/",
            "title": "storage_path=&lt;path&gt;"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#mongo_hosts",
            "text": "A comma-separated list of the hosts and ports in a replica set. E.g.  ua002:27017,ua009:27017,ua019:27017,ua020:27017",
            "title": "mongo_hosts"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#run-the-analysis-platform",
            "text": "",
            "title": "Run the Analysis Platform"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#start-mongodb",
            "text": "Instructions for starting mongo can be found in the MongoDB  manual . For example:  sudo systemctl start mongod  or  sudo service mongod start",
            "title": "Start MongoDB"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#validate-and-run-the-platform",
            "text": "Open  releng/org.eclipse.scava.product/scava.product \n  * Click the  Validate...  icon in the top right of the product configuration editor (the icon is a piece of paper with a tick)\n  * If things do not validate, there's something wrong -- get in touch :) Problems related to  org.eclipse.e4.core.di  aren't critical.\n  * Then, click the  Export an Eclipse product  on the left of the  Validate...  button. Uncheck the  Generate p2 repository  checkbox, select a destination directory and validate. After a while, the SCAVA platform will be generated in the selected directory.\n  * The platform can then be run using the generated  eclipse  binary; it accepts the following arguments:\n    *  -apiServer : Starts up the client API on localhost:8182\n    *  -worker ${id-worker} : Spawns a thread that analyses registered projects\n  * To get a full platform running, first launch a worker thread, then the API server.  When starting the platform, you can pass a configuration file to control the behaviour of the platform:  ./eclipse -worker ${id-worker} -config myconfiguration.properties",
            "title": "Validate and Run the Platform"
        },
        {
            "location": "/developers-guide/runing-from-sources/analysis-platform/#workarounds-to-manage-the-runtime-missing-dependencies",
            "text": "In case that the previous steps doesn't launch your RCP application (mainly due to a runtime missing dependencies), the \"Add Required Plug-ins\" option allows you to validate (i.e. select only the right amount of plugins you actually need) your runtime configuration.\nTo do so, go to  Run menu  and select  Run Configurations... . \nFrom the list of the left hand side, select the run configuration for your Eclipse application, then choose  Plug-ins  tab at the right dialog box. To resolve the runtime dependencies effectively required to run the RCP application, click on  Add Required Plug-ins  button the  Apply  to save the changes and re-run your configuration.  \nFigure-06: Add Required Plug-ins configuration.",
            "title": "Workarounds to manage the runtime missing dependencies"
        },
        {
            "location": "/developers-guide/runing-from-sources/eclipse-plugin/",
            "text": "Running the Eclipse Plugin form Sources (To be completed by FrontendArt)",
            "title": "Eclipse plugin"
        },
        {
            "location": "/developers-guide/runing-from-sources/eclipse-plugin/#running-the-eclipse-plugin-form-sources-to-be-completed-by-frontendart",
            "text": "",
            "title": "Running the Eclipse Plugin form Sources (To be completed by FrontendArt)"
        },
        {
            "location": "/developers-guide/runing-from-sources/",
            "text": "Running SCAVA Platform form Sources\n\n\nThe following section provide informations related to how the main platform components can be run from sources in developers mode\n\n\n\n\nAnalysis Platform\n (Softeam)\n\n\nAdministration Application\n (Softeam)\n\n\nVisualisation Dashboard\n (Bitergia)\n\n\nEclipse Plugin\n (FrontendArt)",
            "title": "Home"
        },
        {
            "location": "/developers-guide/runing-from-sources/#running-scava-platform-form-sources",
            "text": "The following section provide informations related to how the main platform components can be run from sources in developers mode   Analysis Platform  (Softeam)  Administration Application  (Softeam)  Visualisation Dashboard  (Bitergia)  Eclipse Plugin  (FrontendArt)",
            "title": "Running SCAVA Platform form Sources"
        },
        {
            "location": "/developers-guide/runing-from-sources/visualisation-dashboard/",
            "text": "Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia)\n\n\nAll the documentation below describes how to setup and run the different components\nrelated to the Web Dashboards.\n\n\nInstall steps for the different components\n\n\nInstall GrimoireLab Python Env\n\n\nThe data processing is done with GrimoireLab python platform.\n\n\nA virtual env in Python is used to install the tools needed.\n\n\nIn Debian/Ubuntu you need to execute:\n\n\nsudo apt-get install python3-venv\n\n\nTo create the python virtualenv and activate it:\n\n\nmkdir ~/venvs\npython3 -m venv ~/venvs/crossminer\nsource ~/venvs/crossminer/bin/activate\npip3 install grimoire-elk\n\n\n\n\nInstall Elasticsearch and Kibana\n\n\nAn Elasticsearch and Kibana services are needed. docker-compose can be used to start them.\n\n\nElasticsearch needs this config:\n\n\nsudo sh -c \"echo 262144 > /proc/sys/vm/max_map_count\"",
            "title": "Visualisation dashboard"
        },
        {
            "location": "/developers-guide/runing-from-sources/visualisation-dashboard/#running-the-visualisation-dashboard-form-sources-to-be-reviewed-and-completed-by-bitergia",
            "text": "All the documentation below describes how to setup and run the different components\nrelated to the Web Dashboards.",
            "title": "Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia)"
        },
        {
            "location": "/developers-guide/runing-from-sources/visualisation-dashboard/#install-steps-for-the-different-components",
            "text": "",
            "title": "Install steps for the different components"
        },
        {
            "location": "/developers-guide/runing-from-sources/visualisation-dashboard/#install-grimoirelab-python-env",
            "text": "The data processing is done with GrimoireLab python platform.  A virtual env in Python is used to install the tools needed.  In Debian/Ubuntu you need to execute:  sudo apt-get install python3-venv  To create the python virtualenv and activate it:  mkdir ~/venvs\npython3 -m venv ~/venvs/crossminer\nsource ~/venvs/crossminer/bin/activate\npip3 install grimoire-elk",
            "title": "Install GrimoireLab Python Env"
        },
        {
            "location": "/developers-guide/runing-from-sources/visualisation-dashboard/#install-elasticsearch-and-kibana",
            "text": "An Elasticsearch and Kibana services are needed. docker-compose can be used to start them.  Elasticsearch needs this config:  sudo sh -c \"echo 262144 > /proc/sys/vm/max_map_count\"",
            "title": "Install Elasticsearch and Kibana"
        },
        {
            "location": "/installation-guide/database-backup/",
            "text": "Platform Data Base Backup And Restoration\n\n\nThe SCAVA Platform use a Mongo database to strore data related to analysed projets (project data, analyse process data and  measurements collected durnig the analysis process). A backup of this data can be  perfroemd using the default Mongo backup service.\n\n\nSCAVA Paltform Data Model\n\n\nThe SCAVA Data mode is composed of several data bases. Each of this databases must be backuped individualy \n\n\n\n\n\n\n\n\nDatabase\n\n\nDescripton\n\n\n\n\n\n\n\n\n\n\nscava\n\n\nThe scava databases containes informations related to Analysed Projects including the list of all repository related to this project\n\n\n\n\n\n\nscava-analysis\n\n\nThe scava-analysis databases containes informations related to analysis process. For each analysed project, this database containes information related to the defined analysed tasks and information related the execution of this anlaysis tasks\n\n\n\n\n\n\n{AnalysedProjectName}\n\n\nFor each analysed project, the SCAVA Platform store the result of the analyse (collected measurements) a is a separate database. The Name of this database is the shortName of the project. The  shortName of a project cab be retrive using the REST API exposed by the platforme ( http:/{platforme url}:{platforme port}/project)\n\n\n\n\n\n\n\n\nBackup Process\n\n\nThe Mongo installation package(windows and linux)  provide the \nmongodump\n service whic allow to create a database dump\n\n\n\n\nmongodump --gzip --archive=\n --db \n --port \n\n\n\n\nExample :\n\n\nmongodump --gzip --archive=scava.20180906.gz --db scava --port 27018                   // project database\nmongodump --gzip --archive=scava-analysis.20180906.gz --db scava-analysis --port 27018 // analysis process database\nmongodump --gzip --archive=QualityGuard.20180906.gz --db QualityGuard --port 27018     // QualityGuard project database\n\n\n\n\nRestoration Process\n\n\nThe Mongo installation package( windows and linux)  provide the \nmongorestore\n service which allow to restore a database dump\n\n\n\n\nmongorestore --drop --gzip --archive=Activemq.20181203.gz \n\n\n\n\nExample :\n\n\nmongorestore --drop --gzip --archive=scava.20181203.gz           // project database\nmongorestore --drop --gzip --archive=scava-analysis.20181203.gz  // analysis process database\nmongorestore --drop --gzip --archive=QualityGuard.20181203.gz    // QualityGuard project database",
            "title": "Database backup"
        },
        {
            "location": "/installation-guide/database-backup/#platform-data-base-backup-and-restoration",
            "text": "The SCAVA Platform use a Mongo database to strore data related to analysed projets (project data, analyse process data and  measurements collected durnig the analysis process). A backup of this data can be  perfroemd using the default Mongo backup service.",
            "title": "Platform Data Base Backup And Restoration"
        },
        {
            "location": "/installation-guide/database-backup/#scava-paltform-data-model",
            "text": "The SCAVA Data mode is composed of several data bases. Each of this databases must be backuped individualy      Database  Descripton      scava  The scava databases containes informations related to Analysed Projects including the list of all repository related to this project    scava-analysis  The scava-analysis databases containes informations related to analysis process. For each analysed project, this database containes information related to the defined analysed tasks and information related the execution of this anlaysis tasks    {AnalysedProjectName}  For each analysed project, the SCAVA Platform store the result of the analyse (collected measurements) a is a separate database. The Name of this database is the shortName of the project. The  shortName of a project cab be retrive using the REST API exposed by the platforme ( http:/{platforme url}:{platforme port}/project)",
            "title": "SCAVA Paltform Data Model"
        },
        {
            "location": "/installation-guide/database-backup/#backup-process",
            "text": "The Mongo installation package(windows and linux)  provide the  mongodump  service whic allow to create a database dump   mongodump --gzip --archive=  --db   --port    Example :  mongodump --gzip --archive=scava.20180906.gz --db scava --port 27018                   // project database\nmongodump --gzip --archive=scava-analysis.20180906.gz --db scava-analysis --port 27018 // analysis process database\nmongodump --gzip --archive=QualityGuard.20180906.gz --db QualityGuard --port 27018     // QualityGuard project database",
            "title": "Backup Process"
        },
        {
            "location": "/installation-guide/database-backup/#restoration-process",
            "text": "The Mongo installation package( windows and linux)  provide the  mongorestore  service which allow to restore a database dump   mongorestore --drop --gzip --archive=Activemq.20181203.gz    Example :  mongorestore --drop --gzip --archive=scava.20181203.gz           // project database\nmongorestore --drop --gzip --archive=scava-analysis.20181203.gz  // analysis process database\nmongorestore --drop --gzip --archive=QualityGuard.20181203.gz    // QualityGuard project database",
            "title": "Restoration Process"
        },
        {
            "location": "/installation-guide/",
            "text": "SCAVA Installation Guide\n\n\nThe SCAVA installation guide provides instructions on how to install and configure the SCAVA Platform on a server and how to deploy the Eclipses Plugin in development environment. \n\n\n\n\nPlatform Installation using DOCKER images\n \n\n\nPlatform Installation using individual components \n \n\n\nPlatform Data Base Backup And Restoration\n\n\nEclipse Plugin Installation\n\n\nPlatform Configuration\n  \n\n\nLicencing Information",
            "title": "Home"
        },
        {
            "location": "/installation-guide/#scava-installation-guide",
            "text": "The SCAVA installation guide provides instructions on how to install and configure the SCAVA Platform on a server and how to deploy the Eclipses Plugin in development environment.    Platform Installation using DOCKER images    Platform Installation using individual components     Platform Data Base Backup And Restoration  Eclipse Plugin Installation  Platform Configuration     Licencing Information",
            "title": "SCAVA Installation Guide"
        },
        {
            "location": "/installation-guide/licencing/",
            "text": "Licencing for Scava\n\n\nThe Scava project is licensed under \nEclipse Public License - v 2.0\n license.\n\n\nEclipse Public License licensing\n\n\nEclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.",
            "title": "Licencing"
        },
        {
            "location": "/installation-guide/licencing/#licencing-for-scava",
            "text": "The Scava project is licensed under  Eclipse Public License - v 2.0  license.",
            "title": "Licencing for Scava"
        },
        {
            "location": "/installation-guide/licencing/#eclipse-public-license-licensing",
            "text": "Eclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.",
            "title": "Eclipse Public License licensing"
        },
        {
            "location": "/installation-guide/docker-installation/",
            "text": "Platform Installation using DOCKER images\n\n\nThis page is about how to deploy a SCAVA instance on the behalf of Docker.\n\n\nAt the actual stage of the project, there two ways to get started with the docker images:\n1. Ready-to-use images are stored on the \nCrossminer Docker-hub account\n.\n1. Build them from the \nscava-deployment\n repository. They have to be built from various Dockerfile's and with help of a docker-compose file.\n\n\nSummary of containers\n\n\nThe whole Docker stack consists of 11 services:\n\n\n\n\n\n\n\n\nDocker service name\n\n\nFull Name\n\n\nDefault port\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nadmin-webapp\n\n\nAdministration UI\n\n\n5601\n\n\nBuilt from /web-admin\n\n\n\n\n\n\noss-app\n\n\nMetric Plateform\n\n\n8182\n\n\nBuilt from /metric-platform\n\n\n\n\n\n\noss-db\n\n\nMongoDB (metrics storage)\n\n\n27017\n\n\nBuilt from /metric-platform to start with a pre-loaded database (few projects analysed) or use a clean MongoDB image for a clean installation (\nimage: mongo:3.4\n). Can be used to connect a MongoDB visualisation tool\n\n\n\n\n\n\nkb\n\n\nKnowledge base\n\n\n8080\n\n\nBuilt from /KB\n\n\n\n\n\n\nkb-db\n\n\nKnowledge base DB (based on MongoDB)\n\n\n27018\n\n\nBuilt from /KB-db. Can be used to connect a MongoDB visualisation tool\n\n\n\n\n\n\napi-gw\n\n\nAPI Gateway\n\n\n8086\n\n\nBuilt from /api-gw\n\n\n\n\n\n\nauth\n\n\nAuthentication\n\n\n8085\n\n\nBuilt from /auth\n\n\n\n\n\n\nelasticsearch\n\n\nElasticSearch\n\n\n9200\n\n\nPulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool\n\n\n\n\n\n\nkibiter\n\n\nKibiter (Bitergia\u2019s customized Kibana)\n\n\n80\n\n\nPulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1\n\n\n\n\n\n\ndashb-importer\n\n\nDashboard importer (to kibiter)\n\n\n\n\nNo port exposed on the host\n\n\n\n\n\n\nprosoul\n\n\nProsoul Quality Model Viewer\n\n\n8000\n\n\nPulled from docker hub bitergia/prosoul\n\n\n\n\n\n\n\n\nPrerequisites\n\n\nIn order to run Scava, you need to:\n\n\n\n\nEdit the hosts of your machine, creating a new record for the IP address: \n127.0.0.1\n with hostname: \nadmin-webapp\n\n\nEdit the \ndocker-compose-build.yml\n file and change:\n\n\nThe environment variable \nAPI-GATEWAY_VAR\n, replacing \nlocalhost\nby the IP address of your host on service \nadmin-webapp\n\n\nMake sure that the ports defined in the file are not already used on the host, and adjust the various ports as required for your setup. Note that the Kibiter dashboard is on port 80 by default.\n\n\nUpdate the \nALLOWED_HOST\ndirective to include the host name on service \nprosoul\n. This is used by Django on the prosoul image to publish the quality model used by Crossminer.\n\n\n\n\n\n\n\n\nBuilding the Docker images\n\n\nThe deployment setup is hosted in the \nscava-deployment\n repository. One needs to clone the repository locally in order to build and run the docker images.\n\n\nTo build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers.\n\n\n$ docker-compose -f docker-compose-build.yml build --no-cache\n\n\n\n\nThis will build required images using the latest version of the binaries on the CI server and pull images hosted on docker hub.\n\n\nSetup Configurations\n\n\nSome optional configurations can be made to customize the dockerized SCAVA instance. These configuration are applied to the docker-compose description files \ndocker-compose-build.yml\n or \ndocker-compose-dockerhub.yml\n and are applied every time that the instance runs.\n\n\nData persistence\n\n\nVolumes can be created to persist the data of the databases between execution of the Scava platform. To enable the creation of volumes uncomemnt the corresponding lines on the \noss-db\n and \nkb-db\n services:\n\n\n\n\nOn \noss-db\n:\n\n\n\n\n    volumes: #creates volume on container\n        - ~/oss-data:/data/db\n\n\n\n\n\n\nOn \nkb-db\n:\n\n\n\n\n    volumes: #creates volume on container\n        - ~/kb-data:/data/kb-db\n\n\n\n\nMultiple Workers Configuration\n\n\nOne instance of the metric-platform is only able to run one worker. Therefore, a setup where it is required to have more than one worker requires the deployment of multiple instances of metric platform.\n\n\nThe main instance of the metric platform runs under the service \noss-app\n. This is the service started by default with the \nmaster\n and \napiServer\n flags, and launchs the worker \nw1\n.\n\n\nTo enable a second worker, the \ndocker-compose-build.yml\n (or \ndocker-compose-dockerhub.yml\n) file must be edited and uncomented the service \noss-slave\n. This service will launch a new instance of the metric platform that will only run one worker (with default name \nw2\n). This slave instance \ndo not have\n the flags \nmaster\n or \napiServer\n.\n\n\nNew slaves can be added (e.g. \noss-slave01\n,\noss-slave02\n,\noss-slave03\n, etc.) by copy-paste of the \noss-slave\n service and changing the name of service and name of the worker.\n\n\nRunning the locally built docker images\n\n\nTo run the locally built images, run the following command. Note that if the images are not available they will be rebuilt.\n\n\n$ docker-compose -f docker-compose-build.yml up\n\n\n\n\nAccess the administration web app by using the following address in the web browser: http://admin-webapp/\nFor login use user: admin  pass: admin\n\n\nRunning the pre-built docker images\n\n\n\n\nPlease note that the docker hub images are not yet ready! We're working on it! :-)\n\n\n\n\nThe easiest way to run the full Scava setup is to use the docker images \nstored on Docker Hub\n. Use the \ndocker-compose-dockerhub.yml\n file to download all required images and start the stack:\n\n\n$ docker-compose -f docker-compose-dockerhub.yml up\n\n\n\n\nAccess the administration web app by using the following address in the web browser: http://admin-webapp/\n\n\nFor login use user: admin  pass: admin\n\n\nPost-install tasks\n\n\nConfiguring the GitHub token\n\n\nIn order to use the GitHub connectors, one needs to setup an authentication mechanism. Simply create a authentication token in yourGitHub account, and create a new property in the webadmin UI:\n\n\n\n\nkey: githubToken\n\n\nvalue: the github token created on the github account.\n\n\n\n\nKibana dashboard\n\n\nThe first time Kibana is started, it will ask for a default index pattern. To select one, log into the dashboard (admin/admin), go to the dashboard menu item and select \nmetrics-scava\n. Then click on the star on the top right. \n\n\nContinuous integration\n\n\nWe use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the \n#ci\n channel in Slack.",
            "title": "Home"
        },
        {
            "location": "/installation-guide/docker-installation/#platform-installation-using-docker-images",
            "text": "This page is about how to deploy a SCAVA instance on the behalf of Docker.  At the actual stage of the project, there two ways to get started with the docker images:\n1. Ready-to-use images are stored on the  Crossminer Docker-hub account .\n1. Build them from the  scava-deployment  repository. They have to be built from various Dockerfile's and with help of a docker-compose file.",
            "title": "Platform Installation using DOCKER images"
        },
        {
            "location": "/installation-guide/docker-installation/#summary-of-containers",
            "text": "The whole Docker stack consists of 11 services:     Docker service name  Full Name  Default port  Comments      admin-webapp  Administration UI  5601  Built from /web-admin    oss-app  Metric Plateform  8182  Built from /metric-platform    oss-db  MongoDB (metrics storage)  27017  Built from /metric-platform to start with a pre-loaded database (few projects analysed) or use a clean MongoDB image for a clean installation ( image: mongo:3.4 ). Can be used to connect a MongoDB visualisation tool    kb  Knowledge base  8080  Built from /KB    kb-db  Knowledge base DB (based on MongoDB)  27018  Built from /KB-db. Can be used to connect a MongoDB visualisation tool    api-gw  API Gateway  8086  Built from /api-gw    auth  Authentication  8085  Built from /auth    elasticsearch  ElasticSearch  9200  Pulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool    kibiter  Kibiter (Bitergia\u2019s customized Kibana)  80  Pulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1    dashb-importer  Dashboard importer (to kibiter)   No port exposed on the host    prosoul  Prosoul Quality Model Viewer  8000  Pulled from docker hub bitergia/prosoul",
            "title": "Summary of containers"
        },
        {
            "location": "/installation-guide/docker-installation/#prerequisites",
            "text": "In order to run Scava, you need to:   Edit the hosts of your machine, creating a new record for the IP address:  127.0.0.1  with hostname:  admin-webapp  Edit the  docker-compose-build.yml  file and change:  The environment variable  API-GATEWAY_VAR , replacing  localhost by the IP address of your host on service  admin-webapp  Make sure that the ports defined in the file are not already used on the host, and adjust the various ports as required for your setup. Note that the Kibiter dashboard is on port 80 by default.  Update the  ALLOWED_HOST directive to include the host name on service  prosoul . This is used by Django on the prosoul image to publish the quality model used by Crossminer.",
            "title": "Prerequisites"
        },
        {
            "location": "/installation-guide/docker-installation/#building-the-docker-images",
            "text": "The deployment setup is hosted in the  scava-deployment  repository. One needs to clone the repository locally in order to build and run the docker images.  To build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers.  $ docker-compose -f docker-compose-build.yml build --no-cache  This will build required images using the latest version of the binaries on the CI server and pull images hosted on docker hub.",
            "title": "Building the Docker images"
        },
        {
            "location": "/installation-guide/docker-installation/#setup-configurations",
            "text": "Some optional configurations can be made to customize the dockerized SCAVA instance. These configuration are applied to the docker-compose description files  docker-compose-build.yml  or  docker-compose-dockerhub.yml  and are applied every time that the instance runs.",
            "title": "Setup Configurations"
        },
        {
            "location": "/installation-guide/docker-installation/#data-persistence",
            "text": "Volumes can be created to persist the data of the databases between execution of the Scava platform. To enable the creation of volumes uncomemnt the corresponding lines on the  oss-db  and  kb-db  services:   On  oss-db :       volumes: #creates volume on container\n        - ~/oss-data:/data/db   On  kb-db :       volumes: #creates volume on container\n        - ~/kb-data:/data/kb-db",
            "title": "Data persistence"
        },
        {
            "location": "/installation-guide/docker-installation/#multiple-workers-configuration",
            "text": "One instance of the metric-platform is only able to run one worker. Therefore, a setup where it is required to have more than one worker requires the deployment of multiple instances of metric platform.  The main instance of the metric platform runs under the service  oss-app . This is the service started by default with the  master  and  apiServer  flags, and launchs the worker  w1 .  To enable a second worker, the  docker-compose-build.yml  (or  docker-compose-dockerhub.yml ) file must be edited and uncomented the service  oss-slave . This service will launch a new instance of the metric platform that will only run one worker (with default name  w2 ). This slave instance  do not have  the flags  master  or  apiServer .  New slaves can be added (e.g.  oss-slave01 , oss-slave02 , oss-slave03 , etc.) by copy-paste of the  oss-slave  service and changing the name of service and name of the worker.",
            "title": "Multiple Workers Configuration"
        },
        {
            "location": "/installation-guide/docker-installation/#running-the-locally-built-docker-images",
            "text": "To run the locally built images, run the following command. Note that if the images are not available they will be rebuilt.  $ docker-compose -f docker-compose-build.yml up  Access the administration web app by using the following address in the web browser: http://admin-webapp/\nFor login use user: admin  pass: admin",
            "title": "Running the locally built docker images"
        },
        {
            "location": "/installation-guide/docker-installation/#running-the-pre-built-docker-images",
            "text": "Please note that the docker hub images are not yet ready! We're working on it! :-)   The easiest way to run the full Scava setup is to use the docker images  stored on Docker Hub . Use the  docker-compose-dockerhub.yml  file to download all required images and start the stack:  $ docker-compose -f docker-compose-dockerhub.yml up  Access the administration web app by using the following address in the web browser: http://admin-webapp/  For login use user: admin  pass: admin",
            "title": "Running the pre-built docker images"
        },
        {
            "location": "/installation-guide/docker-installation/#post-install-tasks",
            "text": "",
            "title": "Post-install tasks"
        },
        {
            "location": "/installation-guide/docker-installation/#configuring-the-github-token",
            "text": "In order to use the GitHub connectors, one needs to setup an authentication mechanism. Simply create a authentication token in yourGitHub account, and create a new property in the webadmin UI:   key: githubToken  value: the github token created on the github account.",
            "title": "Configuring the GitHub token"
        },
        {
            "location": "/installation-guide/docker-installation/#kibana-dashboard",
            "text": "The first time Kibana is started, it will ask for a default index pattern. To select one, log into the dashboard (admin/admin), go to the dashboard menu item and select  metrics-scava . Then click on the star on the top right.",
            "title": "Kibana dashboard"
        },
        {
            "location": "/installation-guide/docker-installation/#continuous-integration",
            "text": "We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the  #ci  channel in Slack.",
            "title": "Continuous integration"
        },
        {
            "location": "/installation-guide/platform-configuration/",
            "text": "Platform Installation using individual components (UNPARALLEL)",
            "title": "Home"
        },
        {
            "location": "/installation-guide/platform-configuration/#platform-installation-using-individual-components-unparallel",
            "text": "",
            "title": "Platform Installation using individual components (UNPARALLEL)"
        },
        {
            "location": "/installation-guide/platform-installation/",
            "text": "Platform Installation using individual components (UNPARALLEL)\n\n\nKB installation\n\n\nun it by maven:\n\n\nmvn spring-boot:run\n\n\n\n\ntest it by maven:\n\n\nmvn test\n\n\n\n\nWhen starting the platform, you can set configuration application parameters in properly configuration file:\n\n\n\n\ndeploy configuration file at src/test/resources/application.properties.\n\n\ntest cnfiguration file at src/test/resources/application.properties. The configuration file is a typical Java properties file. The properties that can be configured are:\n\n\n\n\nspring.data.mongodb.host=<DB_HOST>\nspring.data.mongodb.port=<DB_PORT>\nspring.data.mongodb.database=<DB_NAME>\nlucene.index.folder=<path_to_lucene_index>\negit.github.token=<GITHUB_TOKEN>\nossmeter.url=<OSSMETER_URL>\n...\n\n\n\n\nAn instance of configuration file:\n\n\nspring.data.mongodb.host=localhost \nspring.data.mongodb.port=27017 \nspring.data.mongodb.database=CROSSMINER_TEST\nlucene.index.folder=/home/user/CROSSMiner_lucene/\negit.github.token=<github_token>\nossmeter.url=http://localhost:8182/\n...",
            "title": "Home"
        },
        {
            "location": "/installation-guide/platform-installation/#platform-installation-using-individual-components-unparallel",
            "text": "",
            "title": "Platform Installation using individual components (UNPARALLEL)"
        },
        {
            "location": "/installation-guide/platform-installation/#kb-installation",
            "text": "un it by maven:  mvn spring-boot:run  test it by maven:  mvn test  When starting the platform, you can set configuration application parameters in properly configuration file:   deploy configuration file at src/test/resources/application.properties.  test cnfiguration file at src/test/resources/application.properties. The configuration file is a typical Java properties file. The properties that can be configured are:   spring.data.mongodb.host=<DB_HOST>\nspring.data.mongodb.port=<DB_PORT>\nspring.data.mongodb.database=<DB_NAME>\nlucene.index.folder=<path_to_lucene_index>\negit.github.token=<GITHUB_TOKEN>\nossmeter.url=<OSSMETER_URL>\n...  An instance of configuration file:  spring.data.mongodb.host=localhost \nspring.data.mongodb.port=27017 \nspring.data.mongodb.database=CROSSMINER_TEST\nlucene.index.folder=/home/user/CROSSMiner_lucene/\negit.github.token=<github_token>\nossmeter.url=http://localhost:8182/\n...",
            "title": "KB installation"
        },
        {
            "location": "/installation-guide/plugin-installation/",
            "text": "Eclipse Plugin Installation",
            "title": "Home"
        },
        {
            "location": "/installation-guide/plugin-installation/#eclipse-plugin-installation",
            "text": "",
            "title": "Eclipse Plugin Installation"
        },
        {
            "location": "/others/",
            "text": "Welcome to the Scava documentation\n\n\nThis web site is the main documentation place for the \nEclipse Scava\n project.\n\n\nUseful links:\n\n\n\n\nEclipse Scava home project: \nEclipse Scava @ Eclipse\n\n\nEclipse Scava code repository: \ngithub.com/crossminer/scava\n\n\nEclipse Scava deployment repository: \ngithub.com/crossminer/scava-deployment\n\n\nEclipse Scava documentation repository: \ngithub.com/crossminer/scava-docs\n\n\n\n\nPlatform installation\n\n\n\n\nDocker-SCAVA\n How to build and run the Scava docker image.\n\n\nRunning the platform\n Quick start guide to get the Scava platform running from source on an Eclipse development environment.\n\n\nConfiguring the platform\n Quick start guide to present how to configure the platform using a configuration file.\n\n\nDocker-OSSMETER\n How to build and run the Ossmeter docker image.\n\n\n\n\nAdministration\n\n\n\n\nScava Administration\n The administration dashboard take care of managing Scava's services.\n\n\nAPI Gateway Configuration\n\n\nExtending MongoDB Data Model\n\n\n\n\nUsers\n\n\n\n\nScava metrics\n lists metrics computed by the various Scava Components.\n\n\nConsuming the REST services\n This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues\n\n\nRunning Scava in Eclipse\n How to setup and run the Scava Eclipse IDE plugin.\n\n\nREST API Documentation\n Reference documentation of REST services provided by the Scava platform.\n\n\nREST API Generation\n Tutorial about automatic generation of REST API Scava library using OpenAPI.\n\n\nAccessing Scava resources\n A summary of where to find the various outputs of the Scava platform.\n\n\n\n\nDevelopment\n\n\n\n\nContributing\n Collection of Architectural and Technical guidelines dedicated to Scava developers.\n\n\nDevelopment guidelines\n Rules and guidelines used for the development of the Scava project.\n\n\nTesting Guidelines\n Collection of testing guidelines dedicated to Scava developers.\n\n\nRepository-Organisation\n\n\nHow to develop a metric provider\n Want to add a new metric provider? Here are some hints.\n\n\nLicensing\n Information about licensing used within Scava.\n\n\n\n\nArchitecture\n\n\n\n\nAPI Gateway Component\n The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway.\n\n\nAuthentication Component\n The administration dashboard takes care of managing Scava's services.",
            "title": "Home"
        },
        {
            "location": "/others/#welcome-to-the-scava-documentation",
            "text": "This web site is the main documentation place for the  Eclipse Scava  project.  Useful links:   Eclipse Scava home project:  Eclipse Scava @ Eclipse  Eclipse Scava code repository:  github.com/crossminer/scava  Eclipse Scava deployment repository:  github.com/crossminer/scava-deployment  Eclipse Scava documentation repository:  github.com/crossminer/scava-docs",
            "title": "Welcome to the Scava documentation"
        },
        {
            "location": "/others/#platform-installation",
            "text": "Docker-SCAVA  How to build and run the Scava docker image.  Running the platform  Quick start guide to get the Scava platform running from source on an Eclipse development environment.  Configuring the platform  Quick start guide to present how to configure the platform using a configuration file.  Docker-OSSMETER  How to build and run the Ossmeter docker image.",
            "title": "Platform installation"
        },
        {
            "location": "/others/#administration",
            "text": "Scava Administration  The administration dashboard take care of managing Scava's services.  API Gateway Configuration  Extending MongoDB Data Model",
            "title": "Administration"
        },
        {
            "location": "/others/#users",
            "text": "Scava metrics  lists metrics computed by the various Scava Components.  Consuming the REST services  This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues  Running Scava in Eclipse  How to setup and run the Scava Eclipse IDE plugin.  REST API Documentation  Reference documentation of REST services provided by the Scava platform.  REST API Generation  Tutorial about automatic generation of REST API Scava library using OpenAPI.  Accessing Scava resources  A summary of where to find the various outputs of the Scava platform.",
            "title": "Users"
        },
        {
            "location": "/others/#development",
            "text": "Contributing  Collection of Architectural and Technical guidelines dedicated to Scava developers.  Development guidelines  Rules and guidelines used for the development of the Scava project.  Testing Guidelines  Collection of testing guidelines dedicated to Scava developers.  Repository-Organisation  How to develop a metric provider  Want to add a new metric provider? Here are some hints.  Licensing  Information about licensing used within Scava.",
            "title": "Development"
        },
        {
            "location": "/others/#architecture",
            "text": "API Gateway Component  The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway.  Authentication Component  The administration dashboard takes care of managing Scava's services.",
            "title": "Architecture"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/",
            "text": "API Gateway configuration\n\n\nWhen to use this guideline ?\n\n\nThis guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.\n\n\nContext\n\n\nThe Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.\n\n\nRouting : Service Configuration\n\n\nTo reference a new remote REST API in the gateway, you have to add  2 new properties in the application.properties configuration file : the relative path of services  which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process.\n\n\nExamples :\n\n api gateway url = http://85.36.10.13:8080\n\n path = /administration/*\n\n\n url = http://85.36.10.12:8082/administration\n\n\nThe request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create\n\n\n\n\nid : \nzuul.routes.**servicename**.path\ndefault :\n NA\n\n\nRelative path of the incoming service which will be redirected. Example : /test1/**\n\n\n\n\n\n\n\nid : \nzuul.routes.**servicename**.url\ndefault :\n NA\n\n\nRedirection URL of the route. Example : http://127.0.0.1:8082/test1\n\n\n\n\n\nConfiguration file example\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2\n\n\n\n\nComments\n\n\nMore information about API Gateway configuration: \nAPI Gateway Component",
            "title": "API Gateway Configuration"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/#api-gateway-configuration",
            "text": "",
            "title": "API Gateway configuration"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/#when-to-use-this-guideline",
            "text": "This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/#context",
            "text": "The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.",
            "title": "Context"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/#routing-service-configuration",
            "text": "To reference a new remote REST API in the gateway, you have to add  2 new properties in the application.properties configuration file : the relative path of services  which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process.  Examples :  api gateway url = http://85.36.10.13:8080  path = /administration/*   url = http://85.36.10.12:8082/administration  The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create   id :  zuul.routes.**servicename**.path default :  NA  Relative path of the incoming service which will be redirected. Example : /test1/**    id :  zuul.routes.**servicename**.url default :  NA  Redirection URL of the route. Example : http://127.0.0.1:8082/test1",
            "title": "Routing : Service Configuration"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/#configuration-file-example",
            "text": "# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Configuration file example"
        },
        {
            "location": "/others/admin/API-Gateway-Configuration/#comments",
            "text": "More information about API Gateway configuration:  API Gateway Component",
            "title": "Comments"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/",
            "text": "When to use ?\n\n\nThis guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.\n\n\nContext\n\n\nThe Scava platform use a MongoDb data base to store his data. We go through \nPONGO\n, a template based \nJava POJO generator\n to access \nMongoDB\n database. With Pongo we can define the data model which generates strongly-typed Java classes.\n\n\nIn this guideligne, we will present  : \n\n The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform.\n\n The access to MongoDb Document on from an external Java application.\n* How to preform basic CRUD operation with a PONGO Java data model.\n\n\nWe consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : \nExtend MongoDB Data Model\n.\n\n\nYou want to access to MongoDB Document from an Eclipse Plugin ?\n\n\n1. Add a dependency to the Java Data Model\n\n\n\n\nEdit the plugin.xml file of your plugin.\n\n\nIn Dependency section, add a dependency to  the plugin which contained the data model you went to access.\n\n\nTo \norg.ossmeter.repository.model\n to access data related to project administration , metric execution process and authentification system.\n\n\nTo \norg.ossmeter.repository.model.'project delta manager'\n to access  configuration informations related to source codes managemeny tools.\n\n\nTo  \nspecific metric provider plugins\n  to access data related to a specific metric provider implementation contains his once data model.\n\n\n... others plugin which contained the data model\n\n\nIn Dependency section, add a dependency to \ncom.googlecode.pongo.runtime\n plugin\n\n\n\n\n\n\n2. Initiate a Connection to the MongoDb\n\n\nIn context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base.\n\n\n\n\nIn Dependency section of plugin.xml file , add a dependency to the \norg.ossmeter.platform\n plugin.\n\n\nYou can now create a new connection to the database using the Configuration service.\n\n\n\n\nMongo mongo = Configuration.getInstance().getMongoConnection(); \n\n\n\n\nYou want to access to MongoDB Document on from an External Java Application ?\n\n\n1. Add a dependency to the Java Data Model\n\n\n\n\nAdd to your java project a dependency to the jar which contained  the data model you went to access. You will have to deliver this jar with your application.\n\n\n\n\n\n\n\n\nAdd o your java project a dependency to the \npongo.jar\n jar file which can be download at this url : https://github.com/kolovos/pongo/releases\n\n\n\n\n2. Initiate a Connection to MongoDb\n\n\n// Define ServerAddress of the MongoDb database\nList<ServerAddress> mongoHostAddresses = new ArrayList<>();\nmongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1])));\n\n// Create Connection\nMongo mongo = new Mongo(mongoHostAddresses);\n\n\n\n\nOnce the connection to MongoDb has been created, you have to make the link  between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document.\n\n\nDB db =  mongo.getDB(\"databasename\");\n\n// Initiate the Project Java model\nProjectRepository = new ProjectRepository(db);\n\n\n\n\nBasic CRUD with a PONGO Java data model\n\n\n1. CREATE\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);\n\n\n\n\n2. READ\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to access object properties\nmetricprovider.getname();\n......\n\n\n\n\n3. UPDATE\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);\n\n\n\n\n4. DELETE\n\n\nMongo mongo = new Mongo();\nmongo.dropDatabase(\"databasename\");\n\n\nComment\n\n\nThis wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here \nExtend MongoDB Data Model\n.",
            "title": "Access to MongoDB database using PONGO"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#when-to-use",
            "text": "This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.",
            "title": "When to use ?"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#context",
            "text": "The Scava platform use a MongoDb data base to store his data. We go through  PONGO , a template based  Java POJO generator  to access  MongoDB  database. With Pongo we can define the data model which generates strongly-typed Java classes.  In this guideligne, we will present  :   The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform.  The access to MongoDb Document on from an external Java application.\n* How to preform basic CRUD operation with a PONGO Java data model.  We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model :  Extend MongoDB Data Model .",
            "title": "Context"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin",
            "text": "",
            "title": "You want to access to MongoDB Document from an Eclipse Plugin ?"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model",
            "text": "Edit the plugin.xml file of your plugin.  In Dependency section, add a dependency to  the plugin which contained the data model you went to access.  To  org.ossmeter.repository.model  to access data related to project administration , metric execution process and authentification system.  To  org.ossmeter.repository.model.'project delta manager'  to access  configuration informations related to source codes managemeny tools.  To   specific metric provider plugins   to access data related to a specific metric provider implementation contains his once data model.  ... others plugin which contained the data model  In Dependency section, add a dependency to  com.googlecode.pongo.runtime  plugin",
            "title": "1. Add a dependency to the Java Data Model"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-the-mongodb",
            "text": "In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base.   In Dependency section of plugin.xml file , add a dependency to the  org.ossmeter.platform  plugin.  You can now create a new connection to the database using the Configuration service.   Mongo mongo = Configuration.getInstance().getMongoConnection();",
            "title": "2. Initiate a Connection to the MongoDb"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application",
            "text": "",
            "title": "You want to access to MongoDB Document on from an External Java Application ?"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model_1",
            "text": "Add to your java project a dependency to the jar which contained  the data model you went to access. You will have to deliver this jar with your application.     Add o your java project a dependency to the  pongo.jar  jar file which can be download at this url : https://github.com/kolovos/pongo/releases",
            "title": "1. Add a dependency to the Java Data Model"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-mongodb",
            "text": "// Define ServerAddress of the MongoDb database\nList<ServerAddress> mongoHostAddresses = new ArrayList<>();\nmongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1])));\n\n// Create Connection\nMongo mongo = new Mongo(mongoHostAddresses);  Once the connection to MongoDb has been created, you have to make the link  between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document.  DB db =  mongo.getDB(\"databasename\");\n\n// Initiate the Project Java model\nProjectRepository = new ProjectRepository(db);",
            "title": "2. Initiate a Connection to MongoDb"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#basic-crud-with-a-pongo-java-data-model",
            "text": "",
            "title": "Basic CRUD with a PONGO Java data model"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#1-create",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);",
            "title": "1. CREATE"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#2-read",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to access object properties\nmetricprovider.getname();\n......",
            "title": "2. READ"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#3-update",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);",
            "title": "3. UPDATE"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#4-delete",
            "text": "Mongo mongo = new Mongo();\nmongo.dropDatabase(\"databasename\");",
            "title": "4. DELETE"
        },
        {
            "location": "/others/admin/Access-to-MongoDB-database-using-PONGO/#comment",
            "text": "This wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here  Extend MongoDB Data Model .",
            "title": "Comment"
        },
        {
            "location": "/others/admin/SCAVA-Administration/",
            "text": "Scava Administration\n\n\nThe SCAVA administration dashboard take care of:\n\n\n\n\nProvide user administration feature, including user profile activation service and roles based authorization management.\n\n\nProvide services to analyse automatically open source software projects.\n\n\n\n\nAdministration Dashboard Installation\n\n\nPrerequired\n\n\nThe SCAVA administration dashboard  is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools:\n\n\nNode.js\n\n\n\n\nDownload Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/\n\n\n\n\nYarn Package Manager\n\n\n\n\nDownload Yarn ver. 1.7.0 or above : https://yarnpkg.com\n\n\n\n\nGet Started Scava Administration\n\n\nScava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).\n\n\nScava Dashboard Deployment\n\n\nIn order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server).\n\n\nUsing the development profile:\n\n\n\n\nExecute the development build using the Angular CLI command line : \nng build\n.\n\n\nCopy everything within the output folder (dist/ by default) to a folder on the server.\n\n\nIf you copy the files into a server sub-folder, append the build flag, --base-href and set the \n appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to \n like this. or simply you can run: \nng build --base-href=/administration/\n\n\n\n\nUsing the production profile:\n\n\n\n\nYou can generate an optimized build with additional CLI command line flags: \nng build -- prod\n.\n\n\nCopy everything within the output folder (dist/ by default) to a folder on the server.\n\n\nIf you copy the files into a server sub-folder, append the build flag, --base-href and set the \n appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to \n like this. or simply you can run: \nng build --base-href=/administration/\n.",
            "title": "SCAVA Administration"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#scava-administration",
            "text": "The SCAVA administration dashboard take care of:   Provide user administration feature, including user profile activation service and roles based authorization management.  Provide services to analyse automatically open source software projects.",
            "title": "Scava Administration"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#administration-dashboard-installation",
            "text": "",
            "title": "Administration Dashboard Installation"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#prerequired",
            "text": "The SCAVA administration dashboard  is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools:",
            "title": "Prerequired"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#nodejs",
            "text": "Download Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/",
            "title": "Node.js"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#yarn-package-manager",
            "text": "Download Yarn ver. 1.7.0 or above : https://yarnpkg.com",
            "title": "Yarn Package Manager"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#get-started-scava-administration",
            "text": "Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).",
            "title": "Get Started Scava Administration"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#scava-dashboard-deployment",
            "text": "In order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server).",
            "title": "Scava Dashboard Deployment"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#using-the-development-profile",
            "text": "Execute the development build using the Angular CLI command line :  ng build .  Copy everything within the output folder (dist/ by default) to a folder on the server.  If you copy the files into a server sub-folder, append the build flag, --base-href and set the   appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to   like this. or simply you can run:  ng build --base-href=/administration/",
            "title": "Using the development profile:"
        },
        {
            "location": "/others/admin/SCAVA-Administration/#using-the-production-profile",
            "text": "You can generate an optimized build with additional CLI command line flags:  ng build -- prod .  Copy everything within the output folder (dist/ by default) to a folder on the server.  If you copy the files into a server sub-folder, append the build flag, --base-href and set the   appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to   like this. or simply you can run:  ng build --base-href=/administration/ .",
            "title": "Using the production profile:"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/",
            "text": "The API Gateway component\n\n\nThe Scava API Gateway :\n\n\n\n\nProvide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application).\n\n\nProvide a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.\n\n\n\n\nAPI Gateway Architecture\n\n\nThe API Gateway  is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change.\n\n\nThe API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the  API Gateway will manage the authentication for all services of the platform.  \n\n\n\n\nAuthentication Mechanism\n\n\nJSON Web Tokens\n\n\nThe Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.\n\n\nIn authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)\n\n\n\n\nAuthentication Architecture\n\n\nIn Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.\n\n\n\n\nAuthentication Flow\n\n\n\n\nTo obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n\n\nWhen the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.\n\n\n\n\n\n\nImplementation\n\n\nThe implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures.\n\n\nhttps://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul\n\n\nAPI Gateway Configuration\n\n\nThe Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.\n\n\nServer Configuration\n\n\n\n\nid : \nserver.port\ndefault :\n 8086\n\n\nPort of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.\n\n\n\n\n\nJWT Security Configuration\n\n\n\n\nid : \napigateway.security.jwt.secret\ndefault :\n NA\n\n\nPrivate key pair which allow to sign jwt tokens using RSA.\n\n\n\n\n\n\n\nid : \napigateway.security.jwt.url\ndefault :\n /login\n\n\nURL Path of the authentication service.\n\n\n\n\n\n\n\nid : \napigateway.security.jwt.expiration\ndefault :\n 86400 (24H)\n\n\nPort of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.\n\n\n\n\n\nRouting : Authentication Service Configuration\n\n\n\n\nid : \nzuul.routes.auth-center.path\ndefault :\n /api/authentication/**\n\n\nRelative path of the authentication service.\n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.url\ndefault :\n NA\n\n\nURL of the authentification server. Example: http://127.0.0.1:8081/ \n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.sensitiveHeaders\ndefault :\n Cookie,Set-Cookie\n\n\nSpecify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers.\n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.stripPrefix\ndefault :\n false\n\n\nSwitch off the stripping of the service-specific prefix from individual routes\n\n\n\n\n\nRouting : Service Configuration\n\n\n\n\nid : \nzuul.routes.**servicename**.path\ndefault :\n NA\n\n\nRelative path of the incoming service which will be redirected. Example : /test1/**\n\n\n\n\n\n\n\nid : \nzuul.routes.**servicename**.url\ndefault :\n NA\n\n\nRedirection URL of the route. Example : http://127.0.0.1:8082/test1\n\n\n\n\n\nConfiguration file example\n\n\n#API Gateway Port\nserver.port=8086\n\n# JWT Configuration\napigateway.security.jwt.secret=otherpeopledontknowit\napigateway.security.jwt.url=/api/authentication\napigateway.security.jwt.expiration=86400\n\n# Rooting Configuration : Authentication Service\nzuul.routes.auth-center.path=/api/authentication/**\nzuul.routes.auth-center.url=http://127.0.0.1:8081/\nzuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie\nzuul.routes.auth-center.stripPrefix=false\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2\n\n\n\n\nControl access API\n\n\nThe Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including:\n\n \u201cROLE_ADMIN\u201d\n\n \u201cROLE_PROJECT_MANAGER\u201d\n* \u201cROLE_USER\u201d\n\n\nBy the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d.\n\n\n# Filtering private restApi\n\nscava.routes.config.adminAccessApi[0]=/api/users\nscava.routes.config.adminAccessApi[1]=/api/user/**\n\nscava.routes.config.projectManagerAccessApi[0]=/administration/projects/create\nscava.routes.config.projectManagerAccessApi[1]=/administration/projects/import\nscava.routes.config.projectManagerAccessApi[2]=/administration/analysis/**\n\nscava.routes.config.userAccessApi[0]=/administration/projects\nscava.routes.config.userAccessApi[1]=/administration/projects/p/**\nscava.routes.config.userAccessApi[2]=/api/users/**\nscava.routes.config.userAccessApi[3]=/api/account\n\n\n\n\nPackaging Form Sources\n\n\nMaven Packaging\n\n\nmvn -Pprod install\n\n\n\n\nAPI Gateway Execution\n\n\n\n\ncomplete an put the \"application.properties\" configuration file in the execute directory.\n\n\nExecute the crossmeter-api-gateway-1.0.0.jar Jar.\n\n\n\n\njava -jar scava-api-gateway-1.0.0.jar\n\n\n\n\nClient Implementation\n\n\nHow to consume a Scava REST services ?\n \\\nThis guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.",
            "title": "API Gateway Component"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#the-api-gateway-component",
            "text": "The Scava API Gateway :   Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application).  Provide a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.",
            "title": "The API Gateway component"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#api-gateway-architecture",
            "text": "The API Gateway  is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change.  The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the  API Gateway will manage the authentication for all services of the platform.",
            "title": "API Gateway Architecture"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#authentication-mechanism",
            "text": "",
            "title": "Authentication Mechanism"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#json-web-tokens",
            "text": "The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.  In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)",
            "title": "JSON Web Tokens"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#authentication-architecture",
            "text": "In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.",
            "title": "Authentication Architecture"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#authentication-flow",
            "text": "To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.  When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.",
            "title": "Authentication Flow"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#implementation",
            "text": "The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures.  https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul",
            "title": "Implementation"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#api-gateway-configuration",
            "text": "The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.",
            "title": "API Gateway Configuration"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#server-configuration",
            "text": "id :  server.port default :  8086  Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.",
            "title": "Server Configuration"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#jwt-security-configuration",
            "text": "id :  apigateway.security.jwt.secret default :  NA  Private key pair which allow to sign jwt tokens using RSA.    id :  apigateway.security.jwt.url default :  /login  URL Path of the authentication service.    id :  apigateway.security.jwt.expiration default :  86400 (24H)  Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.",
            "title": "JWT Security Configuration"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#routing-authentication-service-configuration",
            "text": "id :  zuul.routes.auth-center.path default :  /api/authentication/**  Relative path of the authentication service.    id :  zuul.routes.auth-center.url default :  NA  URL of the authentification server. Example: http://127.0.0.1:8081/     id :  zuul.routes.auth-center.sensitiveHeaders default :  Cookie,Set-Cookie  Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers.    id :  zuul.routes.auth-center.stripPrefix default :  false  Switch off the stripping of the service-specific prefix from individual routes",
            "title": "Routing : Authentication Service Configuration"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#routing-service-configuration",
            "text": "id :  zuul.routes.**servicename**.path default :  NA  Relative path of the incoming service which will be redirected. Example : /test1/**    id :  zuul.routes.**servicename**.url default :  NA  Redirection URL of the route. Example : http://127.0.0.1:8082/test1",
            "title": "Routing : Service Configuration"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#configuration-file-example",
            "text": "#API Gateway Port\nserver.port=8086\n\n# JWT Configuration\napigateway.security.jwt.secret=otherpeopledontknowit\napigateway.security.jwt.url=/api/authentication\napigateway.security.jwt.expiration=86400\n\n# Rooting Configuration : Authentication Service\nzuul.routes.auth-center.path=/api/authentication/**\nzuul.routes.auth-center.url=http://127.0.0.1:8081/\nzuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie\nzuul.routes.auth-center.stripPrefix=false\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Configuration file example"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#control-access-api",
            "text": "The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including:  \u201cROLE_ADMIN\u201d  \u201cROLE_PROJECT_MANAGER\u201d\n* \u201cROLE_USER\u201d  By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d.  # Filtering private restApi\n\nscava.routes.config.adminAccessApi[0]=/api/users\nscava.routes.config.adminAccessApi[1]=/api/user/**\n\nscava.routes.config.projectManagerAccessApi[0]=/administration/projects/create\nscava.routes.config.projectManagerAccessApi[1]=/administration/projects/import\nscava.routes.config.projectManagerAccessApi[2]=/administration/analysis/**\n\nscava.routes.config.userAccessApi[0]=/administration/projects\nscava.routes.config.userAccessApi[1]=/administration/projects/p/**\nscava.routes.config.userAccessApi[2]=/api/users/**\nscava.routes.config.userAccessApi[3]=/api/account",
            "title": "Control access API"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#packaging-form-sources",
            "text": "Maven Packaging  mvn -Pprod install",
            "title": "Packaging Form Sources"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#api-gateway-execution",
            "text": "complete an put the \"application.properties\" configuration file in the execute directory.  Execute the crossmeter-api-gateway-1.0.0.jar Jar.   java -jar scava-api-gateway-1.0.0.jar",
            "title": "API Gateway Execution"
        },
        {
            "location": "/others/architecture/API-Gateway-Component/#client-implementation",
            "text": "How to consume a Scava REST services ?  \\\nThis guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.",
            "title": "Client Implementation"
        },
        {
            "location": "/others/architecture/Authentication-Component/",
            "text": "Authentication Component\n\n\nThe Scava Authentication service:\n\n\n\n\nProvides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform.\n\n\nProvides user management services, including user registration process, user profile editing and roles based authorization management.\n\n\n\n\nAuthentication API\n\n\nThe Authentication server is a component of The Scava platform which manages the authentication for all  services accessible behind the API Gateway.\n\n\n\n\nAuthenticate User\nPOST\n/api/authentication\n\n\nLogin as a registered user.\n\n\n\n\n\n\n\n### JSON Web Tokens (JWT)\n\nJSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are:\n* Header\n* Payload\n* Signature\n\nThis solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/).\n\n### JWT Authentication Implementation\n\n* Users have to login to the authentication service API using their credentials username and password.\n\n\u0002wzxhzdk:0\u0003\n\n* Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header.\n\n\n\n\n* The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway.\n\n\u0002wzxhzdk:1\u0003\n\n\n\n\n## User Management API\n\nThe Authentication component provides web services for CRUD user account.\n\n\n\n\nRegister User\nPOST\n/api/register\n\n\nRegister new user.\n\n\n\n\n\n\nActivate User\nGET\n/api/activate\n\n\nActivate the registered user.\n\n\n\n\n\n\nUpdate User\nPUT\n/api/users\n\n\nUpdate an existing user.\n\n\n\n\n\n\nRetrieve Users\nGET\n/api/users\n\n\nGet all registered users.\n\n\n\n\n\n\nRetrieve Login User\nGET\n/api/users/{login}\n\n\nGet the \"login\" user.\n\n\n\n\n\n\nDelete User\nDELETE\n/api/users/{login}\n\n\nDelete the \"login\" user.\n\n\n\n\n\nAuthentication Server Configuration\n\n\nThe Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.\n\n\nServer Configuration\n\n\n\n\nid : \nserver.port\ndefault :\n 8085\n\n\nPort of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.\n\n\n\n\n\nJWT Security Configuration\n\n\n\n\nid : \napigateway.security.jwt.secret\ndefault :\n NA\n\n\nPrivate key pair which allow to sign jwt tokens using RSA.\n\n\n\n\n\nDefault ADMIN configuration\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nscava.administration.username\n\n\nThe administrator username\n\n\nadmin\n\n\n\n\n\n\nscava.administration.password\n\n\nThe administrator password\n\n\nadmin\n\n\n\n\n\n\nscava.administration.admin-role\n\n\nThe admin role\n\n\nADMIN\n\n\n\n\n\n\nscava.administration.project-manager-role\n\n\nThe project manager role\n\n\nPROJECT_MANAGER\n\n\n\n\n\n\nscava.administration.project-user-role\n\n\nThe user role\n\n\nUSER\n\n\n\n\n\n\n\n\nMongodb Database Configuration\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nspring.data.mongodb.uri\n\n\nUrl of the MongoDB database server\n\n\nmongodb://localhost:27017\n\n\n\n\n\n\nspring.data.mongodb.database\n\n\nName of the MongoDB database\n\n\nscava\n\n\n\n\n\n\n\n\nMail Server configuration\n\n\nIn order to register new users, you have to configure a mail server.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nspring.mail.host\n\n\nUrl of the mail service\n\n\nsmtp.gmail.com\n\n\n\n\n\n\nspring.mail.port\n\n\nPort of the mail service\n\n\n587\n\n\n\n\n\n\nspring.mail.username\n\n\nLogin of the mail account\n\n\n\n\n\n\n\n\nspring.mail.password\n\n\nPassword of the mail account\n\n\n\n\n\n\n\n\nspring.mail.protocol\n\n\nmail protocole\n\n\nsmtp\n\n\n\n\n\n\nspring.mail.tls\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.auth\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.starttls.enable\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.ssl.trust=\n\n\n-\n\n\nsmtp.gmail.com\n\n\n\n\n\n\n\n\nAdministration Dashboard Setting\n\n\n\n\nid : \nscava.administration.base-url\ndefault :\n http://localhost:4200\n\n\nThe SCAVA administration base URL to generate the activation account URL.\n\n\n\n\n\nPackaging From Sources\n\n\nMaven Packaging\n\n\nmvn -Pprod install\n\n\n\n\nAuthentication Server Execution\n\n\n\n\ncomplete an put the \"application.properties\" configuration file in the execution directory.\n\n\nExecute the scava-auth-service-1.0.0.jar Jar.\n\n\n\n\njava -jar scava-auth-service-1.0.0.jar",
            "title": "Authentication Component"
        },
        {
            "location": "/others/architecture/Authentication-Component/#authentication-component",
            "text": "The Scava Authentication service:   Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform.  Provides user management services, including user registration process, user profile editing and roles based authorization management.",
            "title": "Authentication Component"
        },
        {
            "location": "/others/architecture/Authentication-Component/#authentication-api",
            "text": "The Authentication server is a component of The Scava platform which manages the authentication for all  services accessible behind the API Gateway.   Authenticate User POST /api/authentication  Login as a registered user.   \n\n### JSON Web Tokens (JWT)\n\nJSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are:\n* Header\n* Payload\n* Signature\n\nThis solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/).\n\n### JWT Authentication Implementation\n\n* Users have to login to the authentication service API using their credentials username and password.\n\n\u0002wzxhzdk:0\u0003\n\n* Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. \n\n* The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway.\n\n\u0002wzxhzdk:1\u0003 \n\n## User Management API\n\nThe Authentication component provides web services for CRUD user account.  Register User POST /api/register  Register new user.    Activate User GET /api/activate  Activate the registered user.    Update User PUT /api/users  Update an existing user.    Retrieve Users GET /api/users  Get all registered users.    Retrieve Login User GET /api/users/{login}  Get the \"login\" user.    Delete User DELETE /api/users/{login}  Delete the \"login\" user.",
            "title": "Authentication API"
        },
        {
            "location": "/others/architecture/Authentication-Component/#authentication-server-configuration",
            "text": "The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.",
            "title": "Authentication Server Configuration"
        },
        {
            "location": "/others/architecture/Authentication-Component/#server-configuration",
            "text": "id :  server.port default :  8085  Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.",
            "title": "Server Configuration"
        },
        {
            "location": "/others/architecture/Authentication-Component/#jwt-security-configuration",
            "text": "id :  apigateway.security.jwt.secret default :  NA  Private key pair which allow to sign jwt tokens using RSA.",
            "title": "JWT Security Configuration"
        },
        {
            "location": "/others/architecture/Authentication-Component/#default-admin-configuration",
            "text": "Property  Description  Default Value      scava.administration.username  The administrator username  admin    scava.administration.password  The administrator password  admin    scava.administration.admin-role  The admin role  ADMIN    scava.administration.project-manager-role  The project manager role  PROJECT_MANAGER    scava.administration.project-user-role  The user role  USER",
            "title": "Default ADMIN configuration"
        },
        {
            "location": "/others/architecture/Authentication-Component/#mongodb-database-configuration",
            "text": "Property  Description  Default Value      spring.data.mongodb.uri  Url of the MongoDB database server  mongodb://localhost:27017    spring.data.mongodb.database  Name of the MongoDB database  scava",
            "title": "Mongodb Database Configuration"
        },
        {
            "location": "/others/architecture/Authentication-Component/#mail-server-configuration",
            "text": "In order to register new users, you have to configure a mail server.     Property  Description  Default Value      spring.mail.host  Url of the mail service  smtp.gmail.com    spring.mail.port  Port of the mail service  587    spring.mail.username  Login of the mail account     spring.mail.password  Password of the mail account     spring.mail.protocol  mail protocole  smtp    spring.mail.tls  -  true    spring.mail.properties.mail.smtp.auth  -  true    spring.mail.properties.mail.smtp.starttls.enable  -  true    spring.mail.properties.mail.smtp.ssl.trust=  -  smtp.gmail.com",
            "title": "Mail Server configuration"
        },
        {
            "location": "/others/architecture/Authentication-Component/#administration-dashboard-setting",
            "text": "id :  scava.administration.base-url default :  http://localhost:4200  The SCAVA administration base URL to generate the activation account URL.",
            "title": "Administration Dashboard Setting"
        },
        {
            "location": "/others/architecture/Authentication-Component/#packaging-from-sources",
            "text": "Maven Packaging  mvn -Pprod install",
            "title": "Packaging From Sources"
        },
        {
            "location": "/others/architecture/Authentication-Component/#authentication-server-execution",
            "text": "complete an put the \"application.properties\" configuration file in the execution directory.  Execute the scava-auth-service-1.0.0.jar Jar.   java -jar scava-auth-service-1.0.0.jar",
            "title": "Authentication Server Execution"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/",
            "text": "Docker Ossmeter\n\n\nThis page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old \nOssmeter binaries\n and will not be updated -- all new development will go into the Crossminer repository.\n\n\nAll images are stored on the \nCrossminer Docker-hub account\n.\n\n\nThe Docker image is composed of 4 services: \n\n\n\n\noss-web: corresponds to the service of OSSMETER platform website.\n\n\noss-app: service running api server and the orchestrator of OSSMETER slave instances.\n\n\noss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing.\n\n\noss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image.\n\n\n\n\nThe database comes pre-populated with a project and a user. The loaded dump comes from \nmd2manoppello's repo\n. Login information:\n\n\n\n\nuser: \ndemo@crossminer.org\n\n\npassword: \ndemo18\n\n\n\n\nCustom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the \ndemo quality model\n.\n\n\nRunning the Ossmeter docker image\n\n\nThe easiest way to build the full stack is to run the docker-compose file:\n\n\n$ docker-compose up\n\n\n\n\nThis command will download the images and run them. The application is then available on \nlocalhost:9000\n.\n\n\nBuilding the Ossmeter docker image\n\n\nTwo containers actually need to be built. They can be built individually.\n\n\noss-platform\n\n\nBuild the image from the oss-platform directory:\n\n\n$ docker build -t bbaldassari/ossmeter-platform .\nSending build context to Docker daemon  3.072kB\nStep 1/5 : FROM openjdk:8-jdk\n\n\n\n\noss-web\n\n\nBuild the image from the oss-web directory:\n\n\n$ docker build -t bbaldassari/ossmeter-web .\nSending build context to Docker daemon  3.072kB\nStep 1/7 : FROM openjdk:8-jre-alpine\n\n\n\n\nContinuous integration\n\n\nWe use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the \n#ci\n channel in Slack.",
            "title": "Docker Ossmeter"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/#docker-ossmeter",
            "text": "This page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old  Ossmeter binaries  and will not be updated -- all new development will go into the Crossminer repository.  All images are stored on the  Crossminer Docker-hub account .  The Docker image is composed of 4 services:    oss-web: corresponds to the service of OSSMETER platform website.  oss-app: service running api server and the orchestrator of OSSMETER slave instances.  oss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing.  oss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image.   The database comes pre-populated with a project and a user. The loaded dump comes from  md2manoppello's repo . Login information:   user:  demo@crossminer.org  password:  demo18   Custom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the  demo quality model .",
            "title": "Docker Ossmeter"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/#running-the-ossmeter-docker-image",
            "text": "The easiest way to build the full stack is to run the docker-compose file:  $ docker-compose up  This command will download the images and run them. The application is then available on  localhost:9000 .",
            "title": "Running the Ossmeter docker image"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/#building-the-ossmeter-docker-image",
            "text": "Two containers actually need to be built. They can be built individually.",
            "title": "Building the Ossmeter docker image"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/#oss-platform",
            "text": "Build the image from the oss-platform directory:  $ docker build -t bbaldassari/ossmeter-platform .\nSending build context to Docker daemon  3.072kB\nStep 1/5 : FROM openjdk:8-jdk",
            "title": "oss-platform"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/#oss-web",
            "text": "Build the image from the oss-web directory:  $ docker build -t bbaldassari/ossmeter-web .\nSending build context to Docker daemon  3.072kB\nStep 1/7 : FROM openjdk:8-jre-alpine",
            "title": "oss-web"
        },
        {
            "location": "/others/deploy/Docker-Ossmeter/#continuous-integration",
            "text": "We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the  #ci  channel in Slack.",
            "title": "Continuous integration"
        },
        {
            "location": "/others/deploy/Platform-configuration/",
            "text": "When starting the platform, you can pass a configuration file to control the behaviour of the platform:\n\n\n./eclipse -slave -config myconfiguration.properties\n\n\n\n\nThe configuration file is a typical Java properties file. The properties that can be configured are:\n\n\nidentifier=<your name>\n\n\nThe identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. \n\n\nIf you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.\n\n\nlog.type=console|file|rolling\n\n\nYou can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify \nfile\n or \nrolling\n, you must complete the \nlog.file.path\n or \nlog.rolling.path\n property as well. \n\n\nIf the property is not specified, it will default to the console logger.\n\n\nlog.file.path=<path>\n\n\nThe path to the file to store the log. E.g. \nlog.file.path=/tmp/lovelylog.log\n\n\nlog.rolling.path=<path>\n\n\nThe path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify \nlog.rolling.path=/tmp/mylovelylog.log\n, it will store files like so: \n/tmp/mylovelylog.log.2014-12-17-00\n and \n/tmp/mylovelylog.log.2014-12-17-12\n.\n\n\nmaven_executable=<path>\n\n\nThe path to where Maven is installed. E.g. \nmaven_executable=/usr/bin/mvn\n\n\nstorage_path=<path>\n\n\nThe path to where files should be stored. E.g. \nstorage_path=/mnt/ossmeter/\n\n\nmongo_hosts\n\n\nA comma-separated list of the hosts and ports in a replica set. E.g. \nua002:27017,ua009:27017,ua019:27017,ua020:27017",
            "title": "Platform configuration"
        },
        {
            "location": "/others/deploy/Platform-configuration/#identifieryour-name",
            "text": "The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID.   If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.",
            "title": "identifier=&lt;your name&gt;"
        },
        {
            "location": "/others/deploy/Platform-configuration/#logtypeconsolefilerolling",
            "text": "You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify  file  or  rolling , you must complete the  log.file.path  or  log.rolling.path  property as well.   If the property is not specified, it will default to the console logger.",
            "title": "log.type=console|file|rolling"
        },
        {
            "location": "/others/deploy/Platform-configuration/#logfilepathpath",
            "text": "The path to the file to store the log. E.g.  log.file.path=/tmp/lovelylog.log",
            "title": "log.file.path=&lt;path&gt;"
        },
        {
            "location": "/others/deploy/Platform-configuration/#logrollingpathpath",
            "text": "The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify  log.rolling.path=/tmp/mylovelylog.log , it will store files like so:  /tmp/mylovelylog.log.2014-12-17-00  and  /tmp/mylovelylog.log.2014-12-17-12 .",
            "title": "log.rolling.path=&lt;path&gt;"
        },
        {
            "location": "/others/deploy/Platform-configuration/#maven_executablepath",
            "text": "The path to where Maven is installed. E.g.  maven_executable=/usr/bin/mvn",
            "title": "maven_executable=&lt;path&gt;"
        },
        {
            "location": "/others/deploy/Platform-configuration/#storage_pathpath",
            "text": "The path to where files should be stored. E.g.  storage_path=/mnt/ossmeter/",
            "title": "storage_path=&lt;path&gt;"
        },
        {
            "location": "/others/deploy/Platform-configuration/#mongo_hosts",
            "text": "A comma-separated list of the hosts and ports in a replica set. E.g.  ua002:27017,ua009:27017,ua019:27017,ua020:27017",
            "title": "mongo_hosts"
        },
        {
            "location": "/others/deploy/Running-the-platform/",
            "text": "Running the platform\n\n\nThis is a quick start guide to get the OSSMETER platform running from source.\n\n\nAlthough these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package).\n\n\nA step-by-step video guide is also available at \nhttps://youtu.be/3Ry4KKfNdYg\n\n\nStart MongoDB\n\n\nYou can download MongoDB from the \nMongoDb website\n.\n\n\nInstructions for starting mongo can be found in the MongoDB \nmanual\n. For example:\n\n\nmongod --dbpath /data/db --port 27017\n\n\n\n\nGet the Code\n\n\nGet the latest version of the code, and checkout the \ndev\n branch. Please don't commit to the \nmaster\n branch: see the \nDevelopment Guidelines\n:\n\n\nIf you are using \nLinux / OS X\n:\n\n\ngit clone https://github.com/crossminer/scava.git scava\ncd scava\ngit checkout dev\n\n\n\n\nIf you are using \nWindows\n you need to do things differently due to Windows' long file name limit. In the Git shell:\n\n\nmkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/crossminer/scava.git\ngit fetch\ngit checkout dev\n\n\n\n\nSetup Eclipse\n\n\nOpen Eclipse and import all projects from the top level directory of the Scava code (\nFile -> Import -> Existing projects into workspace\n), and wait for all the projects to compile without errors.\n\n\nValidate and Run the Platform\n\n\nOpen \norg.ossmeter.platform.osgi/ossmeterfromfeature.product\n\n  * Click the \nValidate...\n icon in the top right of the product configuration editor (the icon is a piece of paper with a tick)\n  * If things do not validate, there's something wrong -- get in touch :) Problems related to \norg.eclipse.e4.core.di\n aren't critical.\n  * Then, click the \nExport an Eclipse product\n on the left of the \nValidate...\n button. Uncheck the \nGenerate p2 repository\n checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory.\n  * The platform can then be run using the generated \neclipse\n binary; it accepts the following arguments:\n    * \n-apiServer\n: Starts up the client API on localhost:8182\n    * \n-worker ${id-worker}\n: Spawns a thread that analyses registered projects\n  * To get a full platform running, first launch a master thread, then a slave, and finally the API server.\n\n\nIf you are developing code for the Scava platform, be sure to check out the \nContributing\n.\n\n\nRun the api-gateway\n\n\n\n\nRight click on\n\nscava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java\n\n\nThen click on Run As -> Java Application\n\n\n\n\nRun the authentication service\n\n\n\n\nRight click on\n\nscava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java\n\n\nThen click on Run As -> Java Application\n\n\n\n\nRun the administration dashboard\n\n\nScava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).\n\n\nThe following instructions show how to run the dashboard web app:\n  * Enter the \nadministration/scava-administration/\n directory within the scava repository.\n  * Run the web app on port 4200 using angular-cli: \nng serve\n\n  * Navigate to \nhttp://localhost:4200/",
            "title": "Running the platform"
        },
        {
            "location": "/others/deploy/Running-the-platform/#running-the-platform",
            "text": "This is a quick start guide to get the OSSMETER platform running from source.  Although these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package).  A step-by-step video guide is also available at  https://youtu.be/3Ry4KKfNdYg",
            "title": "Running the platform"
        },
        {
            "location": "/others/deploy/Running-the-platform/#start-mongodb",
            "text": "You can download MongoDB from the  MongoDb website .  Instructions for starting mongo can be found in the MongoDB  manual . For example:  mongod --dbpath /data/db --port 27017",
            "title": "Start MongoDB"
        },
        {
            "location": "/others/deploy/Running-the-platform/#get-the-code",
            "text": "Get the latest version of the code, and checkout the  dev  branch. Please don't commit to the  master  branch: see the  Development Guidelines :  If you are using  Linux / OS X :  git clone https://github.com/crossminer/scava.git scava\ncd scava\ngit checkout dev  If you are using  Windows  you need to do things differently due to Windows' long file name limit. In the Git shell:  mkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/crossminer/scava.git\ngit fetch\ngit checkout dev",
            "title": "Get the Code"
        },
        {
            "location": "/others/deploy/Running-the-platform/#setup-eclipse",
            "text": "Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Existing projects into workspace ), and wait for all the projects to compile without errors.",
            "title": "Setup Eclipse"
        },
        {
            "location": "/others/deploy/Running-the-platform/#validate-and-run-the-platform",
            "text": "Open  org.ossmeter.platform.osgi/ossmeterfromfeature.product \n  * Click the  Validate...  icon in the top right of the product configuration editor (the icon is a piece of paper with a tick)\n  * If things do not validate, there's something wrong -- get in touch :) Problems related to  org.eclipse.e4.core.di  aren't critical.\n  * Then, click the  Export an Eclipse product  on the left of the  Validate...  button. Uncheck the  Generate p2 repository  checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory.\n  * The platform can then be run using the generated  eclipse  binary; it accepts the following arguments:\n    *  -apiServer : Starts up the client API on localhost:8182\n    *  -worker ${id-worker} : Spawns a thread that analyses registered projects\n  * To get a full platform running, first launch a master thread, then a slave, and finally the API server.  If you are developing code for the Scava platform, be sure to check out the  Contributing .",
            "title": "Validate and Run the Platform"
        },
        {
            "location": "/others/deploy/Running-the-platform/#run-the-api-gateway",
            "text": "Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java  Then click on Run As -> Java Application",
            "title": "Run the api-gateway"
        },
        {
            "location": "/others/deploy/Running-the-platform/#run-the-authentication-service",
            "text": "Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java  Then click on Run As -> Java Application",
            "title": "Run the authentication service"
        },
        {
            "location": "/others/deploy/Running-the-platform/#run-the-administration-dashboard",
            "text": "Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).  The following instructions show how to run the dashboard web app:\n  * Enter the  administration/scava-administration/  directory within the scava repository.\n  * Run the web app on port 4200 using angular-cli:  ng serve \n  * Navigate to  http://localhost:4200/",
            "title": "Run the administration dashboard"
        },
        {
            "location": "/others/development/Component-Naming/",
            "text": "As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project.\n\n\nIn this section, \"component\" means big funcional componen of the SCAVA project.\n\n\n\n\n\n\n\n\nComponent\n\n\nComponentId\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\ndashboard\n\n\n\n\n\n\nWorkflow Execution Engine\n\n\nworkflow\n\n\n\n\n\n\nKnowledge Base\n\n\nknowledgebase\n\n\n\n\n\n\nMetric Provider\n\n\nmetricprovider\n\n\n\n\n\n\nAdministration\n\n\nadministration\n\n\n\n\n\n\n\n\nProject Naming\n\n\n\n\nFor Eclipse Plugin \n\n\n\n\norg.eclipse.scava.{componentname}\n\n\n\n\n\n\nFor Maven Project : the name of the project if the ArtifactId\n\n\n\n\nIf your component is composed of one project :\n\n\n{component-name}\n\n\n\n\nIf your component is composed of several sub projects :\n\n\n{sub-component-name}\n ```\n\n## Source Code Namsespace\nAll sources must be nemspaces by : \n\n\n\n\norg.eclipse.scava.{componentname}\n\n\n\n## Maven Ids\nFor  the Maven projects:\n\n* If your component is composed of one project : \n\n\n\n\nGroup Id : org.eclipse.scava\nArtifactId : {component-name}\n\n\n\n* If your component is composed of several sub projects : \n\n\n\n\nGroup Id : org.eclipse.scava.{component-name}\nArtifactId : {sub-component-name}\n```",
            "title": "Component Naming"
        },
        {
            "location": "/others/development/Component-Naming/#project-naming",
            "text": "For Eclipse Plugin    org.eclipse.scava.{componentname}   For Maven Project : the name of the project if the ArtifactId   If your component is composed of one project :  {component-name}  If your component is composed of several sub projects :  {sub-component-name}\n ```\n\n## Source Code Namsespace\nAll sources must be nemspaces by :   org.eclipse.scava.{componentname}  \n## Maven Ids\nFor  the Maven projects:\n\n* If your component is composed of one project :   Group Id : org.eclipse.scava\nArtifactId : {component-name}  \n* If your component is composed of several sub projects :   Group Id : org.eclipse.scava.{component-name}\nArtifactId : {sub-component-name}\n```",
            "title": "Project Naming"
        },
        {
            "location": "/others/development/Contributing/",
            "text": "Contributing\n\n\nSubcategories\n\n\n\n\nSCAVA Repository Organisation\n Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository.\n\n\nHow to name SCAVA components?\n Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc.\n\n\nHow to name SCAVA REST services?\n This guideline provide naming rules for each REST services routes implemented by the SCAVA platform.\n\n\nHow to manage  Licensing\n Guideline describing licensing requirements for SCAVA components.\n\n\n\n\nTechnical Guidelines\n\n\nREST API\n\n\nEach implemented REST services must be documented (see /users directory): \nREST API DOCUMENTATION\n\n\n\n\nHow to configure the SCAVA Gateway in order to integrate a new  REST service\n Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new  remote service provider.\n\n\nHow to consume a SCAVA REST services\n This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues.\n\n\nHow to implement Restlet services\n Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework.\n\n\n\n\nDATA ACCESS\n\n\n\n\n\n\nHow to access MongoDB database using PONGO\n Guideline which describe how to the access to MongoDB database using the  PONGO framework.\n\n\n\n\n\n\nHow to extend the SCAVA data model\n Guideline  which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework.\n\n\n\n\n\n\nOSGI\n\n\n\n\nHow to integrate OSGI service plugin in SCAVA Architecture\n Todo\n\n\nHow to communicate between OSGI plugin using JMS\n Todo",
            "title": "Contributing"
        },
        {
            "location": "/others/development/Contributing/#contributing",
            "text": "",
            "title": "Contributing"
        },
        {
            "location": "/others/development/Contributing/#subcategories",
            "text": "SCAVA Repository Organisation  Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository.  How to name SCAVA components?  Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc.  How to name SCAVA REST services?  This guideline provide naming rules for each REST services routes implemented by the SCAVA platform.  How to manage  Licensing  Guideline describing licensing requirements for SCAVA components.",
            "title": "Subcategories"
        },
        {
            "location": "/others/development/Contributing/#technical-guidelines",
            "text": "",
            "title": "Technical Guidelines"
        },
        {
            "location": "/others/development/Contributing/#rest-api",
            "text": "Each implemented REST services must be documented (see /users directory):  REST API DOCUMENTATION   How to configure the SCAVA Gateway in order to integrate a new  REST service  Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new  remote service provider.  How to consume a SCAVA REST services  This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues.  How to implement Restlet services  Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework.",
            "title": "REST API"
        },
        {
            "location": "/others/development/Contributing/#data-access",
            "text": "How to access MongoDB database using PONGO  Guideline which describe how to the access to MongoDB database using the  PONGO framework.    How to extend the SCAVA data model  Guideline  which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework.",
            "title": "DATA ACCESS"
        },
        {
            "location": "/others/development/Contributing/#osgi",
            "text": "How to integrate OSGI service plugin in SCAVA Architecture  Todo  How to communicate between OSGI plugin using JMS  Todo",
            "title": "OSGI"
        },
        {
            "location": "/others/development/Development-Guidelines/",
            "text": "Development Guidelines\n\n\nIntroduction\n\n\nThis document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.\n\n\nProcess overview\n\n\nThe development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system.\n\n\nIt is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests.\n\n\nThe different elements shown below are individually described in the remaining of the section.\n\n\n\nFigure 1. Overview of the CROSSMINER development process and tools\n\n\nSource Code Repository\n\n\nDifferent branches will be created in the repository. In particular, the \nmaster\n branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The \ndev\n branch contains the most recent code integrated by partners before release.\n\n\nOther branches will be created for the different features/components, which will be eventually merged in the \ndev\n branch. The name of each branch contains the id of the issue it is supposed to implement.\n\n\nAll source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository.\n\n\n\n\nCode: https://github.com/crossminer/scava\n\n\nProduct documentation: https://github.com/crossminer/scava-docs\n\n\nProduct deployment: https://github.com/crossminer/scava-deployment\n\n\n\n\nSince all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details).\n\n\nBy taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3.\n\n\n\nFigure 2: Branching model\n\n\nTests\n\n\nEach branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below.\n\n\n\nFigure 2: Explanatory master and branches\n\n\nBRANCH: Set of branches\nTEST: Set of tests\nINTGRATION_TEST < TEST\nUNIT_TEST < TEST\n\nfunction test: BRANCH -> TEST\n\ntest(master) -> test(cool-feature)\n    \u22c3 {itu, \u2026 itz}\n\n\n\n\nContinuous Integration Server\n\n\nContinuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus \nreduce cost and time\n.\n\n\nA complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all \nunit tests\n. \nIntegration tests\n are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities:\n\n\n\n\nAllow developers to write unit tests that can be executed automatically\n\n\nPerform automated tests against newly written code\n\n\nShow a list of tests that have passed and failed\n\n\nQuality analysis on source code\n\n\nPerform all the necessary actions to create a fully functioning build of the software when all tests have passed\n\n\n\n\nThe currently used CI server is available at http://ci5.castalia.camp:8080/\n\n\nDevelopment and Production environments\n\n\nAccording to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.\n\n\nNaming and change conventions\n\n\nThe repository will contain the code of the high-level components shown in Table 1\n\n\n\n\n\n\n\n\nComponents\n\n\nCorresponding folder in repository\n\n\nLeader\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\n\n\nBIT\n\n\n\n\n\n\nWorkflow Diagram Editor\n\n\n/crossflow\n\n\nYORK\n\n\n\n\n\n\nAdministration Web Application\n\n\n/administration\n\n\nSFT\n\n\n\n\n\n\nIDE\n\n\n/eclipse-based-ide\n\n\nFEA\n\n\n\n\n\n\nAPI Gateway\n\n\n/api-gateway\n\n\nSFT\n\n\n\n\n\n\nDevOps Backend\n\n\n\n\nBIT\n\n\n\n\n\n\nKnowledge Base\n\n\n/knowledge-base\n\n\nUDA\n\n\n\n\n\n\nProject Analyser\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nData Collector\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nProject Scheduler\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nMetric Providers\n\n\n/metric-platform\n\n\nYORK, CWI, AUEB, EHU\n\n\n\n\n\n\nData Storage\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nWeb-based dashboards\n\n\n/web-dashboards\n\n\nBIT\n\n\n\n\n\n\n\n\nTable 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes\n\n\n\nFigure 2: Scava components architecture\n\n\nFor each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus:\n\n\n\n\nwhen developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons;\n\n\nwhen a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR.\n\n\n\n\nThe partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder:\n\n\n\n\nreadme.md\n : this \ufb01le will contain the entry-point to all documentation for the component.\n\n\nJenkinsfile\n: in order to compile the component, run tests, etc on the CI server.\n\n\ntest\n: this is a folder containing the unit and/or integration tests.\n\n\n\n\nAs a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base\n\n\nThe following items are typical practices when applying CI:\n\n Maintain a code repository\n\n Automate the build\n\n Make the build self-testing\n\n Everyone commits to the baseline every day\n\n Every commit (to baseline) should be built\n\n Keep the build fast\n\n Test in a clone of the production environment\n\n Make it easy to get the latest deliverables\n\n Everyone can see the results of the latest build\n\n Automate deployment\n\n\nCommunication and collaboration means\n\n\nThe development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of:\n\n\n\n\nSlack channel: http://crossminer.slack.com\n\n\nEclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev\n\n\nGitHub repository for documentation: https://github.com/crossminer/scava-docs/\n\n\n\n\nRemark\n\n\nI recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise:\n\n\n\n\nto have all the deliverables accepted !!!\n\n\nto develop in a way, which is preparatory to achieve our \u201cdreams\u201d\n\n\n\n\n\u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)",
            "title": "Development Guidelines"
        },
        {
            "location": "/others/development/Development-Guidelines/#development-guidelines",
            "text": "",
            "title": "Development Guidelines"
        },
        {
            "location": "/others/development/Development-Guidelines/#introduction",
            "text": "This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.",
            "title": "Introduction"
        },
        {
            "location": "/others/development/Development-Guidelines/#process-overview",
            "text": "The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system.  It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests.  The different elements shown below are individually described in the remaining of the section.  \nFigure 1. Overview of the CROSSMINER development process and tools",
            "title": "Process overview"
        },
        {
            "location": "/others/development/Development-Guidelines/#source-code-repository",
            "text": "Different branches will be created in the repository. In particular, the  master  branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The  dev  branch contains the most recent code integrated by partners before release.  Other branches will be created for the different features/components, which will be eventually merged in the  dev  branch. The name of each branch contains the id of the issue it is supposed to implement.  All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository.   Code: https://github.com/crossminer/scava  Product documentation: https://github.com/crossminer/scava-docs  Product deployment: https://github.com/crossminer/scava-deployment   Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details).  By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3.  \nFigure 2: Branching model",
            "title": "Source Code Repository"
        },
        {
            "location": "/others/development/Development-Guidelines/#tests",
            "text": "Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below.  \nFigure 2: Explanatory master and branches  BRANCH: Set of branches\nTEST: Set of tests\nINTGRATION_TEST < TEST\nUNIT_TEST < TEST\n\nfunction test: BRANCH -> TEST\n\ntest(master) -> test(cool-feature)\n    \u22c3 {itu, \u2026 itz}",
            "title": "Tests"
        },
        {
            "location": "/others/development/Development-Guidelines/#continuous-integration-server",
            "text": "Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus  reduce cost and time .  A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all  unit tests .  Integration tests  are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities:   Allow developers to write unit tests that can be executed automatically  Perform automated tests against newly written code  Show a list of tests that have passed and failed  Quality analysis on source code  Perform all the necessary actions to create a fully functioning build of the software when all tests have passed   The currently used CI server is available at http://ci5.castalia.camp:8080/",
            "title": "Continuous Integration Server"
        },
        {
            "location": "/others/development/Development-Guidelines/#development-and-production-environments",
            "text": "According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.",
            "title": "Development and Production environments"
        },
        {
            "location": "/others/development/Development-Guidelines/#naming-and-change-conventions",
            "text": "The repository will contain the code of the high-level components shown in Table 1     Components  Corresponding folder in repository  Leader      DevOps Dashboard   BIT    Workflow Diagram Editor  /crossflow  YORK    Administration Web Application  /administration  SFT    IDE  /eclipse-based-ide  FEA    API Gateway  /api-gateway  SFT    DevOps Backend   BIT    Knowledge Base  /knowledge-base  UDA    Project Analyser  /metric-platform  SFT    Data Collector  /metric-platform  SFT    Project Scheduler  /metric-platform  SFT    Metric Providers  /metric-platform  YORK, CWI, AUEB, EHU    Data Storage  /metric-platform  SFT    Web-based dashboards  /web-dashboards  BIT     Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes  \nFigure 2: Scava components architecture  For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus:   when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons;  when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR.   The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder:   readme.md  : this \ufb01le will contain the entry-point to all documentation for the component.  Jenkinsfile : in order to compile the component, run tests, etc on the CI server.  test : this is a folder containing the unit and/or integration tests.   As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base  The following items are typical practices when applying CI:  Maintain a code repository  Automate the build  Make the build self-testing  Everyone commits to the baseline every day  Every commit (to baseline) should be built  Keep the build fast  Test in a clone of the production environment  Make it easy to get the latest deliverables  Everyone can see the results of the latest build  Automate deployment",
            "title": "Naming and change conventions"
        },
        {
            "location": "/others/development/Development-Guidelines/#communication-and-collaboration-means",
            "text": "The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of:   Slack channel: http://crossminer.slack.com  Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev  GitHub repository for documentation: https://github.com/crossminer/scava-docs/",
            "title": "Communication and collaboration means"
        },
        {
            "location": "/others/development/Development-Guidelines/#remark",
            "text": "I recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise:   to have all the deliverables accepted !!!  to develop in a way, which is preparatory to achieve our \u201cdreams\u201d   \u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)",
            "title": "Remark"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/",
            "text": "Extending MongoDB data model\n\n\nWhen to use ?\n\n\nIn this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order  the data layer of Scava platform during the evolution.\n\n\nContext\n\n\nThe Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class.\n\n\nEach MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model.\n\n\nThe current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins :\n\n\n\n\nThe Platform data model (org.ossmeter.repository.model)\n : Contains data related to project administration ,  metric execution process and authentification system.\n\n\nSource Code Repositroy managers (org.ossmeter.repository.model.'project delta manager')\n : Contains configuration informations related to source codes managemeny tools.\n\n\nMetric Providers data models (in each metric provider plugins)\n :  Each metric provider implementation contains his once data model.\n\n\n\n\nYou need to Extend an Existing Data Model ?\n\n\nThe first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.\n\n\n1. Locate the *.emf file of this data model\n\n\nA presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model.\n\n\nEx : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)\n\n\n2. Update the Data  Model description\n\n\nA data model description file contains a statical description of a MongoDb document.\n\n\n@db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n    attr String stackTrace;\n}\n...\n\n\n\n\nIf we would like to add one more attribute to the element \nProjectError\n, we could add it this way :\n\n\n@db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n        .\n        .\n    attr String stackTrace;\n+       attr String TestAttribute;\n}\n...\n\n\n\n\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines\n\n\n3. Generate the Java Class using the Pongo Tool.\n\n\n\n\nDownload the Pongo tool : https://github.com/kolovos/pongo/releases\n\n\nRun the Pongo generator from the command line as follows: \njava -jar pongo.jar  youremffile.emf\n\n\nReplace the existing Java class by the new generated java class.\n\n\n\n\nMore information about Pongo  : https://github.com/kolovos/pongo/wiki\n\n\nYou need to Create a new Data Model ?\n\n\nThe second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model.\n\n\nIn this case, we invite you to create a new plugin which will contain your data model.\n\n\n1. Create a new Eclipse Plug-In\n\n\n\n\nCreate a new Eclipse Plug-In Project (\n\n\nIn Eclipse Toolbar : File > New > Plug-In Project\n\n\nName of the project : org.scava.\nmycomponent\n.repository.model\n\n\nDisable the generation of an Activator Class / contribution to the ui\n\n\nEdit the MANIFEST.MF file\n\n\nIn Dependency : add a dependency to the \norg.eclipse.core.runtime\n plugin\n\n\nIn Dependency : add a dependency to the \ncom.googlecode.pongo.runtime\n plugin\n\n\nIn Dependency : add a dependency to the \norg.apache.commons.lang3\n plugin\n\n\nIn Extentions : reference an extension point named \ncom.googlecode.pongo.runtime.osgi\n\n\nIn source directory\n\n\nCreate a package named org.scava.\nmycomponent\n.repository.model\n\n\nIn this package create an emf file named \nmycomponent.emf\n\n\n\n\nA presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file :\n\n\n```package org.scava.mycomponent.repository.model;\n\n\n@db\nclass MyComponent {\n     ....\n}\n```\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines\n\n\n2. Generate the Java Class using the Pongo Tool.\n\n\n\n\nDownload the Pongo tool : https://github.com/kolovos/pongo/releases\n\n\nRun the Pongo generator from the command line as follows: \njava -jar pongo.jar  youremffile.emf\n\n\nAdd this class in your org.scava.\nmycomponent\n.repository.model package\n\n\n\n\nMore information about Pongo  : https://github.com/kolovos/pongo/wiki\n\n\nComment\n\n\nHere we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs \nlink here\n.",
            "title": "Extend MongoDB Data Model"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#extending-mongodb-data-model",
            "text": "",
            "title": "Extending MongoDB data model"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#when-to-use",
            "text": "In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order  the data layer of Scava platform during the evolution.",
            "title": "When to use ?"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#context",
            "text": "The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class.  Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model.  The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins :   The Platform data model (org.ossmeter.repository.model)  : Contains data related to project administration ,  metric execution process and authentification system.  Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager')  : Contains configuration informations related to source codes managemeny tools.  Metric Providers data models (in each metric provider plugins)  :  Each metric provider implementation contains his once data model.",
            "title": "Context"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#you-need-to-extend-an-existing-data-model",
            "text": "The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.",
            "title": "You need to Extend an Existing Data Model ?"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#1-locate-the-emf-file-of-this-data-model",
            "text": "A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model.  Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)",
            "title": "1. Locate the *.emf file of this data model"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#2-update-the-data-model-description",
            "text": "A data model description file contains a statical description of a MongoDb document.  @db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n    attr String stackTrace;\n}\n...  If we would like to add one more attribute to the element  ProjectError , we could add it this way :  @db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n        .\n        .\n    attr String stackTrace;\n+       attr String TestAttribute;\n}\n...  You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines",
            "title": "2. Update the Data  Model description"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#3-generate-the-java-class-using-the-pongo-tool",
            "text": "Download the Pongo tool : https://github.com/kolovos/pongo/releases  Run the Pongo generator from the command line as follows:  java -jar pongo.jar  youremffile.emf  Replace the existing Java class by the new generated java class.   More information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "3. Generate the Java Class using the Pongo Tool."
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#you-need-to-create-a-new-data-model",
            "text": "The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model.  In this case, we invite you to create a new plugin which will contain your data model.",
            "title": "You need to Create a new Data Model ?"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#1-create-a-new-eclipse-plug-in",
            "text": "Create a new Eclipse Plug-In Project (  In Eclipse Toolbar : File > New > Plug-In Project  Name of the project : org.scava. mycomponent .repository.model  Disable the generation of an Activator Class / contribution to the ui  Edit the MANIFEST.MF file  In Dependency : add a dependency to the  org.eclipse.core.runtime  plugin  In Dependency : add a dependency to the  com.googlecode.pongo.runtime  plugin  In Dependency : add a dependency to the  org.apache.commons.lang3  plugin  In Extentions : reference an extension point named  com.googlecode.pongo.runtime.osgi  In source directory  Create a package named org.scava. mycomponent .repository.model  In this package create an emf file named  mycomponent.emf   A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file :  ```package org.scava.mycomponent.repository.model;  @db\nclass MyComponent {\n     ....\n}\n```\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines",
            "title": "1. Create a new Eclipse Plug-In"
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#2-generate-the-java-class-using-the-pongo-tool",
            "text": "Download the Pongo tool : https://github.com/kolovos/pongo/releases  Run the Pongo generator from the command line as follows:  java -jar pongo.jar  youremffile.emf  Add this class in your org.scava. mycomponent .repository.model package   More information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "2. Generate the Java Class using the Pongo Tool."
        },
        {
            "location": "/others/development/Extend-MongoDB-Data-Model/#comment",
            "text": "Here we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs  link here .",
            "title": "Comment"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/",
            "text": "How to Develop a Metric Provider\n\n\nIn this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a \ntransient\n metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a \nhistorical\n metric provider, will count the total number of commits over time. \n\n\nWe'll go through the steps above for each metric provider.\n\n\nPre-requisites\n\n\n\n\nEclipse \n\n\nThe \nEmfatic\n plug-in should be installed in your Eclipse\n\n\nThe \nPongo\n plug-in should be installed in your Eclipse\n\n\nThe OSSMETER source code should be in your workspace\n\n\n\n\nThe Transient Metric Provider\n\n\nThis metric provider will store a complete history of the commits in the version control system(s) used by a project. \n\n\n0. Setup\n\n\nCreate a new Plugin project in Eclipse.\n\n\n\n\nGo to File > New > Project... and select 'Plug-in project'\n\n\nGive the project an appropriate name. The OSSMETER naming convention is:\n\n\nTransient metrics: org.ossmeter.metricprovider.trans.(metric name)\n\n\nHistorical metrics: org.ossmeter.metricprovider.historical.(metric name)\n\n\n\n\n\n\nClick Next\n\n\nIf the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it\n\n\nClick Finish\n\n\nOpen up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.\n\n\n\n\n1. The data model\n\n\nWe define the data model using the Emfatic language. In your newly created plug-in, create a package called \norg.ossmeter.metricprovider.trans.commits.model\n. In that package create an empty file called \ncommits.emf\n. In this file, we will define our data model.\n\n\nFirst of all, we need to state the name of the package.\n\n\npackage org.ossmeter.metricprovider.trans.commits.model;\n\n\n\n\nThis is used by the Pongo code generator - the generated classes will be put in this package.\n\n\nWe then define the database for our model:\n\n\n@db(qualifiedCollectionNames=\"true\")\nclass Commits {\n    val Repository[*] repositories;\n    val Commit[*] commits;\n}\n\n\n\n\nThe \n@db\n annotation tells Pongo that this will be the container database for the data. Adding the \nqualifiedCollectionNames=true\n property will prepend the database name to all Mongo collections. \n\n\nThe \nCommits\n class above says that we want a database with two collections, name \nrepositories\n and \ncommits\n. If \nqualifiedCollectionNames\n is set to \ntrue\n, the collections will be named \nCommits.repositories\n and \nCommits.commits\n. \n\n\nWe now define the schema of the \nCommits.repositories\n collection:\n\n\n~~~java\nclass Repository {\n    @searchable\n    attr String url;\n    attr String repoType;\n    attr String revision;\n    attr int totalNumberOfCommits;\n    ref CommitData[*] commits;\n}\n\n\n\nThis class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection.\n\nThe `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time.\n\nEach commit is represented in the `Commits.commits` collection by the following model:\n\n~~~~java\nclass Commit {\n    @searchable\n    attr Date date;\n    attr String identifier;\n    attr String message;\n    attr String author;\n}\n\n\n\n\nFor each commit, we store its \ndate\n, \nidentifier\n (revision ID), the commit \nmessage\n, and the \nauthor\n. We also create an index on the \ndate\n to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated \nfindByDate(String date)\n or \nfindOneByDate(String date)\n methods).\n\n\nNow we need to use Pongo to generate code from this model. Right-click on the \ncommits.emf\n file and select \nPongo > Generate Pongos and plugin.xml\n. You should see some Java classes appear in your package.\n\n\nNow that we have our data model, we can implement the metric provider.\n\n\n2. The metric provider\n\n\nCreate a Java class called \norg.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider\n\n\nThis class should extend \nAbstractTransientMetricProvider\n and specify \nCommits\n for the generic argument:\n\n\npublic class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits>\n\n\n\n\nThe generic argument states that the metric provider stores objects conforming to the \nCommits\n data model. Note: You do not need to extend \nAbstractTransientMetricProvider\n and can implement \nITransientMetricProvider\n instead should you wish to.\n\n\nThere are a number of methods that need implementing. We will discuss each in turn.\n\n\nadapt(DB db)\n\n\nThis method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows:\n\n\n    @Override\n    public Commits adapt(DB db) {\n        return new Commits(db);\n    }\n\n\n\n\n\nThe next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider:\n\n\n    @Override\n    public String getShortIdentifier() { // This may be deprecated very soon\n        return \"transient-commits\"; \n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Commit History\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"The commit history of the project.\";\n    }\n\n\n\n\nThe next method allows you to declare whether the metric provider is applicable to a given project:\n\n\n@Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }\n\n\n\n\nOur metric applies to any project that has at least one VCS repository.\n\n\nFinally, we have the \nmeasure(...)\n method that performs the actual metric calculation:\n\n\n    @Override\n    public void measure(Project project, ProjectDelta delta, Commits db) {\n\n        for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) {\n            Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl());\n\n            if (repo == null) {\n                repo = new Repository();\n                repo.setUrl(repoDelta.getRepository().getUrl());\n                db.getRepositories().add(repo);\n            }\n\n            for (VcsCommit commit : repoDelta.getCommits()) {\n                Commit c = new Commit();\n                c.setDate(commit.getJavaDate());\n                c.setMessage(commit.getMessage());\n                c.setAuthor(commit.getAuthor());\n                c.setIdentifier(commit.getRevision());\n\n                repo.getCommits().add(c);\n                db.getCommits().add(c);\n            }\n        }\n        db.getCommits().sync();\n        db.getRepositories().sync();\n    }\n\n\n\n\nThe above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new \nCommit\n object is created for each commit in the delta.\n\n\n3. Make the metric provider discoverable\n\n\nMetric providers are registered with the OSSMETER platform using \nextension points\n:\n\n\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.metricprovider\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'CommitsTransientMetricProvider' class\n\n\n\n\nNow everything is ready for the metric to be executed :)\n\n\nThe Historic Metric Provider\n\n\nThis metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.  \n\n\n0. Setup\n\n\nCreate a new Plugin project in Eclipse.  \n\n\n\n\nGo to File > New > Project... and select 'Plug-in project'\n\n\nGive the project an appropriate name. The OSSMETER naming convention is:\n\n\nTransient metrics: org.ossmeter.metricprovider.trans.(metric name)\n\n\nHistorical metrics: org.ossmeter.metricprovider.historical.(metric name)\n\n\n\n\n\n\nClick Next\n\n\nIf the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it\n\n\nClick Finish\n\n\nOpen up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.\n\n\n\n\n1. The data model\n\n\nIn your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model:\n\n\npackage org.ossmeter.metricprovider.historic.commits.model;\n\nclass HistoricCommits {\n    attr int numberOfCommits;\n}\n\n\n\n\nThe data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.\n\n\n2. The metric provider\n\n\nCreate a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider\n\n\nThis class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)):\n\n\npublic class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider\n\nThe generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to.\n\n\nThere are a number of methods that need implementing. We will discuss each in turn.\n\n\nFirst of all, complete the typical information-related methods:\n\n\n    @Override\n    public String getShortIdentifier() {\n        return \"historicalcommits\";\n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Historical commits\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"...\";\n    }\n\n\n\n\nNow complete the standard \nappliesTo\n method:\n\n\n    @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }\n\n\n\n\nWe now need to specify a dependency on the transient metric provider that we just implemented.\n\n\n    @Override\n    public List<String> getIdentifiersOfUses() {\n        return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName());\n    }   \n\n\n\n\nThis tells the platform that we need access to the \nCommitsTransientMetricProvider\n database. The platform will assign this to the \nuses\n field that is available to the historical metric provider, as you'll see in the \nmeasure\n method:\n\n\n    @Override\n    public Pongo measure(Project project) {\n        Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0));\n\n        int commits = (int) transDb.getCommits().size();\n\n        HistoricCommits hist = new HistoricCommits();\n        hist.setNumberOfCommits(commits);\n\n        return hist;\n    }\n\n\n\n\nFirst of all, we get hold of the database of the transient commits metric provider, and simply count the size of the \ncommits\n collection. We save this in an instance of the \nHistoricCommits\n Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.\n\n\n3. Make the metric provider discoverable\n\n\nThis process is the same as the transient metric provider:\n\n\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.metricprovider\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class\n\n\n\n\nNow everything is ready for both metrics to be executed :)\n\n\nBut first. Let's specify how we want this historical metric to be visualised.\n\n\n4. Define a MetVis visualisation specification\n\n\nMetVis\n is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis \nweb page\n has numerous examples of this.\n\n\nCreate a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider:\n\n\n{\n    \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\",\n    \"vis\" : [\n        {\n            \"id\" : \"historicalcommits\",\n            \"name\" : \"Commits over time\",\n            \"description\" : \"This metric shows when the projects commits occurred\",\n            \"type\" : \"LineChart\",\n            \"datatable\" : {\n                \"cols\" : [\n                    { \"name\" : \"Date\", \"field\" : \"$__date\" },\n                    { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" }\n                ]\n            },\n            \"x\" : \"Date\",\n            \"y\" : \"Commits\"\n        }\n    ]\n}\n\n\n\n\nThe \nmetricId\n field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use \n$__date\n. The final two fields (\nx\n and \ny\n) are references to column names in the datatable specification. The \ntype\n states that the data should be plotted as a line chart. You can test your MetVis specifications on the \nMetVis playpen\n.\n\n\n5. Make the visualisation specification discoverable\n\n\nAs with metric providers, visualisation specifications are registered using extension points.\n\n\n\n\nAdd the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.visualisation.metric\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'historicalcommits.json' file\n\n\n\n\nGood job.\n\n\nRunning the metric providers\n\n\nSee \nRunning from Source\n\n\nHomework\n\n\nAdapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.",
            "title": "How To Develop Metric Provider"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#how-to-develop-a-metric-provider",
            "text": "In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a  transient  metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a  historical  metric provider, will count the total number of commits over time.   We'll go through the steps above for each metric provider.",
            "title": "How to Develop a Metric Provider"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#pre-requisites",
            "text": "Eclipse   The  Emfatic  plug-in should be installed in your Eclipse  The  Pongo  plug-in should be installed in your Eclipse  The OSSMETER source code should be in your workspace",
            "title": "Pre-requisites"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#the-transient-metric-provider",
            "text": "This metric provider will store a complete history of the commits in the version control system(s) used by a project.",
            "title": "The Transient Metric Provider"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#0-setup",
            "text": "Create a new Plugin project in Eclipse.   Go to File > New > Project... and select 'Plug-in project'  Give the project an appropriate name. The OSSMETER naming convention is:  Transient metrics: org.ossmeter.metricprovider.trans.(metric name)  Historical metrics: org.ossmeter.metricprovider.historical.(metric name)    Click Next  If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it  Click Finish  Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.",
            "title": "0. Setup"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#1-the-data-model",
            "text": "We define the data model using the Emfatic language. In your newly created plug-in, create a package called  org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called  commits.emf . In this file, we will define our data model.  First of all, we need to state the name of the package.  package org.ossmeter.metricprovider.trans.commits.model;  This is used by the Pongo code generator - the generated classes will be put in this package.  We then define the database for our model:  @db(qualifiedCollectionNames=\"true\")\nclass Commits {\n    val Repository[*] repositories;\n    val Commit[*] commits;\n}  The  @db  annotation tells Pongo that this will be the container database for the data. Adding the  qualifiedCollectionNames=true  property will prepend the database name to all Mongo collections.   The  Commits  class above says that we want a database with two collections, name  repositories  and  commits . If  qualifiedCollectionNames  is set to  true , the collections will be named  Commits.repositories  and  Commits.commits .   We now define the schema of the  Commits.repositories  collection:  ~~~java\nclass Repository {\n    @searchable\n    attr String url;\n    attr String repoType;\n    attr String revision;\n    attr int totalNumberOfCommits;\n    ref CommitData[*] commits;\n}  \nThis class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection.\n\nThe `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time.\n\nEach commit is represented in the `Commits.commits` collection by the following model:\n\n~~~~java\nclass Commit {\n    @searchable\n    attr Date date;\n    attr String identifier;\n    attr String message;\n    attr String author;\n}  For each commit, we store its  date ,  identifier  (revision ID), the commit  message , and the  author . We also create an index on the  date  to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated  findByDate(String date)  or  findOneByDate(String date)  methods).  Now we need to use Pongo to generate code from this model. Right-click on the  commits.emf  file and select  Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package.  Now that we have our data model, we can implement the metric provider.",
            "title": "1. The data model"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#2-the-metric-provider",
            "text": "Create a Java class called  org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider  This class should extend  AbstractTransientMetricProvider  and specify  Commits  for the generic argument:  public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits>  The generic argument states that the metric provider stores objects conforming to the  Commits  data model. Note: You do not need to extend  AbstractTransientMetricProvider  and can implement  ITransientMetricProvider  instead should you wish to.  There are a number of methods that need implementing. We will discuss each in turn.",
            "title": "2. The metric provider"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#adaptdb-db",
            "text": "This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows:      @Override\n    public Commits adapt(DB db) {\n        return new Commits(db);\n    }  The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider:      @Override\n    public String getShortIdentifier() { // This may be deprecated very soon\n        return \"transient-commits\"; \n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Commit History\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"The commit history of the project.\";\n    }  The next method allows you to declare whether the metric provider is applicable to a given project:  @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }  Our metric applies to any project that has at least one VCS repository.  Finally, we have the  measure(...)  method that performs the actual metric calculation:      @Override\n    public void measure(Project project, ProjectDelta delta, Commits db) {\n\n        for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) {\n            Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl());\n\n            if (repo == null) {\n                repo = new Repository();\n                repo.setUrl(repoDelta.getRepository().getUrl());\n                db.getRepositories().add(repo);\n            }\n\n            for (VcsCommit commit : repoDelta.getCommits()) {\n                Commit c = new Commit();\n                c.setDate(commit.getJavaDate());\n                c.setMessage(commit.getMessage());\n                c.setAuthor(commit.getAuthor());\n                c.setIdentifier(commit.getRevision());\n\n                repo.getCommits().add(c);\n                db.getCommits().add(c);\n            }\n        }\n        db.getCommits().sync();\n        db.getRepositories().sync();\n    }  The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new  Commit  object is created for each commit in the delta.",
            "title": "adapt(DB db)"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable",
            "text": "Metric providers are registered with the OSSMETER platform using  extension points :   Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.metricprovider  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class   Now everything is ready for the metric to be executed :)",
            "title": "3. Make the metric provider discoverable"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#the-historic-metric-provider",
            "text": "This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.",
            "title": "The Historic Metric Provider"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#0-setup_1",
            "text": "Create a new Plugin project in Eclipse.     Go to File > New > Project... and select 'Plug-in project'  Give the project an appropriate name. The OSSMETER naming convention is:  Transient metrics: org.ossmeter.metricprovider.trans.(metric name)  Historical metrics: org.ossmeter.metricprovider.historical.(metric name)    Click Next  If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it  Click Finish  Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.",
            "title": "0. Setup"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#1-the-data-model_1",
            "text": "In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model:  package org.ossmeter.metricprovider.historic.commits.model;\n\nclass HistoricCommits {\n    attr int numberOfCommits;\n}  The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.",
            "title": "1. The data model"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#2-the-metric-provider_1",
            "text": "Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider  This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)):  public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider \nThe generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to.  There are a number of methods that need implementing. We will discuss each in turn.  First of all, complete the typical information-related methods:      @Override\n    public String getShortIdentifier() {\n        return \"historicalcommits\";\n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Historical commits\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"...\";\n    }  Now complete the standard  appliesTo  method:      @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }  We now need to specify a dependency on the transient metric provider that we just implemented.      @Override\n    public List<String> getIdentifiersOfUses() {\n        return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName());\n    }     This tells the platform that we need access to the  CommitsTransientMetricProvider  database. The platform will assign this to the  uses  field that is available to the historical metric provider, as you'll see in the  measure  method:      @Override\n    public Pongo measure(Project project) {\n        Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0));\n\n        int commits = (int) transDb.getCommits().size();\n\n        HistoricCommits hist = new HistoricCommits();\n        hist.setNumberOfCommits(commits);\n\n        return hist;\n    }  First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the  commits  collection. We save this in an instance of the  HistoricCommits  Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.",
            "title": "2. The metric provider"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable_1",
            "text": "This process is the same as the transient metric provider:   Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.metricprovider  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class   Now everything is ready for both metrics to be executed :)  But first. Let's specify how we want this historical metric to be visualised.",
            "title": "3. Make the metric provider discoverable"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#4-define-a-metvis-visualisation-specification",
            "text": "MetVis  is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis  web page  has numerous examples of this.  Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider:  {\n    \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\",\n    \"vis\" : [\n        {\n            \"id\" : \"historicalcommits\",\n            \"name\" : \"Commits over time\",\n            \"description\" : \"This metric shows when the projects commits occurred\",\n            \"type\" : \"LineChart\",\n            \"datatable\" : {\n                \"cols\" : [\n                    { \"name\" : \"Date\", \"field\" : \"$__date\" },\n                    { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" }\n                ]\n            },\n            \"x\" : \"Date\",\n            \"y\" : \"Commits\"\n        }\n    ]\n}  The  metricId  field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use  $__date . The final two fields ( x  and  y ) are references to column names in the datatable specification. The  type  states that the data should be plotted as a line chart. You can test your MetVis specifications on the  MetVis playpen .",
            "title": "4. Define a MetVis visualisation specification"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#5-make-the-visualisation-specification-discoverable",
            "text": "As with metric providers, visualisation specifications are registered using extension points.   Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project  Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.visualisation.metric  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'historicalcommits.json' file   Good job.",
            "title": "5. Make the visualisation specification discoverable"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#running-the-metric-providers",
            "text": "See  Running from Source",
            "title": "Running the metric providers"
        },
        {
            "location": "/others/development/How-To-Develop-Metric-Provider/#homework",
            "text": "Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.",
            "title": "Homework"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/",
            "text": "Implementing RESTLET services\n\n\nWhen to use this guideline ?\n\n\nThis guideline present how to create a new REST service using the RESTLET framework in the Scava platform.\n\n\nContext\n\n\nScava project manages REST services with the RESTLET framework.\n\n\nThe usage of Restlet framework  has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin :\n\n\n\n\norg.eclipse.crossmeter.platform.client.api.  \n\n\n\n\nThe RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.\n\n\nYou want to access to create a new REST Service ?\n\n\n1. Create a new Route\n\n\nTo  register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.\n\n\nNaming the Route\n\n\nThe routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : \nNaming-Scava-REST-Services.html\n\n\nRegister the Route\n\n\nThe \norg.scava.platform.services\n plug-in contained the class \nPlatformRoute.java\n  responsible for declaring routes.\n\n\npackage org.scava.platform.services;\nimport org.restlet.Application;\nimport org.restlet.Restlet;\nimport org.restlet.routing.Router;\n\npublic class PlatformRoute extends Application {\n    @Override\n    public Restlet createInboundRoot() {\n        Router router = new Router(getContext());\n\n        router.attach(\"/\", PingResource.class);     \n        router.attach(\"/search\", SearchProjectResource.class);\n                ...\n                router.attach(\"/projects/p/{projectid}\", ProjectResource.class);\n        router.attach(\"/raw/metrics\", RawMetricListResource.class);\n        ...\n        return router;\n    }\n}\n\n\n\n\nRoute Example :\n\n\nrouter.attach(\"/raw/metrics\", RawMetricListResource.class);\n\n\n\n\n\n\n\"/raw/metrics\"\n: Represent the route URL.\n\n\n\"RawMetricListResource.class\"\n : Represent the class where the service to be implemented for this path.\n\n\n\n\nA route can contained some parameters. In this case, parameters are identified by a name with curly brackets \n{}\n.\n\n\n2. Implement the Service\n\n\nA service implementation is a Java class which extend the \nServerResource\n class provided by the RESTLET framework.\nTo create a new service create a new Class :\n\n Named \"\nServiceName\n\" + Resource.  Ex : ProjectCreationResource.java\n\n On a namespace based on the  route. Ex : org.scava.platform.services.administration for platform administration services.\n* Who extend the org.restlet.resource.ServerResource class.\n\n\nGET Service\n\n\nTo implement a service of type GET, create a new method :\n\n Based on the following signature : public final Representation represent()\n\n Add the @Get(\"json\") annotation\n\n\n\n@Get(\"json\")\npublic final Representation represent() {\n  // Initialise Response Header\n  Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\");\n   if (responseHeaders == null) {\n    responseHeaders = new Series(Header.class);\n     getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders);\n  }\n  responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\"));\n  responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\"));\n\n  // Get Route parameter if required {projectid}\n  String projectId = (String) getRequest().getAttributes().get(\"projectid\");\n\n\n  try {\n    ....\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_OK);\n    return new StringRepresentation(...);\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n    return rep;\n  }\n}\n\n\n\n\n\nYou can also extend the \nAbstractApiResource\n , a service class  provided by the platform and dedicate to services who request a connection to MongoDb database instead of the \nServerResource\n. In this case you will have to implement the doRepresent() method.\n\n\npublic class RawMetricListResource extends AbstractApiResource {\n    public Representation doRepresent() {\n        ObjectNode res = mapper.createObjectNode();\n\n        ArrayNode metrics = mapper.createArrayNode();\n        res.put(\"metrics\", metrics);\n        ...\n        return Util.createJsonRepresentation(res);\n    }\n}\n\n\n\n\nPOST Service\n\n\nTo implement a service of type POST, crate a new method :\n\n Based on the following signature : public Representation \nmyServiceName\n (Representation entity)\n\n Add the @Post annotation\n\n\n@Post\npublic Representation myServiceName(Representation entity) {\n  try {\n    // Read Json Datas\n    JsonNode json = mapper.readTree(entity.getText());\n\n    ...\n\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_CREATED);\n    return new StringRepresentation(...);\n\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n   return rep;\n  }\n}\n\n\n\n\nDELETE Service\n\n\nTo do ....\n\n\n3. Document the Service\n\n\nThe REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to  allow an easy integration, this contract must be documented :\n\n\n4. Test the Service\n\n\nTo do ....\n\n\nComment",
            "title": "Implementing Restlet Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#implementing-restlet-services",
            "text": "",
            "title": "Implementing RESTLET services"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#when-to-use-this-guideline",
            "text": "This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#context",
            "text": "Scava project manages REST services with the RESTLET framework.  The usage of Restlet framework  has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin :   org.eclipse.crossmeter.platform.client.api.     The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.",
            "title": "Context"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#you-want-to-access-to-create-a-new-rest-service",
            "text": "",
            "title": "You want to access to create a new REST Service ?"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#1-create-a-new-route",
            "text": "To  register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.",
            "title": "1. Create a new Route"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#naming-the-route",
            "text": "The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service :  Naming-Scava-REST-Services.html",
            "title": "Naming the Route"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#register-the-route",
            "text": "The  org.scava.platform.services  plug-in contained the class  PlatformRoute.java   responsible for declaring routes.  package org.scava.platform.services;\nimport org.restlet.Application;\nimport org.restlet.Restlet;\nimport org.restlet.routing.Router;\n\npublic class PlatformRoute extends Application {\n    @Override\n    public Restlet createInboundRoot() {\n        Router router = new Router(getContext());\n\n        router.attach(\"/\", PingResource.class);     \n        router.attach(\"/search\", SearchProjectResource.class);\n                ...\n                router.attach(\"/projects/p/{projectid}\", ProjectResource.class);\n        router.attach(\"/raw/metrics\", RawMetricListResource.class);\n        ...\n        return router;\n    }\n}  Route Example :  router.attach(\"/raw/metrics\", RawMetricListResource.class);   \"/raw/metrics\" : Represent the route URL.  \"RawMetricListResource.class\"  : Represent the class where the service to be implemented for this path.   A route can contained some parameters. In this case, parameters are identified by a name with curly brackets  {} .",
            "title": "Register the Route"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#2-implement-the-service",
            "text": "A service implementation is a Java class which extend the  ServerResource  class provided by the RESTLET framework.\nTo create a new service create a new Class :  Named \" ServiceName \" + Resource.  Ex : ProjectCreationResource.java  On a namespace based on the  route. Ex : org.scava.platform.services.administration for platform administration services.\n* Who extend the org.restlet.resource.ServerResource class.",
            "title": "2. Implement the Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#get-service",
            "text": "To implement a service of type GET, create a new method :  Based on the following signature : public final Representation represent()  Add the @Get(\"json\") annotation  \n@Get(\"json\")\npublic final Representation represent() {\n  // Initialise Response Header\n  Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\");\n   if (responseHeaders == null) {\n    responseHeaders = new Series(Header.class);\n     getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders);\n  }\n  responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\"));\n  responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\"));\n\n  // Get Route parameter if required {projectid}\n  String projectId = (String) getRequest().getAttributes().get(\"projectid\");\n\n\n  try {\n    ....\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_OK);\n    return new StringRepresentation(...);\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n    return rep;\n  }\n}  You can also extend the  AbstractApiResource  , a service class  provided by the platform and dedicate to services who request a connection to MongoDb database instead of the  ServerResource . In this case you will have to implement the doRepresent() method.  public class RawMetricListResource extends AbstractApiResource {\n    public Representation doRepresent() {\n        ObjectNode res = mapper.createObjectNode();\n\n        ArrayNode metrics = mapper.createArrayNode();\n        res.put(\"metrics\", metrics);\n        ...\n        return Util.createJsonRepresentation(res);\n    }\n}",
            "title": "GET Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#post-service",
            "text": "To implement a service of type POST, crate a new method :  Based on the following signature : public Representation  myServiceName  (Representation entity)  Add the @Post annotation  @Post\npublic Representation myServiceName(Representation entity) {\n  try {\n    // Read Json Datas\n    JsonNode json = mapper.readTree(entity.getText());\n\n    ...\n\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_CREATED);\n    return new StringRepresentation(...);\n\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n   return rep;\n  }\n}",
            "title": "POST Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#delete-service",
            "text": "To do ....",
            "title": "DELETE Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#3-document-the-service",
            "text": "The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to  allow an easy integration, this contract must be documented :",
            "title": "3. Document the Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#4-test-the-service",
            "text": "To do ....",
            "title": "4. Test the Service"
        },
        {
            "location": "/others/development/Implementing-Restlet-Service/#comment",
            "text": "",
            "title": "Comment"
        },
        {
            "location": "/others/development/Licensing/",
            "text": "Licencing for Scava\n\n\nContent\n\n\nThe Scava project is licensed under \nEclipse Public License - v 2.0\n license.\n\n\nAs consequence of our status of project hosted by the eclipse foundation, all Scava components  must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.\n\n\n\"Eclipse Public License\" licensing file\n\n\nThe text below must be integrated to the root folder of your project on a text file name \"LICENSE\".\n\n\nEclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.\n\n\n\n\nSource File header\n\n\nAll sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...).\n\n\nOptionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization.\n\n\nExample of Java license header file\n\n\n/*******************************************************************************\n * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"}\n * This program and the accompanying materials are made\n * available under the terms of the Eclipse Public License 2.0\n * which is available at https://www.eclipse.org/legal/epl-2.0/\n *\n * SPDX-License-Identifier: EPL-2.0\n ******************************************************************************/\n\n\n\n\nComment\n\n\nn Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files.\n\n\nThis plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.",
            "title": "Licensing"
        },
        {
            "location": "/others/development/Licensing/#licencing-for-scava",
            "text": "",
            "title": "Licencing for Scava"
        },
        {
            "location": "/others/development/Licensing/#content",
            "text": "The Scava project is licensed under  Eclipse Public License - v 2.0  license.  As consequence of our status of project hosted by the eclipse foundation, all Scava components  must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.",
            "title": "Content"
        },
        {
            "location": "/others/development/Licensing/#eclipse-public-license-licensing-file",
            "text": "The text below must be integrated to the root folder of your project on a text file name \"LICENSE\".  Eclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.",
            "title": "\"Eclipse Public License\" licensing file"
        },
        {
            "location": "/others/development/Licensing/#source-file-header",
            "text": "All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...).  Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization.  Example of Java license header file  /*******************************************************************************\n * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"}\n * This program and the accompanying materials are made\n * available under the terms of the Eclipse Public License 2.0\n * which is available at https://www.eclipse.org/legal/epl-2.0/\n *\n * SPDX-License-Identifier: EPL-2.0\n ******************************************************************************/",
            "title": "Source File header"
        },
        {
            "location": "/others/development/Licensing/#comment",
            "text": "n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files.  This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.",
            "title": "Comment"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/",
            "text": "Naming Scava REST services\n\n\nWhen to use this guideline ?\n\n\nThis guideline present how to define the route of a new REST service provided by the Scava platform.\n\n\nContext\n\n\nThe REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of  platform services , we need to used a common naming schema for all REST services provided by the platform.\n\n\nHow to name a REST service ?\n\n\n\n\n/{\ncomponentid\n}/{\ncategoryname\n}/{\nservicename\n}\n\n\n\n\n\n\n/\ncomponentid\n/\n: Name of the Architectural component which provide the service\n\n\n/\ncategoryname\n/ (Optional)\n : optional category of the service\n\n\n/\nservicename\n/\n : Name of the rest service\n\n\n\n\nComponent\n\n\n\n\n\n\n\n\nComponent\n\n\nComponentId\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\ndashboard\n\n\n\n\n\n\nWorkflow Execution Engine\n\n\nworkflow\n\n\n\n\n\n\nKnowledge Base\n\n\nknowledgebase\n\n\n\n\n\n\nMetric Provider\n\n\nmetricprovider\n\n\n\n\n\n\nAdministration\n\n\nadministration\n\n\n\n\n\n\n\n\nComment",
            "title": "Naming Scava REST Services"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/#naming-scava-rest-services",
            "text": "",
            "title": "Naming Scava REST services"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/#when-to-use-this-guideline",
            "text": "This guideline present how to define the route of a new REST service provided by the Scava platform.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/#context",
            "text": "The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of  platform services , we need to used a common naming schema for all REST services provided by the platform.",
            "title": "Context"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/#how-to-name-a-rest-service",
            "text": "/{ componentid }/{ categoryname }/{ servicename }    / componentid / : Name of the Architectural component which provide the service  / categoryname / (Optional)  : optional category of the service  / servicename /  : Name of the rest service",
            "title": "How to name a REST service ?"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/#component",
            "text": "Component  ComponentId      DevOps Dashboard  dashboard    Workflow Execution Engine  workflow    Knowledge Base  knowledgebase    Metric Provider  metricprovider    Administration  administration",
            "title": "Component"
        },
        {
            "location": "/others/development/Naming-Scava-REST-Services/#comment",
            "text": "",
            "title": "Comment"
        },
        {
            "location": "/others/development/Repository-Organisation/",
            "text": "The SCAVA code repository  is organized by functional components with one package for each of this components. \n\n\nGeneral organisation\n\n\n\n\nmetric-platform \n\n\nplatform : Core projects of the metric platform\n\n\nplatform-extensions : Extensions of the metric platform\n\n\nmetric-providers : Metric Providers implementations projects\n\n\nfactoids : Factoids implementations projects\n\n\ntests : Test projects related to the metric-platform\n\n\nweb-dashboards : DevOps Dashboard and DevOpsDashboard components \n\n\nknowledge-base : Knowledge Base implementation\n\n\nWorkflow Execution Engine : Workflow Execution Engine implementation\n\n\neclipse-based-ide : SCAVA Eclipse plugin\n\n\napi-gateway :  API Gateway implementation.\n\n\nadministration : Administration component implementation\n\n\nmingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project. \n\n\n\n\nComments",
            "title": "Repository Organisation"
        },
        {
            "location": "/others/development/Repository-Organisation/#general-organisation",
            "text": "metric-platform   platform : Core projects of the metric platform  platform-extensions : Extensions of the metric platform  metric-providers : Metric Providers implementations projects  factoids : Factoids implementations projects  tests : Test projects related to the metric-platform  web-dashboards : DevOps Dashboard and DevOpsDashboard components   knowledge-base : Knowledge Base implementation  Workflow Execution Engine : Workflow Execution Engine implementation  eclipse-based-ide : SCAVA Eclipse plugin  api-gateway :  API Gateway implementation.  administration : Administration component implementation  mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.",
            "title": "General organisation"
        },
        {
            "location": "/others/development/Repository-Organisation/#comments",
            "text": "",
            "title": "Comments"
        },
        {
            "location": "/others/development/Testing-Guidelines/",
            "text": "Knowledge Base\n\n\nThis builds needs some configuration to run successfully. \n\n\nIn the application.properties files:\n\n \n/knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties\n\n\n \n/knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties\n\n\nEdit the following parameters:\n\n\nlucene.index.folder=/tmp/scava_lucene/\negit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57\n\n\n\n\nTo generate the GitHub access token you need to go to \nyour own GitHub account\n and create a new one. Once it's done simply restart the tests, they should pass.",
            "title": "Testing Guidelines"
        },
        {
            "location": "/others/development/Testing-Guidelines/#knowledge-base",
            "text": "This builds needs some configuration to run successfully.   In the application.properties files:   /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties    /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties  Edit the following parameters:  lucene.index.folder=/tmp/scava_lucene/\negit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57  To generate the GitHub access token you need to go to  your own GitHub account  and create a new one. Once it's done simply restart the tests, they should pass.",
            "title": "Knowledge Base"
        },
        {
            "location": "/others/users/Consuming-REST-Services/",
            "text": "Consuming REST services\n\n\nWhen to use ?\n\n\nThis guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.\n\n\nREST API Reference\n\n\nThe reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].\n\n\nAPI Gateway\n\n\nThe Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway.\n\n\nAll web service request form clients have to go through the gateway.\n\n\n\n\nThe api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.\n\n\nPlatform Authentication\n\n\nThe CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io).\n1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n1. When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.\n\n\nAuthentication in Java\n\n\nRetrieve a Web Tokens from authentication service\n\n\nprivate String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException {\n  // Authentication Service URI\n  URL url = new URL(\"http://localhost:8086/api/authentication\");\n\n  // AUthentication Request\n  HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n  connection.setDoOutput(true);\n  connection.setRequestMethod(\"POST\");\n  connection.setRequestProperty(\"Content-Type\", \"application/json\");\n\n  String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\";\n  OutputStream os = connection.getOutputStream();\n  os.write(input.getBytes());\n  os.flush();\n\n  if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n    throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode());\n  }\n\n  connection.disconnect();\n\n  // A JWT Token is return in the Header of the response\n  return connection.getHeaderField(\"Authorization\");\n}\n\n\n\n\nREST Service Call\n\n\ncurl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication\n\n\n\n\nService Consumption\n\n\nTo consume a REST service provided by the integrated platform, the client must include the Token in the header of his request.\n\n\n```java\n// Service URL\nURL url = new URL(\"http://localhost:8086/api/users\");\nHttpURLConnection connection = (HttpURLConnection) url.openConnection();\nconnection.setRequestMethod(\"GET\");\n\n\n// Add Token to the request header\nconnection.setRequestProperty(\"Authorization\",token);\n```\n\n\nComment",
            "title": "Consuming REST Services"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#consuming-rest-services",
            "text": "",
            "title": "Consuming REST services"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#when-to-use",
            "text": "This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.",
            "title": "When to use ?"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#rest-api-reference",
            "text": "The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].",
            "title": "REST API Reference"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#api-gateway",
            "text": "The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway.  All web service request form clients have to go through the gateway.   The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.",
            "title": "API Gateway"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#platform-authentication",
            "text": "The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io).\n1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n1. When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.  Authentication in Java  Retrieve a Web Tokens from authentication service  private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException {\n  // Authentication Service URI\n  URL url = new URL(\"http://localhost:8086/api/authentication\");\n\n  // AUthentication Request\n  HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n  connection.setDoOutput(true);\n  connection.setRequestMethod(\"POST\");\n  connection.setRequestProperty(\"Content-Type\", \"application/json\");\n\n  String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\";\n  OutputStream os = connection.getOutputStream();\n  os.write(input.getBytes());\n  os.flush();\n\n  if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n    throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode());\n  }\n\n  connection.disconnect();\n\n  // A JWT Token is return in the Header of the response\n  return connection.getHeaderField(\"Authorization\");\n}  REST Service Call  curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication",
            "title": "Platform Authentication"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#service-consumption",
            "text": "To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request.  ```java\n// Service URL\nURL url = new URL(\"http://localhost:8086/api/users\");\nHttpURLConnection connection = (HttpURLConnection) url.openConnection();\nconnection.setRequestMethod(\"GET\");  // Add Token to the request header\nconnection.setRequestProperty(\"Authorization\",token);\n```",
            "title": "Service Consumption"
        },
        {
            "location": "/others/users/Consuming-REST-Services/#comment",
            "text": "",
            "title": "Comment"
        },
        {
            "location": "/others/users/REST-API-Generation/",
            "text": "REST API Generation\n\n\nREST API Tutorial files\n\n\nInstall\n\n\nFirst of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install.\n\n\nExecute the following steps.\n\n\n\n\nClone repository: \nhttps://github.com/patrickneubauer/crossminer-workflow\n\n\nImport projects from the cloned repo to empty Eclipse workspace:\n\n\nImport root via \nMaven > Existing Maven Projects\n,\n\n\nthen the rest via \nGeneral > Existing projects into Workspace\n \nwithout\n checking \nSearch for nested projects\n.\n\n\nInstall new software packages:\n\n\nInstall Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling \n[web page]\n\n\nInstall Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ \n[update site]\n\n\nUpdate Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ \n[update site]\n\n\nInstall Emfatic: http://download.eclipse.org/emfatic/update/ \n[update site]\n\n\nNote:\n I didn't see the the feature until \nGroup items by category\n was unchecked.\n\n\nInstall GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ \n[update site]\n\n\nImport the given json projects (\norg.eclipse.epsilon.emc.json\n and \norg.eclipse.epsilon.emc.json.dt\n) into the workspace via \nGeneral > Existing projects into Workspace\n.\n\n\nNow right-click on \norg.eclipse.crossmeter.workflow project\n and select \nMaven > Update Project....\n\n\nCopy the give \nM2M_Environment_oxygen.launch\n (for Windows 10) to \norg.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator\n and overwrite the old one.\n\n\nRefresh project \norg.eclipse.crossmeter.workflow.restmule.generator\n in Eclipse.\n\n\nBefore running the \n.launch\n file, right-click on it, \nRun as > Run configurations....\n Here go to \nPlug-ins\n tab and click on \nAdd Required Plug-ins\n.\n\n\nNow you can run the \n.launch\n file TBA:How?\n\n\nIn the Runtime Eclipse import the \norg.eclipse.crossmeter.workflow.restmule.generator\n project as Maven Project.\n\n\nRun \ngenerateFromOAS.launch\n as you can see in the videos https://youtu.be/BJXuozHJPeg.\n\n\nNow, you should see it generating the project.\n\n\nFor further steps, watch Patrick's videos .\n\n\ngithub client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg\n\n\ngithub client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI\n\n\ngithub client generation example \u2014 https://youtu.be/ltSNnSZRETA\n\n\nkafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc\n\n\nkafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4\n\n\nkafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA\n\n\nMDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk\n\n\n\n\nExample\n\n\nI've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: \n/square/{number}\n. You can replace the \n{number}\n with any number. The repsonse for this request is a \nJSON\n object, which looks like this:\n\n\n{\n    \"squared\": {squaredValue}\n}\n\n\n\n\nIt contains only one field, named \nsquared\n, which has the value of the square of the given number. From the specification the generator will provide us a \nJava Project\n which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines:\n\n\nITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings).\nint number = 7; //The number to be squared.\nIData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server.\nNumberValue numberValue; //NumberValue is the class which represents the object received from the server\nnumberValue = squared.observe().blockingSingle(); //Get the actually received object from the response.\nSystem.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer.\n\n\n\n\nEverything, including the \nIEntityApi\n interface and the \nNumberValue\n class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through \nJava\n to the objects/field inside of the received \nJSON\n object. And once again, they are all generated from the OpenAPI specification.\n\n\nSo, to generate the mentioned \nJava Project\n, you have to put your OpenAPI specification file into the \nschemas\n folder inside the \norg.eclipse.crossmeter.workflow.restmule.generator\n project. I'll call mine as \nTestAPI.json\n, and it's made up of the following:\n\n\n{\n  \"swagger\": \"2.0\",\n  \"schemes\": [\n    \"http\"\n  ],\n  \"host\": \"localhost:8080\",\n  \"basePath\": \"/\",\n  \"info\": {\n    \"description\": \"This is a test API, only for demonstration of the generator.\",\n    \"termsOfService\": \"\",\n    \"title\": \"TestAPI\",\n    \"version\": \"v1\"\n  },\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"securityDefinitions\": {\n\n  },\n  \"paths\": {\n    \"/square/{number}\": {\n      \"get\": {\n        \"description\": \"Square a number.\",\n        \"parameters\": [\n          {\n            \"description\": \"The number to be squared\",\n            \"in\": \"path\",\n            \"name\": \"number\",\n            \"required\": true,\n            \"type\": \"integer\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/NumberValue\"\n            }\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"NumberValue\": {\n      \"properties\": {\n        \"squared\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"type\": \"object\"\n    }\n  }\n}\n\n\n\n\nYou can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own.\nThen we have to set the generator to use our new schema. To do this, open the \nbuild.xml\n in the generator project and modify the \napi\n and \njson.model.file\n property to \ntestapi\n and \nschemas/TestAPI.json\n, respectively.\n\n\n<!--API Variables -->\n<property name=\"api\" value=\"testapi\" />\n<property name=\"json.model.file\" value=\"schemas/TestAPI.json\" />\n\n\n\n\nWe are almost done, but we have to take one more small step. We have to provide an \n.eol\n file in the \nepsilon/util/fix/\n folder in the generator project. Its name must be the same as the value of the \napi\n property in the previous step, so for this example we use the \ntestapi.eol\n. The content of this file:\n\n\nimport \"../restmule.eol\";\n\nvar api = RestMule!API.all.first();\n\n// RATE LIMITS\n\nvar search = new RestMule!RatePolicyScope;\nsearch.scope = \"Search\";\nvar entity = new RestMule!RatePolicyScope;\nentity.scope = \"Entity\";\n\n// RATE POLICY\n\nvar policy = new RestMule!RatePolicy;\npolicy.scopes.add(entity);\npolicy.scopes.add(search);\n\nvar reset = new RestMule!ResponseHeader;\nvar resetInt = new RestMule!TInteger;\nresetInt.label = \"X-RateLimit-Reset\";\nreset.type = resetInt;\npolicy.reset = reset;\n\nvar limit = new RestMule!ResponseHeader;\nvar limitInt = new RestMule!TInteger;\nlimitInt.label = \"X-RateLimit-Limit\";\nlimit.type = limitInt;\npolicy.limit = limit;\n\nvar remaining = new RestMule!ResponseHeader;\nvar remainingInt = new RestMule!TInteger;\nremainingInt.label = \"X-RateLimit-Remaining\";\nremaining.type = remainingInt;\npolicy.remaining = remaining;\n\napi.ratePolicy = policy;\n\n// PAGINATION\n\nvar pagination= new RestMule!PaginationPolicy;\npagination.start = 1;\npagination.max = 10;\npagination.increment = 1;\npagination.maxPerIteration = 100;\n\nvar perIteration = new RestMule!Query;\nperIteration.description = \"Items per page\";\nperIteration.required = false;\nvar type = new RestMule!TInteger;\ntype.label = \"per_page\";\ntype.name = type.label;\nperIteration.type = type;\npagination.perIteration = perIteration;\n\nvar page = new RestMule!Query;\npage.description = \"Page identifier\";\npage.required = false;\nvar type1 = new RestMule!TInteger;\ntype1.label = \"page\";\ntype1.name = type1.label;\npage.type = type1;\npagination.page = page;\n\nvar link = new RestMule!ResponseHeader;\nlink.description = \"Page links\";\nvar format = new RestMule!TFormattedString;\nformat.label = \"Link\";\nformat.name = format.label;\nlink.type = format;\npagination.links = link;\n\napi.pagination = pagination;\n\n// WRAPPER\n\nvar wrapper = new RestMule!Wrapper;\nwrapper.name = \"Wrapper\";\nvar items = new RestMule!ListType;\nitems.label = \"items\";\nwrapper.items = items;\nwrapper.totalLabel= \"total_count\";\nwrapper.incompleteLabel = \"incomplete_results\";\napi.pageWrapper = wrapper;\n\n// ADD RATE & WRAPPER TO REQUESTS (FIXME)\n\nfor (r in RestMule!Request.all){\n    if (r.parent.path.startsWith(\"/search\")){\n        r.scope = search;\n    } else {\n        r.scope = entity;\n    }\n    r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader)));\n    for (resp in r.responses.select(s|s.responseType <> null)){\n        resp.unwrap();\n    }\n}\n\n/*  //////////\n * OPERATIONS\n *//////////\noperation RestMule!ObjectType hasWrapper() : Boolean {\n    var wrapper = RestMule!Wrapper.all.first;\n    var lists = self.listFields.collect(a|a.label);\n    return (not lists.isEmpty()) and lists\n        .includes(wrapper.items.label);\n}\n\noperation RestMule!Response unwrap() : RestMule!ObjectType{\n    if (self.responseType.instanceOf(ObjectType)){\n        if (self.responseType.hasWrapper()){\n            (\"Unwrapping : \"+ self.responseType.name).println;\n            var wrapper = RestMule!Wrapper.all.first;\n            var name = self.responseType.name.println;\n            self.responseType = self.responseType.println.listFields\n                .select(b| b.label == wrapper.items.label).first.elements.first;            \n            self.responseType.name = name;\n            self.pageWrapped = true;\n            self.responseType.description = \"UNWRAPPED: \" + self.responseType.description;\n        }\n    }\n}\n\n\n\n\nAfter this, we can run the generator and it will generate our \nJava Project\n from the specification.\n\n\nIn the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.",
            "title": "REST API Generation"
        },
        {
            "location": "/others/users/REST-API-Generation/#rest-api-generation",
            "text": "REST API Tutorial files",
            "title": "REST API Generation"
        },
        {
            "location": "/others/users/REST-API-Generation/#install",
            "text": "First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install.  Execute the following steps.   Clone repository:  https://github.com/patrickneubauer/crossminer-workflow  Import projects from the cloned repo to empty Eclipse workspace:  Import root via  Maven > Existing Maven Projects ,  then the rest via  General > Existing projects into Workspace   without  checking  Search for nested projects .  Install new software packages:  Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling  [web page]  Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/  [update site]  Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/  [update site]  Install Emfatic: http://download.eclipse.org/emfatic/update/  [update site]  Note:  I didn't see the the feature until  Group items by category  was unchecked.  Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/  [update site]  Import the given json projects ( org.eclipse.epsilon.emc.json  and  org.eclipse.epsilon.emc.json.dt ) into the workspace via  General > Existing projects into Workspace .  Now right-click on  org.eclipse.crossmeter.workflow project  and select  Maven > Update Project....  Copy the give  M2M_Environment_oxygen.launch  (for Windows 10) to  org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator  and overwrite the old one.  Refresh project  org.eclipse.crossmeter.workflow.restmule.generator  in Eclipse.  Before running the  .launch  file, right-click on it,  Run as > Run configurations....  Here go to  Plug-ins  tab and click on  Add Required Plug-ins .  Now you can run the  .launch  file TBA:How?  In the Runtime Eclipse import the  org.eclipse.crossmeter.workflow.restmule.generator  project as Maven Project.  Run  generateFromOAS.launch  as you can see in the videos https://youtu.be/BJXuozHJPeg.  Now, you should see it generating the project.  For further steps, watch Patrick's videos .  github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg  github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI  github client generation example \u2014 https://youtu.be/ltSNnSZRETA  kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc  kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4  kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA  MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk",
            "title": "Install"
        },
        {
            "location": "/others/users/REST-API-Generation/#example",
            "text": "I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following:  /square/{number} . You can replace the  {number}  with any number. The repsonse for this request is a  JSON  object, which looks like this:  {\n    \"squared\": {squaredValue}\n}  It contains only one field, named  squared , which has the value of the square of the given number. From the specification the generator will provide us a  Java Project  which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines:  ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings).\nint number = 7; //The number to be squared.\nIData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server.\nNumberValue numberValue; //NumberValue is the class which represents the object received from the server\nnumberValue = squared.observe().blockingSingle(); //Get the actually received object from the response.\nSystem.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer.  Everything, including the  IEntityApi  interface and the  NumberValue  class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through  Java  to the objects/field inside of the received  JSON  object. And once again, they are all generated from the OpenAPI specification.  So, to generate the mentioned  Java Project , you have to put your OpenAPI specification file into the  schemas  folder inside the  org.eclipse.crossmeter.workflow.restmule.generator  project. I'll call mine as  TestAPI.json , and it's made up of the following:  {\n  \"swagger\": \"2.0\",\n  \"schemes\": [\n    \"http\"\n  ],\n  \"host\": \"localhost:8080\",\n  \"basePath\": \"/\",\n  \"info\": {\n    \"description\": \"This is a test API, only for demonstration of the generator.\",\n    \"termsOfService\": \"\",\n    \"title\": \"TestAPI\",\n    \"version\": \"v1\"\n  },\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"securityDefinitions\": {\n\n  },\n  \"paths\": {\n    \"/square/{number}\": {\n      \"get\": {\n        \"description\": \"Square a number.\",\n        \"parameters\": [\n          {\n            \"description\": \"The number to be squared\",\n            \"in\": \"path\",\n            \"name\": \"number\",\n            \"required\": true,\n            \"type\": \"integer\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/NumberValue\"\n            }\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"NumberValue\": {\n      \"properties\": {\n        \"squared\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"type\": \"object\"\n    }\n  }\n}  You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own.\nThen we have to set the generator to use our new schema. To do this, open the  build.xml  in the generator project and modify the  api  and  json.model.file  property to  testapi  and  schemas/TestAPI.json , respectively.  <!--API Variables -->\n<property name=\"api\" value=\"testapi\" />\n<property name=\"json.model.file\" value=\"schemas/TestAPI.json\" />  We are almost done, but we have to take one more small step. We have to provide an  .eol  file in the  epsilon/util/fix/  folder in the generator project. Its name must be the same as the value of the  api  property in the previous step, so for this example we use the  testapi.eol . The content of this file:  import \"../restmule.eol\";\n\nvar api = RestMule!API.all.first();\n\n// RATE LIMITS\n\nvar search = new RestMule!RatePolicyScope;\nsearch.scope = \"Search\";\nvar entity = new RestMule!RatePolicyScope;\nentity.scope = \"Entity\";\n\n// RATE POLICY\n\nvar policy = new RestMule!RatePolicy;\npolicy.scopes.add(entity);\npolicy.scopes.add(search);\n\nvar reset = new RestMule!ResponseHeader;\nvar resetInt = new RestMule!TInteger;\nresetInt.label = \"X-RateLimit-Reset\";\nreset.type = resetInt;\npolicy.reset = reset;\n\nvar limit = new RestMule!ResponseHeader;\nvar limitInt = new RestMule!TInteger;\nlimitInt.label = \"X-RateLimit-Limit\";\nlimit.type = limitInt;\npolicy.limit = limit;\n\nvar remaining = new RestMule!ResponseHeader;\nvar remainingInt = new RestMule!TInteger;\nremainingInt.label = \"X-RateLimit-Remaining\";\nremaining.type = remainingInt;\npolicy.remaining = remaining;\n\napi.ratePolicy = policy;\n\n// PAGINATION\n\nvar pagination= new RestMule!PaginationPolicy;\npagination.start = 1;\npagination.max = 10;\npagination.increment = 1;\npagination.maxPerIteration = 100;\n\nvar perIteration = new RestMule!Query;\nperIteration.description = \"Items per page\";\nperIteration.required = false;\nvar type = new RestMule!TInteger;\ntype.label = \"per_page\";\ntype.name = type.label;\nperIteration.type = type;\npagination.perIteration = perIteration;\n\nvar page = new RestMule!Query;\npage.description = \"Page identifier\";\npage.required = false;\nvar type1 = new RestMule!TInteger;\ntype1.label = \"page\";\ntype1.name = type1.label;\npage.type = type1;\npagination.page = page;\n\nvar link = new RestMule!ResponseHeader;\nlink.description = \"Page links\";\nvar format = new RestMule!TFormattedString;\nformat.label = \"Link\";\nformat.name = format.label;\nlink.type = format;\npagination.links = link;\n\napi.pagination = pagination;\n\n// WRAPPER\n\nvar wrapper = new RestMule!Wrapper;\nwrapper.name = \"Wrapper\";\nvar items = new RestMule!ListType;\nitems.label = \"items\";\nwrapper.items = items;\nwrapper.totalLabel= \"total_count\";\nwrapper.incompleteLabel = \"incomplete_results\";\napi.pageWrapper = wrapper;\n\n// ADD RATE & WRAPPER TO REQUESTS (FIXME)\n\nfor (r in RestMule!Request.all){\n    if (r.parent.path.startsWith(\"/search\")){\n        r.scope = search;\n    } else {\n        r.scope = entity;\n    }\n    r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader)));\n    for (resp in r.responses.select(s|s.responseType <> null)){\n        resp.unwrap();\n    }\n}\n\n/*  //////////\n * OPERATIONS\n *//////////\noperation RestMule!ObjectType hasWrapper() : Boolean {\n    var wrapper = RestMule!Wrapper.all.first;\n    var lists = self.listFields.collect(a|a.label);\n    return (not lists.isEmpty()) and lists\n        .includes(wrapper.items.label);\n}\n\noperation RestMule!Response unwrap() : RestMule!ObjectType{\n    if (self.responseType.instanceOf(ObjectType)){\n        if (self.responseType.hasWrapper()){\n            (\"Unwrapping : \"+ self.responseType.name).println;\n            var wrapper = RestMule!Wrapper.all.first;\n            var name = self.responseType.name.println;\n            self.responseType = self.responseType.println.listFields\n                .select(b| b.label == wrapper.items.label).first.elements.first;            \n            self.responseType.name = name;\n            self.pageWrapped = true;\n            self.responseType.description = \"UNWRAPPED: \" + self.responseType.description;\n        }\n    }\n}  After this, we can run the generator and it will generate our  Java Project  from the specification.  In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.",
            "title": "Example"
        },
        {
            "location": "/others/users/Running-Scava-in-Eclipse/",
            "text": "Running Scava in Eclipse\n\n\nThis page gives \ngeneral guidelines for running Ossmeter platform in Eclipse\n\n\nThe screeshot below will be used as a reference :\n\n\n\n\n(Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) )\n\n\n\n\nWe compile with \nOssmeterfromfeature.product\n file in \norg.ossmeter.platform.osgi\n (highlighted above)\n\n\nIn the file we put program arguements : \n-apiServer\n, \n-master\n, \n-slave\n. More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform\n\n\nWe must validate with the button on corner top right (highlighted above). This will add the required packages.\n\n\nIf the required packages are still missing, go to \nRun Configurations -> Plug-ins\n and click on \nAdd Required Plugins\n.\n\n\n\n\n\n\n\n\nYou could also check the \ngeneral information\n, if its the same (as in the screenshot below).\n\n\n\n\n\n\nThese were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.",
            "title": "Running Scava in Eclipse"
        },
        {
            "location": "/others/users/Running-Scava-in-Eclipse/#running-scava-in-eclipse",
            "text": "This page gives  general guidelines for running Ossmeter platform in Eclipse  The screeshot below will be used as a reference :   (Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) )   We compile with  Ossmeterfromfeature.product  file in  org.ossmeter.platform.osgi  (highlighted above)  In the file we put program arguements :  -apiServer ,  -master ,  -slave . More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform  We must validate with the button on corner top right (highlighted above). This will add the required packages.  If the required packages are still missing, go to  Run Configurations -> Plug-ins  and click on  Add Required Plugins .     You could also check the  general information , if its the same (as in the screenshot below).    These were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.",
            "title": "Running Scava in Eclipse"
        },
        {
            "location": "/others/users/Scava-Metrics/",
            "text": "Metrics computed by Scava\n\n\nMetric-platform\n\n\nOSGi metrics\n are defined in:\n\n\n\n\nhttps://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc\n\n\n\n\nList of metrics:\n\n\n\n\nallOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\nallOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).\n\n\nallOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file.\n\n\nnumberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).\n\n\nnumberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\nunusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell).\n\n\nratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages.\n\n\nratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code.\n\n\nnumberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest.\n\n\nnumberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest..\n\n\nunversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header.\n\n\nratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles.\n\n\nunversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest.\n\n\nratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages.\n\n\nunversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest.\n\n\nratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages.\n\n\n\n\nMaven metrics\n are defined in \n\n\n\n\nhttps://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc\n\n\n\n\nList of metrics: \n\n\n\n\nallMavenDependencies -- Retrieves all the Maven dependencies.\n\n\nallOptionalMavenDependencies -- Retrieves all the optional Maven dependencies.\n\n\nnumberMavenDependencies -- Retrieves the number of Maven dependencies.\n\n\nnumberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies.\n\n\nratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies.\n\n\nisUsingTycho -- Checks if the current project is a Tycho project.\n\n\n\n\nIDE plugin\n\n\nAll metrics are defined in \nmetrics_definition_ide_plugin.docx\n\n\n\n\nscava-lib-usage -- Level of using CROSSMINER library change function.\n\n\nscava-search-usage -- Level of using CROSSMINER search function.\n\n\nscava-search-success -- Level of successfull free text serach using CROSSMINER plug-in.\n\n\nmodifictaion-rate -- Rate of changes applied to document.\n\n\ngui-usage-rate -- Rate of activly using the GUI of Eclipse.\n\n\ntesting-rate -- Count of executing tests.\n\n\nworking-time -- Average working time for Java source files.\n\n\nfile-access-rate -- Average count of Java source files open and brought to top.\n\n\n\n\nNatural Language Processing\n\n\nAll metrics are defined in \nmetrics_definition_nlp.docx\n.\n\n\nIssue tracking\n metrics:\n\n\n\n\nNumber of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project.\n\n\nNumber of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project.\n\n\nEmotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project.\n\n\nNumber new bugs: It indicates how many bugs are created every delta through a lapse of time.\n\n\nNumber of new users: How many users are new in the bug tracking system.\n\n\nOpen time: This metric shows the average open time of an issue.\n\n\nNumber of patches: It indicates the number of patches located in the issue tracking system.\n\n\nNumber of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests.\n\n\nResponse time: It presents the average time to reply a comment in an issue tracker.\n\n\nSentiments: Which sentiments are found in a issue tracking system for a project.\n\n\nSeverity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement.\n\n\nStatus: It indicates the issues status, like open or closed.\n\n\nTopics: Using clustering methods, this metric indicates the topics that are discussed through the time.\n\n\nUnasnwered bugs: It indicates how many issues do not have any reply.\n\n\nUsers: How many users are found in a issue tracker system.\n\n\n\n\nNewsgroup\n metrics\n\n\n\n\nNumber of articles; It determines the number of articles found in a newsgroup.\n\n\nEmotions: Which emotions are found in the newgroup.\n\n\nNumber of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup.\n\n\nNumber of new users: How many users make use of the newsgroup.\n\n\nNumber of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies.\n\n\nResponse time: The average time to reply a message.\n\n\nSentiments: The sentiments found in the newsgroup.\n\n\nSeverity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is.\n\n\nNumber of new threads: How many threads are created per delta.\n\n\nTopics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup.\n\n\nNumber of unanswered topics: How many messages haven\u2019t been answered.\n\n\nNumber of users: Number of users that make use of the newsgroup.\n\n\n\n\nForums\n metrics\n\n\n\n\nNumber of posts: For every topic (i.e. forum thread), how many posts in total exist.\n\n\nEmotions: Which are the emotions located in the forum.\n\n\nNumber of topics: How many forum threads contain the forum.\n\n\nNumber of request and replies: From all the posts in the forum, how many are request and how many are reply.\n\n\nSentiments: Which are the sentiments that can be found in a forum.\n\n\nSeverity: In the case the forums are used to express issues, we can determine their severity.\n\n\nNumber of new topics: Number of new forum threads created.\n\n\nTopics: Number of content topics, i.e. subjects, are discussed in the forum. \n\n\nNumber of unanswered topics: Number of forums threads which do not have a reply.\n\n\n\n\nConfiguration analysis metrics\n\n\nMetrics of Puppet are defined in detail in \nDeliverable 4.2\n.\n\n\nPuppet Design\n Metrics:\n\n\n\n\nnumberOfMultifacetedSmells --  The number of Multifaceted Abstraction smells\n\n\nnumberOfUnnecessarySmells -- The number of Unnecessary Abstraction smells\n\n\nnumberOfImperativeSmells -- The number of Imperative Abstraction smells\n\n\nnumberOfMissAbSmells -- The number of Missing Abstraction smells\n\n\nnumberOfInsufficientSmells -- The number of Insufficient Modularization smells\n\n\nnumberOfUnstructuredSmells -- The number of Unstructured Module smells\n\n\nnumberOfTightSmells -- The number of Tightly-coupled Module smells\n\n\nnumberOfBrokenSmells -- The number of Broken Hierarchy smells\n\n\nnumberOfMissingDepSmells -- The number of Missing Dependency smells\n\n\nnumberOfHairballSmells -- The number of Hairball Structure smells\n\n\nnumberOfDeficientSmells -- The number of Deficient Encapsulation smells\n\n\nnumberOfWeakenSmells -- The number of Weaken Modularity smells\n\n\ncumulativeNumberOfDesignSmells -- The number of design smells\n\n\n\n\nPuppet Implementation\n Metrics:\n\n\n\n\nnumberOfMissingDefaultCaseSmells -- Smell exists when a default case is missing in a case or selector statement.\n\n\nnumberOfInconsistentNamingConventionSmells -- Smell exists when the used naming convention deviates from the recommended naming convention.\n\n\nnumberOfComplexExpressionSmells -- Smell exists when a program contains a difficult to understand complex expression.  \n\n\nnumberOfDuplicateEntitySmells -- Smell exists duplicate parameters are present in the configuration code.\n\n\nnumberOfMisplacedAttributeSmells -- Smell exists when attribute placement within a resource or a class has not followed a recommended order.\n\n\nnumberOfImproperAlignmentSmells -- Smell exists when the code is not properly aligned or tabulation characters are used \n\n\nnumberOfInvalidPropertyValueSmells -- Smell exists when an invalid value of a property or attribute is used.\n\n\nnumberOfIncompleteTasksSmells -- Smell exists when the code has \u201cfixme\u201d and \u201ctodo\u201d tags indicating incomplete tasks.\n\n\nnumberOfDeprecatedStatementUsageSmells -- Smell exists when the configuration code uses one of the deprecated statements.\n\n\nnumberOfImproperQuoteUsageSmells -- Smell exists when single and double quotes are not used properly.\n\n\nnumberOfLongStatementSmells -- Smell exists when the code contains long statements.\n\n\nnumberOfIncompleteConditionalSmells -- Smell exists when an \u201cif..elseif\u201d construct used without a terminating \u201celse\u201d clause.\n\n\nnumberOfUnguardedVariableSmells -- Smell exists when a variable is not enclosed in braces when being interpolated in a string.\n\n\n\n\nDocker Metrics are based on the following \nrules\n.\n\n\nDocker\n Metrics:\n\n\n\n\nnumberOfUpgradeSmells -- Smell exists when apt-get or apk commands are used problematically.\n\n\nnumberOfPinVersionSmells -- Smell exists when version of packages is not explicitly declared.\n\n\nnumberOfUntaggedImageSmells -- Smell exists when version of images is not explicitly declared.\n\n\nnumberOfSudoSmells -- Smell exists when sudo command and root user are used inappropriately.\n\n\nnumberOfCopySmells -- Smell exists when COPY instruction is not used properly.\n\n\nnumberOfFromSmells -- Smell exists when FROM instruction is not used properly.\n\n\nnumberOfCmdSmells -- Smell exists when CMD and ENTRYPOINT instruction is not used properly.\n\n\nnumberOfAddSmells -- Smell exists when ADD instruction is not used properly.\n\n\nnumberOfMeaninglessCommandsSmells -- Smell exists when there are commands that makes no sense running them in a Docker container.\n\n\nnumberOfInvalidPortsSmells -- Smell exists when invalid UNIX ports range is used.\n\n\nnumberOfShellSmells -- Smell exists when SHELL instruction is not used when it should.\n\n\n\n\nAll of the above metrics have their corresponding allOfSmells metrics (e.g. numberOfMultifacetedSmells -> allOfMultifacetedSmells) that lists the actual smells.\n\n\nPattern and Anti-pattern\n Metrics:\n\n\n\n\npuppetPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a puppet file.\n\n\npuppetAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a puppet file.\n\n\ndockerPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a Dockerfile.\n\n\ndockerAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a Dockerfile.\n\n\n\n\nComponents\n metrics:\n\n\n\n\nallDockerLibraries: Retrieves all the libraries that are installed with the use of apt-get install or apk install in Dockerfiles\n\n\nnumberOfDockerLibraries: Retrieves the number of the above libraries\n\n\nallDockerImages: Retrieves all the images that are used with the use of FROM instruction in Dockerfiles\n\n\nnumberOfDockerImages: Retrieves the number of the above images\n\n\n\n\nNew versions\n Metrics:\n\n\n\n\nnewVersionFound: The metric will list the new versions (if any) of libraries that are used in the project.",
            "title": "Scava Metrics"
        },
        {
            "location": "/others/users/Scava-Metrics/#metrics-computed-by-scava",
            "text": "",
            "title": "Metrics computed by Scava"
        },
        {
            "location": "/others/users/Scava-Metrics/#metric-platform",
            "text": "OSGi metrics  are defined in:   https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc   List of metrics:   allOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies).  allOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).  allOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file.  numberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).  numberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).  unusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell).  ratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages.  ratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code.  numberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest.  numberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest..  unversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header.  ratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles.  unversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest.  ratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages.  unversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest.  ratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages.   Maven metrics  are defined in    https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc   List of metrics:    allMavenDependencies -- Retrieves all the Maven dependencies.  allOptionalMavenDependencies -- Retrieves all the optional Maven dependencies.  numberMavenDependencies -- Retrieves the number of Maven dependencies.  numberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies.  ratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies.  isUsingTycho -- Checks if the current project is a Tycho project.",
            "title": "Metric-platform"
        },
        {
            "location": "/others/users/Scava-Metrics/#ide-plugin",
            "text": "All metrics are defined in  metrics_definition_ide_plugin.docx   scava-lib-usage -- Level of using CROSSMINER library change function.  scava-search-usage -- Level of using CROSSMINER search function.  scava-search-success -- Level of successfull free text serach using CROSSMINER plug-in.  modifictaion-rate -- Rate of changes applied to document.  gui-usage-rate -- Rate of activly using the GUI of Eclipse.  testing-rate -- Count of executing tests.  working-time -- Average working time for Java source files.  file-access-rate -- Average count of Java source files open and brought to top.",
            "title": "IDE plugin"
        },
        {
            "location": "/others/users/Scava-Metrics/#natural-language-processing",
            "text": "All metrics are defined in  metrics_definition_nlp.docx .  Issue tracking  metrics:   Number of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project.  Number of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project.  Emotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project.  Number new bugs: It indicates how many bugs are created every delta through a lapse of time.  Number of new users: How many users are new in the bug tracking system.  Open time: This metric shows the average open time of an issue.  Number of patches: It indicates the number of patches located in the issue tracking system.  Number of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests.  Response time: It presents the average time to reply a comment in an issue tracker.  Sentiments: Which sentiments are found in a issue tracking system for a project.  Severity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement.  Status: It indicates the issues status, like open or closed.  Topics: Using clustering methods, this metric indicates the topics that are discussed through the time.  Unasnwered bugs: It indicates how many issues do not have any reply.  Users: How many users are found in a issue tracker system.   Newsgroup  metrics   Number of articles; It determines the number of articles found in a newsgroup.  Emotions: Which emotions are found in the newgroup.  Number of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup.  Number of new users: How many users make use of the newsgroup.  Number of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies.  Response time: The average time to reply a message.  Sentiments: The sentiments found in the newsgroup.  Severity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is.  Number of new threads: How many threads are created per delta.  Topics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup.  Number of unanswered topics: How many messages haven\u2019t been answered.  Number of users: Number of users that make use of the newsgroup.   Forums  metrics   Number of posts: For every topic (i.e. forum thread), how many posts in total exist.  Emotions: Which are the emotions located in the forum.  Number of topics: How many forum threads contain the forum.  Number of request and replies: From all the posts in the forum, how many are request and how many are reply.  Sentiments: Which are the sentiments that can be found in a forum.  Severity: In the case the forums are used to express issues, we can determine their severity.  Number of new topics: Number of new forum threads created.  Topics: Number of content topics, i.e. subjects, are discussed in the forum.   Number of unanswered topics: Number of forums threads which do not have a reply.",
            "title": "Natural Language Processing"
        },
        {
            "location": "/others/users/Scava-Metrics/#configuration-analysis-metrics",
            "text": "Metrics of Puppet are defined in detail in  Deliverable 4.2 .  Puppet Design  Metrics:   numberOfMultifacetedSmells --  The number of Multifaceted Abstraction smells  numberOfUnnecessarySmells -- The number of Unnecessary Abstraction smells  numberOfImperativeSmells -- The number of Imperative Abstraction smells  numberOfMissAbSmells -- The number of Missing Abstraction smells  numberOfInsufficientSmells -- The number of Insufficient Modularization smells  numberOfUnstructuredSmells -- The number of Unstructured Module smells  numberOfTightSmells -- The number of Tightly-coupled Module smells  numberOfBrokenSmells -- The number of Broken Hierarchy smells  numberOfMissingDepSmells -- The number of Missing Dependency smells  numberOfHairballSmells -- The number of Hairball Structure smells  numberOfDeficientSmells -- The number of Deficient Encapsulation smells  numberOfWeakenSmells -- The number of Weaken Modularity smells  cumulativeNumberOfDesignSmells -- The number of design smells   Puppet Implementation  Metrics:   numberOfMissingDefaultCaseSmells -- Smell exists when a default case is missing in a case or selector statement.  numberOfInconsistentNamingConventionSmells -- Smell exists when the used naming convention deviates from the recommended naming convention.  numberOfComplexExpressionSmells -- Smell exists when a program contains a difficult to understand complex expression.    numberOfDuplicateEntitySmells -- Smell exists duplicate parameters are present in the configuration code.  numberOfMisplacedAttributeSmells -- Smell exists when attribute placement within a resource or a class has not followed a recommended order.  numberOfImproperAlignmentSmells -- Smell exists when the code is not properly aligned or tabulation characters are used   numberOfInvalidPropertyValueSmells -- Smell exists when an invalid value of a property or attribute is used.  numberOfIncompleteTasksSmells -- Smell exists when the code has \u201cfixme\u201d and \u201ctodo\u201d tags indicating incomplete tasks.  numberOfDeprecatedStatementUsageSmells -- Smell exists when the configuration code uses one of the deprecated statements.  numberOfImproperQuoteUsageSmells -- Smell exists when single and double quotes are not used properly.  numberOfLongStatementSmells -- Smell exists when the code contains long statements.  numberOfIncompleteConditionalSmells -- Smell exists when an \u201cif..elseif\u201d construct used without a terminating \u201celse\u201d clause.  numberOfUnguardedVariableSmells -- Smell exists when a variable is not enclosed in braces when being interpolated in a string.   Docker Metrics are based on the following  rules .  Docker  Metrics:   numberOfUpgradeSmells -- Smell exists when apt-get or apk commands are used problematically.  numberOfPinVersionSmells -- Smell exists when version of packages is not explicitly declared.  numberOfUntaggedImageSmells -- Smell exists when version of images is not explicitly declared.  numberOfSudoSmells -- Smell exists when sudo command and root user are used inappropriately.  numberOfCopySmells -- Smell exists when COPY instruction is not used properly.  numberOfFromSmells -- Smell exists when FROM instruction is not used properly.  numberOfCmdSmells -- Smell exists when CMD and ENTRYPOINT instruction is not used properly.  numberOfAddSmells -- Smell exists when ADD instruction is not used properly.  numberOfMeaninglessCommandsSmells -- Smell exists when there are commands that makes no sense running them in a Docker container.  numberOfInvalidPortsSmells -- Smell exists when invalid UNIX ports range is used.  numberOfShellSmells -- Smell exists when SHELL instruction is not used when it should.   All of the above metrics have their corresponding allOfSmells metrics (e.g. numberOfMultifacetedSmells -> allOfMultifacetedSmells) that lists the actual smells.  Pattern and Anti-pattern  Metrics:   puppetPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a puppet file.  puppetAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a puppet file.  dockerPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a Dockerfile.  dockerAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a Dockerfile.   Components  metrics:   allDockerLibraries: Retrieves all the libraries that are installed with the use of apt-get install or apk install in Dockerfiles  numberOfDockerLibraries: Retrieves the number of the above libraries  allDockerImages: Retrieves all the images that are used with the use of FROM instruction in Dockerfiles  numberOfDockerImages: Retrieves the number of the above images   New versions  Metrics:   newVersionFound: The metric will list the new versions (if any) of libraries that are used in the project.",
            "title": "Configuration analysis metrics"
        },
        {
            "location": "/others/users/Scava-Resources/",
            "text": "Accessing Scava generated data\n\n\n\n\n\n\nKnowledge-base API description: http://\n:8080/swagger-ui.html\n\n\n\n\n\n\nGet value of a metric for a single project: http://\n:8182/projects/p/\n/m/\n\n\n\n\n\n\nList of metrics: http://\n:8182/metrics \n\n\n\n\nList of factoids: http://\n:8182/factoids",
            "title": "Scava Resources"
        },
        {
            "location": "/others/users/Scava-Resources/#accessing-scava-generated-data",
            "text": "Knowledge-base API description: http:// :8080/swagger-ui.html    Get value of a metric for a single project: http:// :8182/projects/p/ /m/    List of metrics: http:// :8182/metrics    List of factoids: http:// :8182/factoids",
            "title": "Accessing Scava generated data"
        },
        {
            "location": "/user-guide/",
            "text": "SCAVA User Guide\n\n\nThe SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.\n\n\nQuick Start Guide\n\n\n[TODO] Overivew  Required\n\n\nPlatform Administration and Project Analysis\n\n\nThe SCAVA Administration application provide services to analyse  open source software projects repository and provide several general administration feature including user managements services and platform configuration services.\n\n\nVisualisation Dashboard\n\n\nThis Visualisation Dashboard guide describes the different dashboards that are available with the platform.\n\n\nEclipse Plugin\n\n\n[TODO] Overivew  Required\n\n\nWorkflow Engine\n\n\n[TODO] Overivew  Required\n\n\nMetrics Reference Guide\n\n\nThis Metrics Reference Guide describes the historic and transient metric providers provided by the Scava platform.\n\n\nIndexes Reference Guide\n\n\nThis Indexes Reference Guide presents the mappings used to index the data in Scava platform, as well as some examples of how to retrieve the data.\n\n\nReaders Guide\n\n\nThis Readers Guide introduce the users to the readers created for retrieving data in Scava.",
            "title": "Home"
        },
        {
            "location": "/user-guide/#scava-user-guide",
            "text": "The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.",
            "title": "SCAVA User Guide"
        },
        {
            "location": "/user-guide/#quick-start-guide",
            "text": "[TODO] Overivew  Required",
            "title": "Quick Start Guide"
        },
        {
            "location": "/user-guide/#platform-administration-and-project-analysis",
            "text": "The SCAVA Administration application provide services to analyse  open source software projects repository and provide several general administration feature including user managements services and platform configuration services.",
            "title": "Platform Administration and Project Analysis"
        },
        {
            "location": "/user-guide/#visualisation-dashboard",
            "text": "This Visualisation Dashboard guide describes the different dashboards that are available with the platform.",
            "title": "Visualisation Dashboard"
        },
        {
            "location": "/user-guide/#eclipse-plugin",
            "text": "[TODO] Overivew  Required",
            "title": "Eclipse Plugin"
        },
        {
            "location": "/user-guide/#workflow-engine",
            "text": "[TODO] Overivew  Required",
            "title": "Workflow Engine"
        },
        {
            "location": "/user-guide/#metrics-reference-guide",
            "text": "This Metrics Reference Guide describes the historic and transient metric providers provided by the Scava platform.",
            "title": "Metrics Reference Guide"
        },
        {
            "location": "/user-guide/#indexes-reference-guide",
            "text": "This Indexes Reference Guide presents the mappings used to index the data in Scava platform, as well as some examples of how to retrieve the data.",
            "title": "Indexes Reference Guide"
        },
        {
            "location": "/user-guide/#readers-guide",
            "text": "This Readers Guide introduce the users to the readers created for retrieving data in Scava.",
            "title": "Readers Guide"
        },
        {
            "location": "/user-guide/administration/",
            "text": "Platform Administration User Guide\n\n\nThe guide show up the different features available through the Administration Dashboard, including:\n\n The Login feature\n\n The Projects management feature\n\n The Users management feature\n\n The Workers feature\n\n The Properties management feature\n\n The Stack Traces feature\n\n\nThe Login feature\n\n\nThe first time the user access the administration dashboard, a login form will be shown in order to authenticate the associated user.\n\n\n\n\nThe Projects management feature\n\n\nThe Projects view displays the list of the OSS projects downloaded from the OSS forges.\n\n\n\n\nThe administration dashboard provides two operations:\n* Import Project: \nIf your project is hosted on an OSS forge, you can simply paste the URL on the field and add it.\n\n\n\n\n\nCreate Project:\nThe second operation provides an extra-options to customize the project creation alongside to the metadata available from differents related sources, eg., communication channels and bug tracking systems.\n\n\n\n\n\nOnce the project has been registred, it's possible to configure inside it some analysis tasks. An analysis task is consist of:\n\n Label name: the analysis task name.\n\n Task type: the scheduling tasks execution mechanism which could be:\n  * Single Execution: which allows to execute\n a task between a start date and an end date.\n  * Monitoring Execution: which permits to run a task from a start date until the current date then to schedule the task execution each new day.\n\n Start date: the start time range of the analysis process.\n\n End date: the end time range of the analysis process.\n* Metric Providers: the metrics available on the metric-platform through the extension points mechanism.\n\n\n\n\nThese tasks will be executed later to compute/calculate some metrics that will be used to measure the quality of the OSS projects during various period of time.\n\n\n\n\nThe Users management feature\n\n\nThe Users view allows to manage the differents users of the the administration dashboards. It provides three levels of roles:\n\n\n\n\nUSER ROLE: The user profile is enable to shows up the basic administration dashboard features including:\n\n\nShow up the list of registred projects.\n\n\nShow up the analysis tasks status.\n\n\nShow up the list of metric-providers.\n\n\nManage its own token authorities.\n\n\nShow up the stack traces.\n\n\nPROJECT MANAGER ROLE: The project manager profile is enable to manage the main administration dashboard features including:\n\n\nImport/create new projects.\n\n\nShow up the list of registred projects.\n\n\nManage new analysis tasks.\n\n\nShow up the analysis tasks status.\n\n\nShow up the list of metric-providers.\n\n\nManage its own token authorities.\n\n\nShow up the stack traces.\n\n\nADMIN ROLE: The admin profile is the super-user of the admin-ui which is enable to get access to almost all the features come with the administration dashboard, including:\n\n\nImport/create new projects.\n\n\nShow up the list of registred projects.\n\n\nManage new analysis tasks.\n\n\nShow up the analysis tasks status.\n\n\nShow up the list of metric-providers.\n\n\nManage the admin-ui users and their roles.\n\n\nMonitor the workers and theirs analysis tasks.\n\n\nManage configuration properties.\n\n\nShow up the stack traces.\n\n\n\n\n\n\nThe profile view\n\n\nBoth the user and the project manager profiles have the ability to edit its own account and manage the token authorities related the Eclipse Integrated Development Environments (IDEs) component. The generated token authorities is assigned and available only on current profile.\n\n\n\nThe Workers feature\n\n\nThe workers view is dedicated to the dashboard administrators which allows them to monitor the status of the analysis tasks processes of the metric-platform. The Platform Workers section shows up the workers with theirs assigned analysis tasks. The Pending Tasks presents the analysis tasks waiting for a free worker.\n\n\n\n\nThe Properties management feature\n\n\nThe Properties feature allows to configure generic configurations applied to the metric-platform, eg., Github OAuth tokens.\n\n\n\n\nAmong the properties that can be set, we can name:\n\n GitHub\n   * Token githubToken\n\n GitLab\n   * Token: gitlabToken\n* Eclipse (both elements mandatory if an authenticated connection want to be used )\n   * ClientID: eclipseClientId\n   * ClientSecret: eclipseClientSecret\n\n\nThe Stack Traces feature\n\n\nThe Stack traces feature allows to display the errors/stracktraces produced during the metric-platform analysis process in the admin UI to ease debugging.",
            "title": "Home"
        },
        {
            "location": "/user-guide/administration/#platform-administration-user-guide",
            "text": "The guide show up the different features available through the Administration Dashboard, including:  The Login feature  The Projects management feature  The Users management feature  The Workers feature  The Properties management feature  The Stack Traces feature",
            "title": "Platform Administration User Guide"
        },
        {
            "location": "/user-guide/administration/#the-login-feature",
            "text": "The first time the user access the administration dashboard, a login form will be shown in order to authenticate the associated user.",
            "title": "The Login feature"
        },
        {
            "location": "/user-guide/administration/#the-projects-management-feature",
            "text": "The Projects view displays the list of the OSS projects downloaded from the OSS forges.   The administration dashboard provides two operations:\n* Import Project: \nIf your project is hosted on an OSS forge, you can simply paste the URL on the field and add it.   Create Project:\nThe second operation provides an extra-options to customize the project creation alongside to the metadata available from differents related sources, eg., communication channels and bug tracking systems.   Once the project has been registred, it's possible to configure inside it some analysis tasks. An analysis task is consist of:  Label name: the analysis task name.  Task type: the scheduling tasks execution mechanism which could be:\n  * Single Execution: which allows to execute\n a task between a start date and an end date.\n  * Monitoring Execution: which permits to run a task from a start date until the current date then to schedule the task execution each new day.  Start date: the start time range of the analysis process.  End date: the end time range of the analysis process.\n* Metric Providers: the metrics available on the metric-platform through the extension points mechanism.   These tasks will be executed later to compute/calculate some metrics that will be used to measure the quality of the OSS projects during various period of time.",
            "title": "The Projects management feature"
        },
        {
            "location": "/user-guide/administration/#the-users-management-feature",
            "text": "The Users view allows to manage the differents users of the the administration dashboards. It provides three levels of roles:   USER ROLE: The user profile is enable to shows up the basic administration dashboard features including:  Show up the list of registred projects.  Show up the analysis tasks status.  Show up the list of metric-providers.  Manage its own token authorities.  Show up the stack traces.  PROJECT MANAGER ROLE: The project manager profile is enable to manage the main administration dashboard features including:  Import/create new projects.  Show up the list of registred projects.  Manage new analysis tasks.  Show up the analysis tasks status.  Show up the list of metric-providers.  Manage its own token authorities.  Show up the stack traces.  ADMIN ROLE: The admin profile is the super-user of the admin-ui which is enable to get access to almost all the features come with the administration dashboard, including:  Import/create new projects.  Show up the list of registred projects.  Manage new analysis tasks.  Show up the analysis tasks status.  Show up the list of metric-providers.  Manage the admin-ui users and their roles.  Monitor the workers and theirs analysis tasks.  Manage configuration properties.  Show up the stack traces.",
            "title": "The Users management feature"
        },
        {
            "location": "/user-guide/administration/#the-profile-view",
            "text": "Both the user and the project manager profiles have the ability to edit its own account and manage the token authorities related the Eclipse Integrated Development Environments (IDEs) component. The generated token authorities is assigned and available only on current profile.",
            "title": "The profile view"
        },
        {
            "location": "/user-guide/administration/#the-workers-feature",
            "text": "The workers view is dedicated to the dashboard administrators which allows them to monitor the status of the analysis tasks processes of the metric-platform. The Platform Workers section shows up the workers with theirs assigned analysis tasks. The Pending Tasks presents the analysis tasks waiting for a free worker.",
            "title": "The Workers feature"
        },
        {
            "location": "/user-guide/administration/#the-properties-management-feature",
            "text": "The Properties feature allows to configure generic configurations applied to the metric-platform, eg., Github OAuth tokens.   Among the properties that can be set, we can name:  GitHub\n   * Token githubToken  GitLab\n   * Token: gitlabToken\n* Eclipse (both elements mandatory if an authenticated connection want to be used )\n   * ClientID: eclipseClientId\n   * ClientSecret: eclipseClientSecret",
            "title": "The Properties management feature"
        },
        {
            "location": "/user-guide/administration/#the-stack-traces-feature",
            "text": "The Stack traces feature allows to display the errors/stracktraces produced during the metric-platform analysis process in the admin UI to ease debugging.",
            "title": "The Stack Traces feature"
        },
        {
            "location": "/user-guide/dashboard/",
            "text": "Visualisation Dashboard\n\n\nThis guide describes the different dashboards that are available with the platform.\n\n\nProject Dashboard (dev/debug)\n\n\nThe Project Dashboard (dev/debug) shows all the metrics available in CROSSMINER, fetched from the SCAVA API. It displays the global value and the evolution in time of the metrics. It includes also filters to ease the analysis of a specific group of metrics. The dashboard is composed of different tables and visualizations, the tables show some statistics on the metrics (e.g., number of metrics per type, name and project), while the visualizations plot the average, sum and max values for every snapshots of a given metric.\n\n\nThe Project Dashboard (dev/debug) allows to understand the SCAVA metrics in terms of global and time values. A screenshot of the dashboard is shown below, as can be seen, the tables on the left contain global values, while the visualizations on the right allow to understand the metrics evolution. Finally, the last row of the dashboard shows different numeric values as a result of applying different metrics to the metric values. \n\n\n\n\nOverview dashboard\n\n\nThe Overview dashboard, starting from top to bottom and from left to right, includes the following visualizations:\n\n\nFirst of all, there is a table that shows the list of last facts. Then there are two \"selection\" pies, a pie chart that shows the projects. It can be used to filter by project and a pie chart that shows the top projects. It can be used to filter by top project. To follow, there is a table with the last recommendations (they come from the trans.rascal.OO metrics) and a heat map that allows to compare the projects wrt to attributes of the quality model defined \nin prosoul\n. Then, there is a table that shows the similar projects in terms of recommendation, it describes if it's active, the type and the number of recommendations. The next row shows a bar chart that shows the evolution of bugs (fixed vs closed/non resolved) and another bar chart that shows the evolution of commits. The next two rows show two bar charts as well, starting with a bar chart that shows the evolution of percentage emotions (anger, joy, sadness, surprise, love) in bugs and a bar chart that shows the evolution of active and inactive users on bug tracker. Finally, the last row includes information about a pie chart and a bar chart that show the top 10 topics on comments and their evolution. \n\n\nThe figures below show an example of the dashboard:\n\n\n\n\n\n\n\n\n\n\nFactoids dashboard\n\n\nThe Factoids dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the factoids that are in the projects:\n\n\nThe first table shows the number of projects that have from one to four factoids and the next two \"selection\" pies in order to filter the dashboard by project or top project if it is required. The following radar shows an overview of the project and how the number of factoids are distributed on them, the table shows the last factoids added.\nThe following three heat maps show different specific factoids with the projects, grouped them by bugs, commits and code quality. The last large heat map shows all the projects and all the factoids with the corresponding values.\n\n\nThe figures below show an example of the dashboard:\n\n\n\n\n\n\nDependency dashboard\n\n\nThis Dependency dashboard is useful to show the dependecies. It contains the following visualizations/tables (from left to right, top to bottom):\n\n\nIt starts with a visualization that shows the number of total deps and two pie charts, the first one is a pie chart that shows the deps grouped by project and the second one is a pie chart that shows the deps grouped by top project. Then, there are two tables, one that shows the deps grouped by type (e.g., osgi and maven) and project and other table that shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. To finish the dashboard has a table that provides details (name, versions, project and datetime) of the project deps and a table that shows the old and new version of each dependency.\n\n\nThe figures below show an example of the dashboard:\n\n\n\n\n\n\nDevOps dependencies Dashboard\n\n\nThe Devops Dependencies dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the devops dependencies that are included in the project:\n\n\nFirst of all, it has a pie chart that shows the deps grouped by project, it has also a pie chart that shows the deps grouped by top project, these two pies can be used to filter the dashboard. then there are two tables, the first one shows the deps grouped by type (e.g., puppet and docker) and project, and the second one shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. Then, there are two tables, the first one provides details (name and versions) of the project deps and the second one shows the old and new version of each dependency. To finish, there is a graph that shows the relation between projects that are using configuration files, the relation can be derived from Puppet or Docker, the table shows the type of relation in addition.\n\n\nThe figures below show an example of the dashboard:\n\n\n\n\n\n\nDevOps smells dashboard\n\n\nThe DevOps smells dashboard includes two visualizations to focus on a specific project and/or top project. Two bar charts show the evolution of the smells extracted from puppet and docker. They rely on the following historic metrics:\n- puppet.designsmells.smells\n- puppet.implementation.smells\n- docker.smells\n\n\nTwo tables allow to focus on the smells details. The former shows information about design and configuration smells, while the latter gives insights about antipatterns smells. Both of them have the information of the file and the line.\n\n\nThe figures below show an example of the dashboard:\n\n\n\n\n\n\nQuality model dashboard\n\n\nThe Quality model dashboard shows the quality model assessment based on the model available \nin prosoul repo\n\n\nIt starts with a table that ranks the projects based on the sum of their scores, which are the normalized values (0-5) of the metrics based on the threshold defined in the quality model and a pie chart that shows the projects. It can be used to filter by project. Then there is a heat map that allows to compare projects based on the qm metric scores and a bar chart that shows the qm metric scores (useful to focus on one project). Following, it has a heat map that allows to compare projects based on the qm attributes (median of the corresponding metric scores) and a bar chart that shows the qm attributes (useful to focus on one project). It continues with another heat map that allows to compare projects based on the qm goals (median of the corresponding metric scores) and a bar chart that shows the qm goals (useful to focus on one project). To finish, there are three bar charts that show the evolution in time of the quality models by quarters.\n\n\nThe figures below show an example of the dashboard:\n\n\n\n\n\n\nUsers dashboard\n\n\nThe Users dashboard is useful to show user activity. It contains the following visualizations/tables (from left to right, top to bottom):\n\n\nFirst, it has two pie charts, a pie chart that shows the projects (it can be used to filter by project) and a pie chart that shows the top projects (it can be used to filter by top project). Then, there is a table that shows the top users based on the sum of their churns and a bar chart that shows the churn evolution (it can be filtered by user). To finish, there is a bar chart that shows the evolution of active and inactive users on bug tracker.\n\n\nThe figure below shows an example of the dashboard:\n\n\n\n\nSentiment and emotion dashboard\n\n\nThe Sentiment and emotion dashboard includes 2 visualizations to select a project and top project, a visualization that summarizes the number of emotions, 2 visualizations that show the emotion trend per project based on weighted values and the corresponding details, 2 visualizations that show the sentiment trend per project and details about the sentiment at the beginning and end of the threads.\n\n\nThe figure below shows an example of the dashboard:",
            "title": "Home"
        },
        {
            "location": "/user-guide/dashboard/#visualisation-dashboard",
            "text": "This guide describes the different dashboards that are available with the platform.",
            "title": "Visualisation Dashboard"
        },
        {
            "location": "/user-guide/dashboard/#project-dashboard-devdebug",
            "text": "The Project Dashboard (dev/debug) shows all the metrics available in CROSSMINER, fetched from the SCAVA API. It displays the global value and the evolution in time of the metrics. It includes also filters to ease the analysis of a specific group of metrics. The dashboard is composed of different tables and visualizations, the tables show some statistics on the metrics (e.g., number of metrics per type, name and project), while the visualizations plot the average, sum and max values for every snapshots of a given metric.  The Project Dashboard (dev/debug) allows to understand the SCAVA metrics in terms of global and time values. A screenshot of the dashboard is shown below, as can be seen, the tables on the left contain global values, while the visualizations on the right allow to understand the metrics evolution. Finally, the last row of the dashboard shows different numeric values as a result of applying different metrics to the metric values.",
            "title": "Project Dashboard (dev/debug)"
        },
        {
            "location": "/user-guide/dashboard/#overview-dashboard",
            "text": "The Overview dashboard, starting from top to bottom and from left to right, includes the following visualizations:  First of all, there is a table that shows the list of last facts. Then there are two \"selection\" pies, a pie chart that shows the projects. It can be used to filter by project and a pie chart that shows the top projects. It can be used to filter by top project. To follow, there is a table with the last recommendations (they come from the trans.rascal.OO metrics) and a heat map that allows to compare the projects wrt to attributes of the quality model defined  in prosoul . Then, there is a table that shows the similar projects in terms of recommendation, it describes if it's active, the type and the number of recommendations. The next row shows a bar chart that shows the evolution of bugs (fixed vs closed/non resolved) and another bar chart that shows the evolution of commits. The next two rows show two bar charts as well, starting with a bar chart that shows the evolution of percentage emotions (anger, joy, sadness, surprise, love) in bugs and a bar chart that shows the evolution of active and inactive users on bug tracker. Finally, the last row includes information about a pie chart and a bar chart that show the top 10 topics on comments and their evolution.   The figures below show an example of the dashboard:",
            "title": "Overview dashboard"
        },
        {
            "location": "/user-guide/dashboard/#factoids-dashboard",
            "text": "The Factoids dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the factoids that are in the projects:  The first table shows the number of projects that have from one to four factoids and the next two \"selection\" pies in order to filter the dashboard by project or top project if it is required. The following radar shows an overview of the project and how the number of factoids are distributed on them, the table shows the last factoids added.\nThe following three heat maps show different specific factoids with the projects, grouped them by bugs, commits and code quality. The last large heat map shows all the projects and all the factoids with the corresponding values.  The figures below show an example of the dashboard:",
            "title": "Factoids dashboard"
        },
        {
            "location": "/user-guide/dashboard/#dependency-dashboard",
            "text": "This Dependency dashboard is useful to show the dependecies. It contains the following visualizations/tables (from left to right, top to bottom):  It starts with a visualization that shows the number of total deps and two pie charts, the first one is a pie chart that shows the deps grouped by project and the second one is a pie chart that shows the deps grouped by top project. Then, there are two tables, one that shows the deps grouped by type (e.g., osgi and maven) and project and other table that shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. To finish the dashboard has a table that provides details (name, versions, project and datetime) of the project deps and a table that shows the old and new version of each dependency.  The figures below show an example of the dashboard:",
            "title": "Dependency dashboard"
        },
        {
            "location": "/user-guide/dashboard/#devops-dependencies-dashboard",
            "text": "The Devops Dependencies dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the devops dependencies that are included in the project:  First of all, it has a pie chart that shows the deps grouped by project, it has also a pie chart that shows the deps grouped by top project, these two pies can be used to filter the dashboard. then there are two tables, the first one shows the deps grouped by type (e.g., puppet and docker) and project, and the second one shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. Then, there are two tables, the first one provides details (name and versions) of the project deps and the second one shows the old and new version of each dependency. To finish, there is a graph that shows the relation between projects that are using configuration files, the relation can be derived from Puppet or Docker, the table shows the type of relation in addition.  The figures below show an example of the dashboard:",
            "title": "DevOps dependencies Dashboard"
        },
        {
            "location": "/user-guide/dashboard/#devops-smells-dashboard",
            "text": "The DevOps smells dashboard includes two visualizations to focus on a specific project and/or top project. Two bar charts show the evolution of the smells extracted from puppet and docker. They rely on the following historic metrics:\n- puppet.designsmells.smells\n- puppet.implementation.smells\n- docker.smells  Two tables allow to focus on the smells details. The former shows information about design and configuration smells, while the latter gives insights about antipatterns smells. Both of them have the information of the file and the line.  The figures below show an example of the dashboard:",
            "title": "DevOps smells dashboard"
        },
        {
            "location": "/user-guide/dashboard/#quality-model-dashboard",
            "text": "The Quality model dashboard shows the quality model assessment based on the model available  in prosoul repo  It starts with a table that ranks the projects based on the sum of their scores, which are the normalized values (0-5) of the metrics based on the threshold defined in the quality model and a pie chart that shows the projects. It can be used to filter by project. Then there is a heat map that allows to compare projects based on the qm metric scores and a bar chart that shows the qm metric scores (useful to focus on one project). Following, it has a heat map that allows to compare projects based on the qm attributes (median of the corresponding metric scores) and a bar chart that shows the qm attributes (useful to focus on one project). It continues with another heat map that allows to compare projects based on the qm goals (median of the corresponding metric scores) and a bar chart that shows the qm goals (useful to focus on one project). To finish, there are three bar charts that show the evolution in time of the quality models by quarters.  The figures below show an example of the dashboard:",
            "title": "Quality model dashboard"
        },
        {
            "location": "/user-guide/dashboard/#users-dashboard",
            "text": "The Users dashboard is useful to show user activity. It contains the following visualizations/tables (from left to right, top to bottom):  First, it has two pie charts, a pie chart that shows the projects (it can be used to filter by project) and a pie chart that shows the top projects (it can be used to filter by top project). Then, there is a table that shows the top users based on the sum of their churns and a bar chart that shows the churn evolution (it can be filtered by user). To finish, there is a bar chart that shows the evolution of active and inactive users on bug tracker.  The figure below shows an example of the dashboard:",
            "title": "Users dashboard"
        },
        {
            "location": "/user-guide/dashboard/#sentiment-and-emotion-dashboard",
            "text": "The Sentiment and emotion dashboard includes 2 visualizations to select a project and top project, a visualization that summarizes the number of emotions, 2 visualizations that show the emotion trend per project based on weighted values and the corresponding details, 2 visualizations that show the sentiment trend per project and details about the sentiment at the beginning and end of the threads.  The figure below shows an example of the dashboard:",
            "title": "Sentiment and emotion dashboard"
        },
        {
            "location": "/user-guide/indexes/",
            "text": "Indexing Metrics Guide\n\n\nThis guide describes the indexing metric providers available in Scava platform. It provides the mapping so that users can query the index.\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.bugs\n\n\n\n\nShort name\n: bug indexing metric\n\n\nFriendly name\n: Bugs tracking system indexer\n\n\n\n\nThis metric prepares and indexes analyses documents relating to bug tracking system.\n\n\nMapping Information\n:\n\n\n\n\nbug.comment\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncomment_Id\n\n\nkeyword\n\n\n\n\n\n\nbody\n\n\ntext\n\n\n\n\n\n\nemotional_dimension *\n\n\nkeyword\n\n\n\n\n\n\nsentiment\n\n\nkeyword\n\n\n\n\n\n\nplain_text\n\n\ntext\n\n\n\n\n\n\nrequest_reply_classification\n\n\nkeyword\n\n\n\n\n\n\ncontent_class\n\n\nkeyword\n\n\n\n\n\n\ncontains_code\n\n\nboolean\n\n\n\n\n\n\nbug_Id\n\n\nkeyword\n\n\n\n\n\n\nproject_name\n\n\nkeyword\n\n\n\n\n\n\ncreator\n\n\nkeyword\n\n\n\n\n\n\ncreated_at\n\n\ndate\n\n\n\n\n\n\nuid\n\n\nkeyword\n\n\n\n\n\n\nreferring_to \n   bugs *\n    commits *\n\n\nObject\ntext\n \ntext\n\n\n\n\n\n\n\n\n\n\nbug.post\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncreated_at\n\n\ndate\n\n\n\n\n\n\nbug_summary\n\n\ntext\n\n\n\n\n\n\nseverity\n\n\nkeyword\n\n\n\n\n\n\nbug_id\n\n\nkeyword\n\n\n\n\n\n\nproject_name\n\n\ntext\n\n\n\n\n\n\ncreator\n\n\nkeyword\n\n\n\n\n\n\nuid\n\n\nkeyword\n\n\n\n\n\n\nmigration_issue\n found\n problematic_changes\n       change *\n      type\n\n\nObject\nboolean\nObject\ntext\ndouble\n\n\n\n\n\n\n\n\nAdditional Note\n: \n\n\n\n\nAsterisk (\n*\n) is used to indicate entries that are stored as an array.\n\n\nType \nObject\n is used to indicate an entry with a nested JSON object passed as its value. The depth of nesting is shown with Indentation at each node/level.    \n\n\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.commits\n\n\n\n\nShort name\n: metricprovider.indexing.commits\n\n\nFriendly name\n: Commits indexer\n\n\n\n\nThis metric prepares and indexes documents related to commits.\n\n\nMapping Information\n:\n\n\n\n\ncommits\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncreated_at\n\n\ndate\n\n\n\n\n\n\nproject_name\n\n\nkeyword\n\n\n\n\n\n\nuid\n\n\ntext\n\n\n\n\n\n\nrepository\n\n\nkeyword\n\n\n\n\n\n\nrevision\n\n\nkeyword\n\n\n\n\n\n\nauthor\n\n\nkeyword\n\n\n\n\n\n\nauthor_email\n\n\nkeyword\n\n\n\n\n\n\nbody\n\n\ntext\n\n\n\n\n\n\nplain_text\n\n\ntext\n\n\n\n\n\n\ncommits_references *\n\n\ntext\n\n\n\n\n\n\nbugs_references *\n\n\ntext\n\n\n\n\n\n\n\n\nAdditional Note\n: \n\n\nAsterisk (\n*\n) is used to indicate entries that are stored as an array.\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.communicationchannels\n\n\n\n\nShort name\n: communication channels indexing metric\n\n\nFriendly name\n: communication channels indexer\n\n\n\n\nThis metric prepares and indexes documents relating to communication channels.\n\n\nMapping Information\n:\n\n\n\n\narticle\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\narticle_Id\n\n\ndate\n\n\n\n\n\n\ncommunication_channel_id\n\n\nkeyword\n\n\n\n\n\n\nuid\n\n\nkeyword\n\n\n\n\n\n\nthread_id *\n\n\nkeyword\n\n\n\n\n\n\nproject_name\n\n\nkeyword\n\n\n\n\n\n\nmessage_body\n\n\ntext\n\n\n\n\n\n\nsubject\n\n\ntext\n\n\n\n\n\n\ncreator\n\n\nkeyword\n\n\n\n\n\n\ncreated_at\n\n\ndate\n\n\n\n\n\n\nemotional_dimension *\n\n\nkeyword\n\n\n\n\n\n\nsentiment\n\n\nkeyword\n\n\n\n\n\n\nplain_text\n\n\ntext\n\n\n\n\n\n\nrequest_reply_classification\n\n\nkeyword\n\n\n\n\n\n\ncontent_class\n\n\nkeyword\n\n\n\n\n\n\ncontains_code\n\n\nboolean\n\n\n\n\n\n\n\n\n\n\nthread\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncommunication_channel_id\n\n\nkeyword\n\n\n\n\n\n\nuid\n\n\nkeyword\n\n\n\n\n\n\nthread_id\n\n\nkeyword\n\n\n\n\n\n\nproject_name\n\n\nkeyword\n\n\n\n\n\n\nsubject\n\n\ntext\n\n\n\n\n\n\nmigration_issue\n found\n problematic_changes\n       change *\n      matching_score\n\n\nObject\nboolean\nObject\ntext\ndouble\n\n\n\n\n\n\n\n\nAdditional Note\n: \n\n\n\n\nAsterisk (\n*\n) is used to indicate entries that are stored as an array.\n\n\nType \nObject\n is used to indicate an entry with a nested JSON object passed as its value. The depth of nesting is shown with Indentation at each node/level.  \n\n\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.documentation\n\n\n\n\nShort name\n: communication channels indexing metric\n\n\nFriendly name\n: communication channels indexer\n\n\n\n\nThis metric prepares and indexes documents relating to communication channels.\n\n\nMapping Information\n:\n\n\n\n\ndocumentation_entry\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncreated_at\n\n\ndate\n\n\n\n\n\n\nproject_name\n\n\nkeyword\n\n\n\n\n\n\nuid\n\n\nkeyword\n\n\n\n\n\n\ndocumentation_id\n\n\nkeyword\n\n\n\n\n\n\ndocumentation_entry_id\n\n\nkeyword\n\n\n\n\n\n\ncommunication_channel_id\n\n\nkeyword\n\n\n\n\n\n\nbody\n\n\ntext\n\n\n\n\n\n\noriginal_format_mime\n\n\nkeyword\n\n\n\n\n\n\noriginal_format_name\n\n\ntext\n\n\n\n\n\n\nplain_text\n\n\ntext\n\n\n\n\n\n\nsentiment\n\n\nkeyword\n\n\n\n\n\n\nreadability\n\n\nkeyword\n\n\n\n\n\n\ndocumentation_type *\n\n\nkeyword\n\n\n\n\n\n\nlicence_found\n\n\nboolean\n\n\n\n\n\n\nlicence\n group\n name\n  header_found\n  score\n is_deprecated_license\n is_fsf_libre\n  is_osi_approved\n   license_comments\n\n\nObject\nkeyword\nkeyword\nboolean\ndouble\nboolean\nboolean\nboolean\ntext\n\n\n\n\n\n\n\n\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nType\n\n\n\n\n\n\n\n\n\n\nlast_update\n\n\ndate\n\n\n\n\n\n\nproject_name\n\n\nkeyword\n\n\n\n\n\n\nuid\n\n\ntext\n\n\n\n\n\n\ndocumentation_id\n\n\nkeyword\n\n\n\n\n\n\ndocumentation_entries *\n\n\nkeyword\n\n\n\n\n\n\n\n\nAdditional Note\n: \n\n\n\n\nAsterisk (\n*\n) is used to indicate entries that are stored as an array.\n\n\nType \nObject\n is used to indicate an entry with a nested JSON object passed as its value. The depth of nesting is shown with Indentation at each node/level.  \n\n\n\n\n\n\nQuery Examples\n\n\nHere we present a series of examples of how you can use ElasticSearch queries for finding specific data or doing some aggregations.\n\n\nMore information about ElasticSearch queries can be found at https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search.html\n\n\nExample 1\n\n\nRetrieve the documentation entries that have been determined containing a \nGetting Started\n guide.\n\n\nGET documentation.documentation.entry.nlp/_search\n{\n  \"query\": {\n    \"match\" : {\n      \"documentation_types\": \"Started\"\n\n    }\n  }\n}\n\n\n\n\nAs \ndocumentation_types\n is an array, we need to use a \nmatch\n within the query.\n\n\nExample 2\n\n\nRetrieve the documentation entries that have at least one of the following characteristics:\n- Are a \nGetting Started\n guide\n- Are a \nDevelopment\n guide\n- Have a neutral sentiment\n\n\nGET documentation.documentation.entry.nlp/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n          {\n            \"match\" : {\n              \"documentation_types\": \"Started\"\n            }\n          },\n          {\n            \"match\" : {\n              \"documentation_types\": \"Development\"\n            }\n          },\n          {\n            \"term\":{\n              \"sentiment\" : \"__label__neutral\"\n            }\n          }\n      ]\n    }\n  }\n}\n\n\n\n\nIn this case, the field sentiment is not an array, thus, for looking inside of it we should use a \nterm\n query, while a \nmatch\n query for fields that are arrays. Furthermore, as we have multiple queries that are linked through OR, we need to declare a query of type \nbool\n and \nshould\n. If the query would be to find the documentation entries that have a \nGetting Started\n, a \nDevelopment\n guide and have a neutral sentiment, instead of using a \nshould\n operation, it need to be used a \nmust\n.\n\n\nGET documentation.documentation.entry.nlp/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n          {\n            \"match\" : {\n              \"documentation_types\": \"Started\"\n            }\n          },\n          {\n            \"match\" : {\n              \"documentation_types\": \"Development\"\n            }\n          },\n          {\n            \"term\":{\n              \"sentiment\" : \"__label__neutral\"\n            }\n          }\n      ]\n    }\n  }\n}\n\n\n\n\nExample 3\n\n\nSearching for GitHub Issues comments containing the word \nissue\n in the field \nplain_text\n:\n\n\nGET github.bug.comment.nlp/_search\n{\n  \"query\": {\n    \"match\": {\n      \"plain_text\": \"issue\"\n    }\n  }\n}\n\n\n\n\nExample 4\n\n\nSearching in all the indexes the string \nAst\u00e9rix\n:\n\n\nGET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix\"\n   }\n }\n}\n\n\n\n\nIn this case, as we are not quering any particular field, we use the \nquery_string\n.\n\n\nIf we would like to search either for \nAst\u00e9rix\n or \nOb\u00e9lix\n, we can use boolean operators:\n\n\nGET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix OR Ob\u00e9lix\"\n   }\n }\n}\n\n\n\n\nIf both names must appear, instead of using OR, we can use AND.\n\n\nGET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix AND Ob\u00e9lix\"\n   }\n }\n}\n\n\n\n\nQuery strings such as \nAst\u00e9rix Ob\u00e9lix\n will be considered to have an implicit OR, i.e. \nAst\u00e9ric OR Ob\u00e9lix\n.\n\n\nGET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix Ob\u00e9lix\"\n   }\n }\n}\n\n\n\n\nIf the we are looking for the exact combination of words, such as \nAst\u00e9rix and Ob\u00e9lix\n, we can use parenthesis to indicate ElasticSearch that the words should be split.\n\n\nGET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"(Ast\u00e9rix and Ob\u00e9lix) OR (Felix the Cat)\"\n   }\n }\n}\n\n\n\n\nExample 5\n\n\nElasticSearch can do some statistics regarding the output of queries. In this example, we want extended statistics about the field readability in the documentation entries:\n\n\nGET documentation.documentation.entry.nlp/_search\n{\n  \"size\": 0,\n  \"aggs\" : {\n        \"readability_stats\" : {\n            \"extended_stats\" : {\n                \"field\" : \"readability\"\n            }\n        }\n    }\n}\n\n\n\n\nThe option \nsize=0\n, indicates that only the aggregation with the statistics must be returned, otherwise the query will return the elements that also match the searching query.\n\n\nIf instead of wanting some statistics, we want to autogenerate histograms, then we can use the following query:\n\n\nPOST documentation.documentation.entry.nlp/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"histogram_analysis\": {\n      \"histogram\" : {\n                \"field\" : \"readability\",\n                \"interval\" : 2\n      }\n    }\n  }\n}\n\n\n\n\nExample 6\n\n\nWith the indexes it is possible to do aggregations and how many times a field value defined as a keyword, such as documentation_id, appears in an index.\n\n\nPOST documentation.documentation.entry.nlp/_search\n{\n  \"size\": 0, \n  \"aggs\": {\n    \"doc_id\": {\n      \"terms\": {\n        \"field\": \"documentation_id\",\n        \"size\": 100\n      }\n    }\n  }\n}",
            "title": "Home"
        },
        {
            "location": "/user-guide/indexes/#indexing-metrics-guide",
            "text": "This guide describes the indexing metric providers available in Scava platform. It provides the mapping so that users can query the index.",
            "title": "Indexing Metrics Guide"
        },
        {
            "location": "/user-guide/indexes/#orgeclipsescavametricproviderindexingbugs",
            "text": "Short name : bug indexing metric  Friendly name : Bugs tracking system indexer   This metric prepares and indexes analyses documents relating to bug tracking system.  Mapping Information :   bug.comment      Properties  Type      comment_Id  keyword    body  text    emotional_dimension *  keyword    sentiment  keyword    plain_text  text    request_reply_classification  keyword    content_class  keyword    contains_code  boolean    bug_Id  keyword    project_name  keyword    creator  keyword    created_at  date    uid  keyword    referring_to     bugs *     commits *  Object text   text      bug.post      Properties  Type      created_at  date    bug_summary  text    severity  keyword    bug_id  keyword    project_name  text    creator  keyword    uid  keyword    migration_issue  found  problematic_changes        change *       type  Object boolean Object text double     Additional Note :    Asterisk ( * ) is used to indicate entries that are stored as an array.  Type  Object  is used to indicate an entry with a nested JSON object passed as its value. The depth of nesting is shown with Indentation at each node/level.",
            "title": "org.eclipse.scava.metricprovider.indexing.bugs"
        },
        {
            "location": "/user-guide/indexes/#orgeclipsescavametricproviderindexingcommits",
            "text": "Short name : metricprovider.indexing.commits  Friendly name : Commits indexer   This metric prepares and indexes documents related to commits.  Mapping Information :   commits      Properties  Type      created_at  date    project_name  keyword    uid  text    repository  keyword    revision  keyword    author  keyword    author_email  keyword    body  text    plain_text  text    commits_references *  text    bugs_references *  text     Additional Note :   Asterisk ( * ) is used to indicate entries that are stored as an array.",
            "title": "org.eclipse.scava.metricprovider.indexing.commits"
        },
        {
            "location": "/user-guide/indexes/#orgeclipsescavametricproviderindexingcommunicationchannels",
            "text": "Short name : communication channels indexing metric  Friendly name : communication channels indexer   This metric prepares and indexes documents relating to communication channels.  Mapping Information :   article      Properties  Type      article_Id  date    communication_channel_id  keyword    uid  keyword    thread_id *  keyword    project_name  keyword    message_body  text    subject  text    creator  keyword    created_at  date    emotional_dimension *  keyword    sentiment  keyword    plain_text  text    request_reply_classification  keyword    content_class  keyword    contains_code  boolean      thread      Properties  Type      communication_channel_id  keyword    uid  keyword    thread_id  keyword    project_name  keyword    subject  text    migration_issue  found  problematic_changes        change *       matching_score  Object boolean Object text double     Additional Note :    Asterisk ( * ) is used to indicate entries that are stored as an array.  Type  Object  is used to indicate an entry with a nested JSON object passed as its value. The depth of nesting is shown with Indentation at each node/level.",
            "title": "org.eclipse.scava.metricprovider.indexing.communicationchannels"
        },
        {
            "location": "/user-guide/indexes/#orgeclipsescavametricproviderindexingdocumentation",
            "text": "Short name : communication channels indexing metric  Friendly name : communication channels indexer   This metric prepares and indexes documents relating to communication channels.  Mapping Information :   documentation_entry      Properties  Type      created_at  date    project_name  keyword    uid  keyword    documentation_id  keyword    documentation_entry_id  keyword    communication_channel_id  keyword    body  text    original_format_mime  keyword    original_format_name  text    plain_text  text    sentiment  keyword    readability  keyword    documentation_type *  keyword    licence_found  boolean    licence  group  name   header_found   score  is_deprecated_license  is_fsf_libre   is_osi_approved    license_comments  Object keyword keyword boolean double boolean boolean boolean text      documentation      Properties  Type      last_update  date    project_name  keyword    uid  text    documentation_id  keyword    documentation_entries *  keyword     Additional Note :    Asterisk ( * ) is used to indicate entries that are stored as an array.  Type  Object  is used to indicate an entry with a nested JSON object passed as its value. The depth of nesting is shown with Indentation at each node/level.",
            "title": "org.eclipse.scava.metricprovider.indexing.documentation"
        },
        {
            "location": "/user-guide/indexes/#query-examples",
            "text": "Here we present a series of examples of how you can use ElasticSearch queries for finding specific data or doing some aggregations.  More information about ElasticSearch queries can be found at https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search.html",
            "title": "Query Examples"
        },
        {
            "location": "/user-guide/indexes/#example-1",
            "text": "Retrieve the documentation entries that have been determined containing a  Getting Started  guide.  GET documentation.documentation.entry.nlp/_search\n{\n  \"query\": {\n    \"match\" : {\n      \"documentation_types\": \"Started\"\n\n    }\n  }\n}  As  documentation_types  is an array, we need to use a  match  within the query.",
            "title": "Example 1"
        },
        {
            "location": "/user-guide/indexes/#example-2",
            "text": "Retrieve the documentation entries that have at least one of the following characteristics:\n- Are a  Getting Started  guide\n- Are a  Development  guide\n- Have a neutral sentiment  GET documentation.documentation.entry.nlp/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n          {\n            \"match\" : {\n              \"documentation_types\": \"Started\"\n            }\n          },\n          {\n            \"match\" : {\n              \"documentation_types\": \"Development\"\n            }\n          },\n          {\n            \"term\":{\n              \"sentiment\" : \"__label__neutral\"\n            }\n          }\n      ]\n    }\n  }\n}  In this case, the field sentiment is not an array, thus, for looking inside of it we should use a  term  query, while a  match  query for fields that are arrays. Furthermore, as we have multiple queries that are linked through OR, we need to declare a query of type  bool  and  should . If the query would be to find the documentation entries that have a  Getting Started , a  Development  guide and have a neutral sentiment, instead of using a  should  operation, it need to be used a  must .  GET documentation.documentation.entry.nlp/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n          {\n            \"match\" : {\n              \"documentation_types\": \"Started\"\n            }\n          },\n          {\n            \"match\" : {\n              \"documentation_types\": \"Development\"\n            }\n          },\n          {\n            \"term\":{\n              \"sentiment\" : \"__label__neutral\"\n            }\n          }\n      ]\n    }\n  }\n}",
            "title": "Example 2"
        },
        {
            "location": "/user-guide/indexes/#example-3",
            "text": "Searching for GitHub Issues comments containing the word  issue  in the field  plain_text :  GET github.bug.comment.nlp/_search\n{\n  \"query\": {\n    \"match\": {\n      \"plain_text\": \"issue\"\n    }\n  }\n}",
            "title": "Example 3"
        },
        {
            "location": "/user-guide/indexes/#example-4",
            "text": "Searching in all the indexes the string  Ast\u00e9rix :  GET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix\"\n   }\n }\n}  In this case, as we are not quering any particular field, we use the  query_string .  If we would like to search either for  Ast\u00e9rix  or  Ob\u00e9lix , we can use boolean operators:  GET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix OR Ob\u00e9lix\"\n   }\n }\n}  If both names must appear, instead of using OR, we can use AND.  GET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix AND Ob\u00e9lix\"\n   }\n }\n}  Query strings such as  Ast\u00e9rix Ob\u00e9lix  will be considered to have an implicit OR, i.e.  Ast\u00e9ric OR Ob\u00e9lix .  GET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"Ast\u00e9rix Ob\u00e9lix\"\n   }\n }\n}  If the we are looking for the exact combination of words, such as  Ast\u00e9rix and Ob\u00e9lix , we can use parenthesis to indicate ElasticSearch that the words should be split.  GET _search\n{\n \"query\": {\n   \"query_string\": {\n     \"query\": \"(Ast\u00e9rix and Ob\u00e9lix) OR (Felix the Cat)\"\n   }\n }\n}",
            "title": "Example 4"
        },
        {
            "location": "/user-guide/indexes/#example-5",
            "text": "ElasticSearch can do some statistics regarding the output of queries. In this example, we want extended statistics about the field readability in the documentation entries:  GET documentation.documentation.entry.nlp/_search\n{\n  \"size\": 0,\n  \"aggs\" : {\n        \"readability_stats\" : {\n            \"extended_stats\" : {\n                \"field\" : \"readability\"\n            }\n        }\n    }\n}  The option  size=0 , indicates that only the aggregation with the statistics must be returned, otherwise the query will return the elements that also match the searching query.  If instead of wanting some statistics, we want to autogenerate histograms, then we can use the following query:  POST documentation.documentation.entry.nlp/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"histogram_analysis\": {\n      \"histogram\" : {\n                \"field\" : \"readability\",\n                \"interval\" : 2\n      }\n    }\n  }\n}",
            "title": "Example 5"
        },
        {
            "location": "/user-guide/indexes/#example-6",
            "text": "With the indexes it is possible to do aggregations and how many times a field value defined as a keyword, such as documentation_id, appears in an index.  POST documentation.documentation.entry.nlp/_search\n{\n  \"size\": 0, \n  \"aggs\": {\n    \"doc_id\": {\n      \"terms\": {\n        \"field\": \"documentation_id\",\n        \"size\": 100\n      }\n    }\n  }\n}",
            "title": "Example 6"
        },
        {
            "location": "/user-guide/metrics/",
            "text": "Metrics Reference Guide\n\n\nThis guide describes the historic and transient metric providers, as well as factoids, provided by the Scava platform.\n\n\n\n\nHistoric Metric Providers\n for:\n\n\nBug Trackers\n\n\nNewsgroups and Forums\n\n\nCommits and Committers\n\n\nDocumentation\n\n\nGeneric Source Code\n\n\nJava Code\n\n\nOSGi Dependencies\n\n\nMaven Dependencies\n\n\nDocker Dependencies\n\n\nPuppet Dependencies\n\n\n\n\n\n\nDocker Smells\n\n\nPuppet Smells\n\n\n\n\n\n\nTransient Metric Providers\n for:\n\n\nBug Trackers\n\n\nNewsgroups and forums\n\n\nDocumentation\n\n\nNatural Language Processing\n\n\nCommits and Committers\n\n\nGeneric Source Code\n\n\nJava Code\n\n\nOSGi Dependencies\n\n\nMaven Dependencies\n\n\nDocker Dependencies\n\n\nPuppet Dependencies\n\n\n\n\n\n\nDocker Smells\n\n\nPuppet Smells\n\n\nDocker Antipatterns\n\n\nPuppet Antipatterns\n\n\nProjects Relations\n\n\nNew Versions\n\n\nIndexing\n\n\nAPI\n\n\n\n\n\n\nFactoids\n for:\n\n\nBug Trackers\n\n\nNewsgroups and Forums\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\n\nHistoric Metric Providers\n\n\nHistoric metrics maintain a record of various heuristics associated with a specific open source project over its lifetime. They typically depend on the results from one or more transient metrics and are typically displayed in the Scava dashboards.\n\n\nHistoric Metric Providers for Bug Trackers\n\n\nThe following Historic Metric Providers are associated with Issue trackers\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.bugs\n\n\n\n\nShort name\n: historic.bugs.bugs\n\n\nFriendly name\n: Number of bugs per day per bug tracker\n\n\n\n\nThis metric computes the number of bugs per day for each bug tracker seperately. It also computes additional information such as average comments per bug, average comments per user, average requests and/or replies per user and bug.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n,\n\n\norg.eclipse.scava.metricprovider.trans.bugs.activeusers\n\n\n\n\n\n\nReturns\n :  \nBugsBugsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackers\n\n\nList<DailyBugTrackerData>\n\n\n\n\n\n\nnumberOfBugs\n\n\nint\n\n\n\n\n\n\naverageCommentsPerBug\n\n\nfloat\n\n\n\n\n\n\naverageRequestsPerBug\n\n\nfloat\n\n\n\n\n\n\naverageRepliesPerBug\n\n\nfloat\n\n\n\n\n\n\naverageCommentsPerUser\n\n\nfloat\n\n\n\n\n\n\naverageRequestsPerUser\n\n\nfloat\n\n\n\n\n\n\naverageRepliesPerUser\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfBugs\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nBugsHistoricMetricProvider :\n\n\nid\n  bugs.bugs\n\n\nid\n  bugs.comments-bugaverage\n\n\nid\n  bugs.comments-useraverage\n\n\nid\n  bugs.requests-bugaverage\n\n\nid\n  bugs.requests-useraverage\n\n\nid\n  bugs.replies-bugaverage\n\n\nid\n  bugs.replies-useraverage\n\n\nid\n  bugs.requestsreplies-useraverage\n\n\nid\n  bugs.requestsreplies-bugaverage\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.comments\n\n\n\n\nShort name\n: historic.bugs.comments\n\n\nFriendly name\n: Number of bug comments per day per bug tracker\n\n\n\n\nThis metric computes the number of bug comments submitted by the community (users) per day for each bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.comments\n\n\n\n\n\n\nReturns\n :  \nBugsCommentsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nBugs\n\n\nList<DailyBugData>\n\n\n\n\n\n\nnumberOfComments\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfComments\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugData :\n\n\nString\n  bugTrackerID\n\n\nint\n numberOfComments\n\n\nint\n cumulativeNumberOfComments\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nCommentsHistoricMetricProvider :\n\n\nid\n  bugs.comments\n\n\nid\n  bugs.cumulativeComments\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.emotions\n\n\n\n\nShort name\n: historic.bugs.emotions\n\n\nFriendly name\n: Number of emotions per day per bug tracker\n\n\n\n\nThis metric computes the emotional dimensions present in bug comments submitted by the community (users) per day for each bug tracker. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.emotions\n\n\n\n\n\n\nReturns\n :  \nBugsEmotionsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugData\n\n\nList<BugData>\n\n\n\n\n\n\nDimensions\n\n\nList<Dimensions>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nBugData :\n\n\n\n\nString\n bugTrackerID\n\n\nint\n    numberOfComments\n\n\nint\n    cumulativeNumberOfComments\n\n\n\n\n\n\n\n\nDimensions :\n\n\n\n\nString\n   bugTrackerId\n\n\nString\n  emotionLabel (\nanger\n, \nfear\n, \njoy\n, \nsadness\n, \nlove\n, \nsurprise\n)\n\n\nint\n    numberOfComments\n\n\nint\n    cumulativeNumberOfComments\n\n\nfloat\n  percentage\n\n\nfloat\n  cumulativePercentage\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nEmotionsHistoricMetricProvider :\n\n\nid\n  bugs.emotions.cumulativeComments\n\n\nid\n  bugs.emotions.cumulativeCommentPercentages\n\n\nid\n  bugs.emotions.comments\n\n\nid\n  bugs.emotions.commentPercentages\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.migrationissues\n\n\n\n\nShort name\n: historic.bugs.migrationissues\n\n\nFriendly name\n: Migration Issues Detection in Bug Trackers per day per bug tracker\n\n\n\n\nThis metric stores how many migration issues have been found per day for each bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.migrationissues\n\n\n\n\n\n\nReturns\n :  \nBugTrackerMigrationIssueHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyBugTrackerMigrationData\n\n\nList<DailyBugTrackerMigrationData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerMigrationData :\n\n\nString\n  bugTrackerId\n\n\nList<String>\n bugsId;\n\n\nint\n numberOfBugs\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nBugTrackerMigrationIssueHistoricMetricProvider :\n\n\nid\n  bugs.dailymigrationissues\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.migrationissuesmaracas\n\n\n\n\nShort name\n: historic.bugs.migrationissuesmaracas\n\n\nFriendly name\n: Migration Issues Detection along with Maracas in Bug Trackers per day per bug tracker\n\n\n\n\nThis metric stores how many migration issues have been found containing changes detected with \nMARACAS\n per day for each bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas\n\n\n\n\n\n\nReturns\n :  \nBugTrackerMigrationIssueMaracasHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyBugTrackerMigrationMaracasData\n\n\nList<DailyBugTrackerMigrationMaracasData>\n\n\n\n\n\n\nbugTrackerMigrationMaracasData\n\n\nList<BugTrackerMigrationMaracasData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerMigrationMaracasData :\n\n\nString\n  bugTrackerId\n\n\nList<String>\n bugsId;\n\n\nint\n numberOfIssues\n\n\n\n\n\n\nBugTrackerMigrationMaracasData :\n\n\nString\n  bugTrackerId\n\n\nString\n bugId;\n\n\nList<String>\n changesAndMatchingPercentage\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nBugTrackerMigrationIssueMaracasHistoricMetricProvider :\n\n\nid\n  bugs.dailymigrationissuesmaracas\n\n\nid\n  bugs.migrationissuesmaracas.changes\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.newbugs\n\n\n\n\nShort name\n: historic.bugs.newbugs\n\n\nFriendly name\n: Number of new bugs per day per bug tracker\n\n\n\n\nThis metric computes the number of new bugs reported by the community (users) per day for each bug tracker. A small number of bug reports can indicate either a bug-free, robust project or a project with a small/inactive user community.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.newbugs\n\n\n\n\n\n\nReturns\n :  \nBugsNewBugsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyBugData\n\n\nList<DailyBugData>\n\n\n\n\n\n\nnumberOfBugs\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfBugs\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfBugs\n\n\nint\n     cumulativeNumberOfBugs\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nNewUsersHistoricMetricProvider :\n\n\nid\n  bugs.cumulativeNewUsers\n\n\nid\n  bugs.newUsers\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.newusers\n\n\n\n\nShort name\n: historic.bugs.newusers\n\n\nFriendly name\n: Number of new users per day per bug tracker\n\n\n\n\nThis metric computes the number of new users per day  for each bug tracker seperately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.activeusers\n\n\n\n\n\n\nReturns\n :  \nBugsNewUsersHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackers\n\n\nList<DailyBugTrackerData>\n\n\n\n\n\n\nnumberOfNewUsers\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfNewUsers\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfNewUsers\n\n\nint\n     cumulativeNumberOfNewUsers\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nNewUsersHistoricMetricProvider :\n\n\nid\n  bugs.cumulativeNewUsers\n\n\nid\n  bugs.newUsers\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.opentime\n\n\n\n\nShort name\n: historic.bugs.opentime\n\n\nFriendly name\n: Average duration to close an open bug\n\n\n\n\nThis metric computes the average duration between creating and closing bugs. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nOpenTimeHistoricMetricProvider\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\navgBugOpenTime\n\n\nString\n\n\n\n\n\n\navgBugOpenTimeInDays\n\n\ndouble\n\n\n\n\n\n\nbugsConsidered\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nOpenTimeHistoricMetricProvider :\n\n\nid\n  bugs.bugOpenTime\n\n\nid\n  bugs.bugOpenTime-bugs\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.patches\n\n\n\n\nShort name\n: historic.bugs.patches\n\n\nFriendly name\n: Number of bug patches per day\n\n\n\n\nThis class computes the number of bug patches per day, for each bug tracker seperately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.patches\n\n\n\n\n\n\nReturns\n :  \nPatchesHistoricMetricProvider\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnumberOfPatches\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfPatches\n\n\nint\n\n\n\n\n\n\nbugs\n\n\nList<DailyBugData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfPatches\n\n\nint\n cumulativeNumberOfPatches\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nPatchesHistoricMetricProvider :\n\n\nid\n  bugs.cumulativePatches\n\n\nid\n  bugs.patches\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.requestsreplies\n\n\n\n\nShort name\n: historic.bugs.requestsreplies\n\n\nFriendly name\n: Number of request and replies in bug comments per bug tracker\n\n\n\n\nThis metric computes the number of requests and replies realting to comments posted to bugs by the community (users) per day  for each bug tracker seperately.\n\n\n\n\n\n\nDepends-on\n :\norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsRequestsRepliesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nBugs\n\n\nList<DailyBugTrackerData>\n\n\n\n\n\n\nnumberOfRequests\n\n\nint\n\n\n\n\n\n\nnumberOfReplies\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfRequests\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfReplies\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nint\n cumulativeNumberOfRequests\n\n\nint\n cumulativeNumberOfReplies\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nRequestsRepliesHistoricMetricProvider :\n\n\nid\n  bugs.replies\n\n\nid\n  bugs.cumulativereplies\n\n\nid\n  bugs.requests\n\n\nid\n  bugs.cumulativerequests\n\n\nid\n  bugs.requestsreplies\n\n\nid\n  bugs.cumulativerequestsreplies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.requestsreplies.average\n\n\n\n\nShort name\n: historic.bugs.requestsreplies.average\n\n\nFriendly name\n: Average number of requests and replies in bug comments per bug tracker\n\n\n\n\nThis metric computes the average number of bug comments considered as request and reply for each bug tracker per day.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.activeusers\n\n\n\n\n\n\nReturns\n :  \nBugsRequestsRepliesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugs\n\n\nList<DailyBugTrackerData>\n\n\n\n\n\n\nnumberOfRequests\n\n\nint\n\n\n\n\n\n\nnumberOfReplies\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfRequests\n\n\nint\n\n\n\n\n\n\ncumulativeNumberOfReplies\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nint\n cumulativeNumberOfRequests\n\n\nint\n cumulativeNumberOfReplies\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nRequestsRepliesHistoricMetricProvider :\n\n\nid\n  bugs.requests-averageperday\n\n\nid\n  bugs.requestsreplies-averageperday\n\n\nid\n  bugs.comments-averageperday\n\n\nid\n  bugs.replies-averageperday\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.responsetime\n\n\n\n\nShort name\n: historic.bugs.responsetime\n\n\nFriendly name\n: Average response time to open bugs per bug tracker\n\n\n\n\nThis metric computes the average time in which the community (users) responds to open bugs per day for each bug tracker seperately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.requestsreplies\n\n\n\n\n\n\nReturns\n :  \nBugsResponseTimeHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerId\n\n\nString\n\n\n\n\n\n\navgResponseTimeFormatted\n\n\nString\n\n\n\n\n\n\ncumulativeAvgResponseTimeFormatted\n\n\nString\n\n\n\n\n\n\navgResponseTime\n\n\nfloat\n\n\n\n\n\n\ncumulativeAvgResponseTime\n\n\nfloat\n\n\n\n\n\n\nbugsConsidered\n\n\nint\n\n\n\n\n\n\ncumulativeBugsConsidered\n\n\nint\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nResponseTimeHistoricMetricProvider :\n\n\nid\n  bugs.averageResponseTime\n\n\nid\n  bugs.cumulativeAverageResponseTime\n\n\nid\n  bugs.cumulativeAverageResponseTime-bugs\n\n\nid\n  bugs.averageResponseTime-bugs\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.sentiment\n\n\n\n\nShort name\n: historic.bugs.sentiment\n\n\nFriendly name\n: Overall sentiment per bug tracker\n\n\n\n\nThis metric computes the overall sentiment per bug tracker up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score for each bug contributes equally, regardless of it's size.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsSentimentHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\noverallAverageSentiment\n\n\nfloat\n\n\n\n\n\n\noverallSentimentAtThreadBeggining\n\n\nfloat\n\n\n\n\n\n\noverallSentimentAtThreadEnd\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\n\n\nSentimentHistoricMetricProvider :\n\n\n\n\nid\n  bugs.averageSentiment\n\n\nid\n  bugs.sentimentAtThreadEnd\n\n\nid\n  bugs.sentimentAtThreadBeggining\n\n\nid\n  bugs.sentiment\n\n\n\n\n\n\n\n\nThe sentiment related variables above all represent a \nPolarity\n value. A polarity value closer to: \n-1\n indicates negative sentiment, closer to \n0\n indicates neutral sentiment and closer to \n1\n indicates positive sentiment.\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.severity\n\n\n\n\nShort name\n: historic.bugs.severity\n\n\nFriendly name\n: Number of bugs per severity level per bug tracker\n\n\n\n\nThis metric computes the number of severity levels for bugs submitted by the community (users) every day for each bug tracker. Specifically, it calculates the number and percentage of bugs that have been categorised into 1 of 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered \nunknown\n if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.  \n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n\n\n\n\n\n\nReturns\n :  \nBugsSeveritiesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugData\n\n\nList<BugData>\n\n\n\n\n\n\nseverityLevels\n\n\nList<ServerityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfBugs\n\n\n\n\n\n\nSeverityLevel :\n\n\nString\n  bugTrackerId\n\n\nString\n  severityLevel   (\nblocker\n,\ncritical\n,\nmajor\n,\nminor\n,\nenhancement\n, \nnormal\n, \ntrivial\n, \nunknown\n)\n\n\nint\n numberOfBugs\n\n\nint\n percentage\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nSeverityHistoricMetricProvider :\n\n\nid\n  bugs.severity\n\n\nid\n  bugs.severity.percentages\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.severitybugstatus\n\n\n\n\nShort name\n: historic.bugs.severitybugstatus\n\n\nFriendly name\n: Number of each bug status per bug severity level\n\n\n\n\nThis metric computes the total number and percentage of each bug status per severity level, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate) and 8 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered \nunknown\n if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n, \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsSeverityBugStatusHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nseverityLevels\n\n\nList<SeverityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nSeverityLevel :\n\n\nString\n  severityLevel   (\nblocker\n,\ncritical\n,\nmajor\n,\nminor\n,\nenhancement\n, \nnormal\n, \ntrivial\n, \nunknown\n)\n\n\nint\n numberOfBugs\n\n\nint\n numberOfWontFixBugs\n\n\nint\n numberOfWorksForMeBugs\n\n\nint\n numberOfNonResolvedClosedBugs\n\n\nint\n numberOfInvalidBugs\n\n\nint\n numberOfFixedBugs\n\n\nint\n numberOfDuplicateBugs\n\n\nfloat\n   percentageOfResolvedClosedBugs\n\n\nfloat\n   percentageOfWontFixBugs\n\n\nfloat\n   percentageOfWorksForMeBugs\n\n\nfloat\n   percentageOfNonResolvedClosedBugs\n\n\nfloat\n   percentageOfInvalidBugs\n\n\nfloat\n   percentageOfFixedBugs\n\n\nfloat\n   percentageOfDuplicateBugs\n\n\n\n\n\n\n\n\nVisualisation Output\n :\n\n\n\n\nSeverityBugStatusHistoricMetricProvider :\n\n\nid\n  bugs.severity.duplicateBugs\n\n\nid\n  bugs.severity.duplicateBugs.percentages\n\n\nid\n  bugs.severity.fixedBugs\n\n\nid\n  bugs.severity.fixedBugs.percentages\n\n\nid\n  bugs.severity.invalidBugs\n\n\nid\n  bugs.severity.invalidBugs.percentages\n\n\nid\n  bugs.severity.nonResolvedClosedBugs\n\n\nid\n  bugs.severity.nonResolvedClosedBugs.percentages\n\n\nid\n  bugs.severity.resolvedClosedBugs\n\n\nid\n  bugs.severity.resolvedClosedBugs.percentages\n\n\nid\n  bugs.severity.wontFixBugs\n\n\nid\n  bugs.severity.wontFixBugs.percentages\n\n\nid\n  bugs.severity.worksForMeBugs\n\n\nid\n  bugs.severity.worksForMeBugs.percentages\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.severityresponsetime\n\n\n\n\nShort name\n: historic.bugs.severityresponsetime\n\n\nFriendly name\n: Average response time to bugs per severity level per day\n\n\n\n\nThis metric computes the average time in which the community (users) responds to open bugs per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 8 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered \nunknown\n if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n, \norg.eclipse.scava.metricprovider.trans.bugs.requestsreplies\n\n\n\n\n\n\nReturns\n :  \nBugsSeverityBugStatusHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nseverityLevels\n\n\nList<SeverityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nSeverityLevel :\n\n\nString\n  severityLevel   (\nblocker\n,\ncritical\n,\nmajor\n,\nminor\n,\nenhancement\n, \nnormal\n, \ntrivial\n, \nunknown\n)\n\n\nString\n  avgResponseTimeFormatted\n\n\nint\n numberOfBugs\n\n\nlong\n    avgResponseTime\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nSeverityResponseTimeHistoricMetricProvider :\n\n\nid\n  bugs.severity.averageResponseTime\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.severitysentiment\n\n\n\n\nShort name\n: historic.bugs.severitysentiment\n\n\nFriendly name\n: Average sentiment per bugs severity level per day\n\n\n\n\nThis metric computes for each bug severity level, the average sentiment, sentiment at the begining and end of bug comments posted by the community (users) every day for each bug tracker. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered \nunknown\n if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n, \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsSeverityBugStatusHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nseverityLevels\n\n\nList<SeverityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nSeverityLevel :\n\n\nString\n  severityLevel   (\nblocker\n,\ncritical\n,\nmajor\n,\nminor\n,\nenhancement\n, \nnormal\n, \ntrivial\n, \nunknown\n)\n\n\nint\n numberOfBugs\n\n\nfloat\n   averageSentiment\n\n\nfloat\n   sentimentAtThreadBeggining\n\n\nfloat\n   sentimentAtThreadEnd\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\n\n\nSeveritySentimentHistoricMetricProvider :\n\n\n\n\nid\n  bugs.severity.sentiment\n\n\nid\n  bugs.severity.averageSentiment\n\n\nid\n  bugs.severity.sentimentAtThreadBeggining\n\n\nid\n  bugs.severity.sentimentAtThreadEnd\n\n\n\n\n\n\n\n\nThe sentiment related variables above all represent a \nPolarity\n value. A polarity value closer to: \n-1\n indicates negative sentiment, closer to \n0\n indicates neutral sentiment and closer to \n1\n indicates positive sentiment.\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.status\n\n\n\n\nShort name\n: historic.bugs.status\n\n\nFriendly name\n: Number of bugs per bug status per day\n\n\n\n\nThis metric computes the total number of bugs that corresponds to each bug status, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsStatusHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnumberOfBugs\n\n\nlong\n\n\n\n\n\n\nnumberOfResolvedClosedBugs\n\n\nint\n\n\n\n\n\n\nnumberOfWontFixBugs\n\n\nint\n\n\n\n\n\n\nnumberOfWorksForMeBugs\n\n\nint\n\n\n\n\n\n\nnumberOfNonResolvedClosedBugs\n\n\nint\n\n\n\n\n\n\nnumberOfInvalidBugs\n\n\nint\n\n\n\n\n\n\nnumberOfFixedBugs\n\n\nint\n\n\n\n\n\n\nnumberOfDuplicateBugs\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nStatusHistoricMetricProvider :\n\n\nid\n  bugs.duplicateBugs\n\n\nid\n  bugs.fixedBugs\n\n\nid\n  bugs.invalidBugs\n\n\nid\n  bugs.nonResolvedClosedBugs\n\n\nid\n  bugs.wontFixBugs\n\n\nid\n  bugs.worksForMeBugs\n\n\nid\n  bugs.resolvedClosedBugs\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.topics\n\n\n\n\nShort name\n: historic.bugs.topics\n\n\nFriendly name\n: Labels of topic clusters in bug comments per bug tracker\n\n\n\n\nThis metric computes the labels of topic clusters extracted from bug comments submitted by the community (users), per bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.topics\n\n\n\n\n\n\nReturns\n :  \nBugsTopicsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTopics\n\n\nList<BugTopic>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nSeverityLevel :\n\n\nString\n  bugTrackerId\n\n\nList<String>\n    labels\n\n\nfloat\n   numberOfDocuments\n\n\nList<String>\n commentsId\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.unansweredbugs\n\n\n\n\nShort name\n: historic.bugs.unansweredbugs\n\n\nFriendly name\n: Number of unanswered bugs per day\n\n\n\n\nThis metric computes the number of unanswered bugs per day.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.requestsreplies\n\n\n\n\n\n\nReturns\n :  \nBugsUnansweredBugsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnumberOfUnansweredBugs\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nUnansweredThreadsHistoricMetricProvider :\n\n\nid\n  bugs.unansweredBugs\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.bugs.users\n\n\n\n\nShort name\n: historic.bugs.users\n\n\nFriendly name\n: Number of users, active and inactive per day per bug tracker\n\n\n\n\nThis metric computes the number of users, number of active and inactive users per day for each bug tracker separately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.activeusers\n\n\n\n\n\n\nReturns\n :  \nBugsUsersHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackers\n\n\nList<DailyBugTrackingData>\n\n\n\n\n\n\nnumberOfUsers\n\n\nint\n\n\n\n\n\n\nnumberOfActiveUsers\n\n\nint\n\n\n\n\n\n\nnumberOfInactiveUsers\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackingData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfUsers\n\n\nint\n numberOfActiveUsers\n\n\nint\n numberOfInactiveUsers\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nUsersHistoricMetricProvider :\n\n\nid\n  bugs.users\n\n\nid\n  bugs.activeusers\n\n\nid\n  bugs.inactiveusers\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Newsgroups and Forums\n\n\nThe following Historic Metric Providers are associated with newsgroups.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.articles\n\n\n\n\nShort name\n: historic.newsgroups.articles\n\n\nFriendly name\n: Number of articles per day per news group\n\n\n\n\nThis metric computes the number of articles submitted by the community (users) per day for each newsgroup separately\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.articles\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsArticlesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfArticles\n\n\nint\n cummulativeNumberOfArticles\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nArticlesHistoricMetricProvider :\n\n\nid\n  newsgroups.articles\n\n\nid\n  newsgroups.cumulativeArticles\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.emotions\n\n\n\n\nShort name\n: historic.newsgroups.emotions\n\n\nFriendly name\n: Number of emotions per day per newsgroup\n\n\n\n\nThis metric computes the emotional dimensions present in newsgroup comments submitted by the community (users) per day for each newsgroup. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.emotions\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsEmotionsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupsData\n\n\nList<Newsgroups>\n\n\n\n\n\n\nemotionDimension\n\n\nList<Emotion>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nNewsgroupsData :\n\n\n\n\nString\n  newsgroupName\n\n\nint\n numberOfArticles\n\n\nint\n cummulativeNumberOfArticles\n\n\n\n\n\n\n\n\nEmotionDimension :\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  emotionLabel (\nanger\n, \nfear\n, \njoy\n, \nsadness\n, \nlove\n, \nsurprise\n)\n\n\nint\n numberOfArticles\n\n\nint\n cumulativeNumberOfArticles\n\n\nfloat\n   percentage\n\n\nfloat\n   cumulativePercentage\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nEmotionsHistoricMetricProvider :\n\n\nid\n  newsgroups.emotions.articlePercentages\n\n\nid\n  newsgroups.emotions.cumulativeArticles\n\n\nid\n  newsgroups.emotions.cumulativeArticlePercentages\n\n\nid\n  newsgroups.emotions.articles\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.migrationissues\n\n\n\n\nShort name\n: historic.newsgroups.migrationissues\n\n\nFriendly name\n: Migration Issues Detection in articles per day per newsgroup\n\n\n\n\nThis metric detects migration issues in articles per day for each newgroup.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.migrationissues\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsMigrationIssueHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupsMigrationData\n\n\nList<DailyNewsgroupsMigrationData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyBugTrackerMigrationData :\n\n\nString\n  newsgroupName\n\n\nList<Integer>\n threadsId\n\n\nint\n numberOfIssues\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nNewsgroupsMigrationIssueHistoricMetricProvider :\n\n\nid\n  newsgroups.dailymigrationissues\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.migrationissuesmaracas\n\n\n\n\nShort name\n: historic.newsgroups.migrationissuesmaracas\n\n\nFriendly name\n: Migration Issues Detection along with Maracas in articles per day per newsgroup\n\n\n\n\nThis metric stores how many migration issues have been found containing changes detected with \nMARACAS\n per day for each newsgroup.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas\n\n\n\n\n\n\nReturns\n :  \nNewsgroupMigrationIssueMaracasHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupMigrationMaracasData\n\n\nList<DailyNewsgroupMigrationMaracasData>\n\n\n\n\n\n\nnewsgroupMigrationMaracasData\n\n\nList<NewsgroupMigrationMaracasData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupMigrationMaracasData :\n\n\nString\n  newsgroupName\n\n\nList<Integer>\n threadsId;\n\n\nint\n numberOfIssues\n\n\n\n\n\n\nNewsgroupMigrationMaracasData :\n\n\nString\n  newsgroupName\n\n\nint threadId;\n\n\nList<String>\n changesAndMatchingPercentage\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nNewsgroupMigrationIssueMaracasHistoricMetricProvider :\n\n\nid\n  newsgroups.dailymigrationissuesmaracas\n\n\nid\n  newsgroups.migrationissuesmaracas.changes\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.newthreads\n\n\n\n\nShort name\n: historic.newsgroups.newthreads\n\n\nFriendly name\n: Number of new threads per day per newsgroup\n\n\n\n\nThis metric computes the number of new threads submitted by the community (users) per day for each newsgroup separately\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsNewThreadsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfNewThreads\n\n\nint\n cummulativeNumberOfNewThreads\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nNewThreadsHistoricMetricProvider :\n\n\nid\n  newsgroups.newThreads\n\n\nid\n  newsgroups.cumulativeNewThreads\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.newusers\n\n\n\n\nShort name\n: historic.newsgroups.newusers\n\n\nFriendly name\n: Number of new users per day per newsgroup\n\n\n\n\nThis metric computes the number of new users per day for each newsgroup seperately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.activeusers\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsNewUsersHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfNewUsers\n\n\nint\n cummulativeNumberOfNewUsers\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nNewUsersHistoricMetricProvider:\n\n\nid\n  newsgroups.cumulativeNewUsers\n\n\nid\n  newsgroups.newUsers\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies\n\n\n\n\nShort name\n: historic.newsgroups.requestsreplies\n\n\nFriendly name\n: Number of requests and replies in articles per day\n\n\n\n\nThis metric computes the number of requests and replies in newsgroup articles submitted by the community (users) per day for each newsgroup separately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsRequestsRepliesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nint\n cumulativeNumberOfRequests\n\n\nint\n cumulativeNumberOfReplies\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nRequestsRepliesHistoricMetricProvider :\n\n\nid\n  newsgroups.requests\n\n\nid\n  newsgroups.cumulativerequests\n\n\nid\n  newsgroups.replies\n\n\nid\n  newsgroups.cumulativereplies\n\n\nid\n  newsgroups.requestsreplies\n\n\nid\n  newsgroups.cumulativerequestsreplies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average\n\n\n\n\nShort name\n: historic.newsgroups.requestsreplies.average\n\n\nFriendly name\n: Average number of articles, requests and replies per day\n\n\n\n\nThis metric computes the average number of newsgroup articles, including the number of requests and replies within the newsgroup articles per day.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.activeusers\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsRequestsRepliesAverageHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\naverageArticlesPerDay\n\n\nfloat\n\n\n\n\n\n\naverageRequestsPerDay\n\n\nfloat\n\n\n\n\n\n\naverageRepliesPerDay\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nRequestsRepliesAverageHistoricMetricProvider :\n\n\nid\n  newsgroups.requestsreplies-averageperday\n\n\nid\n  newsgroups.requests-averageperday\n\n\nid\n  newsgroups.replies-averageperday\n\n\nid\n  newsgroups.comments-averageperday\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.responsetime\n\n\n\n\nShort name\n: historic.newsgroups.responsetime\n\n\nFriendly name\n: Average response time to threads per day per newsgroup\n\n\n\n\nThis metric computes the average time in which the community responds to open threads per day for each newsgroup separately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsResponseTimeHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupName\n\n\nString\n\n\n\n\n\n\navgResponseTime\n\n\nlong\n\n\n\n\n\n\navgResponseTimeFormatted\n\n\nString\n\n\n\n\n\n\nthreadsConsidered\n\n\nint\n\n\n\n\n\n\ncumulativeAvgResponseTimeFormatted\n\n\nString\n\n\n\n\n\n\ncumulativeThreadsConsidered\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nResponseTimeHistoricMetricProvider :\n\n\nid\n  newsgroups.averageResponseTime\n\n\nid\n  newsgroups.cumulativeAverageResponseTime\n\n\nid\n  newsgroups.cumulativeAverageResponseTime-threads\n\n\n\n\nid\n  newsgroups.averageResponseTime-threads\n\n\nBack to top\n\n\n\n\n\n\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.sentiment\n\n\n\n\nShort name\n: historic.newsgroups.sentiment\n\n\nFriendly name\n: Overall sentiment of newsgroup articles\n\n\n\n\nThis metric computes the overall sentiment per newsgroup repository up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score of each thread contributes equally, irrespective of its size.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.sentiment\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsSentimentHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\noverallAverageSentiment\n\n\nfloat\n\n\n\n\n\n\noverallSentimentAtThreadBegining\n\n\nfloat\n\n\n\n\n\n\noverallSentimentAtThreadEnd\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\n\n\nSentimentHistoricMetricProvider :\n\n\n\n\nid\n  newsgroups.averageSentiment\n\n\nid\n  newsgroups.sentimentAtThreadEnd\n\n\nid\n  newsgroups.sentimentAtThreadBeggining\n\n\nid\n  newsgroups.sentiment\n\n\n\n\n\n\n\n\nThe sentiment related variables above all represent a \nPolarity\n value. A polarity value closer to: \n-1\n indicates negative sentiment, closer to \n0\n indicates neutral sentiment and closer to \n1\n indicates positive sentiment.\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.severity\n\n\n\n\nShort name\n: historic.newsgroups.severity\n\n\nFriendly name\n: Number of each severity level in newsgroup threads per day\n\n\n\n\nThis metric computes the number of each severity levels in threads submitted every day, per newsgroup. There are 7 severity  levels (blocker, critical, major, minor, enhancement,  normal, trivial).  \n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsSeveritiesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupData\n\n\nList<Newsgroups>\n\n\n\n\n\n\nseverityLevel\n\n\nList<SeverityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nNewsgroupData :\n\n\n\n\nString\n  newsgroupName\n\n\nint\n numberThreads\n\n\n\n\n\n\n\n\nSeverityLevel :\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  severityLabel (\nblocker\n, \ncritical\n, \nmajor\n, \nminor\n, \nenhancement\n,  \nnormal\n, \ntrivial\n)\n\n\nint\n numberOfThreads\n\n\nfloat\n   percentage\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nSeverityHistoricMetricProvider :\n\n\nid\n  newsgroups.severity\n\n\nid\n  newsgroups.severity.percentages\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime\n\n\n\n\nShort name\n: historic.newsgroups.severityresponsetime\n\n\nFriendly name\n: Average response time to threads per severity level per day\n\n\n\n\nThis metric computes the average time in which the community (users) responds to open threads per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 7 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial).\n\n\nAverage response time to threads per severity level\n\n\nThis metric computes the average response time for newsgroup threads submitted every day, based on their severity levels.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n,\n\norg.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsSeverityResponseTimeHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nseverityLevel\n\n\nList<SeverityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nSeverityLevel :\n\n\nString\n  severityLabel (\nblocker\n, \ncritical\n, \nmajor\n, \nminor\n, \nenhancement\n,  \nnormal\n, \ntrivial\n)\n\n\nint\n numberOfThreads\n\n\nlong\n    avgResponseTime\n\n\nString\n  avgResponseTimeFormatted\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nSeverityResponseTimeHistoricMetricProvider :\n\n\nid\n  newsgroups.severity.averageResponseTime\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment\n\n\n\n\nShort name\n: historic.newsgroups.severitysentiment\n\n\nFriendly name\n: Average sentiment in threads per severity level per day\n\n\n\n\nThis metric computes the average sentiment, the sentiment at the beginning of threads and the sentiment at the end of threads; for each severity level in newsgroup threads submitted every day. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). Note: there are 7 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.severityclassification\n,\n\norg.eclipse.scava.metricprovider.trans.newsgroups.sentiment\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsSeveritySentimentHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nseverityLevel\n\n\nList<SeverityLevel>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nSeverityLevel :\n\n\nString\n  severityLabel (\nblocker\n, \ncritical\n, \nmajor\n, \nminor\n, \nenhancement\n,  \nnormal\n, \ntrivial\n)\n\n\nint\n numberOfThreads\n\n\nfloat\n   avgSentiment\n\n\nfloat\n   avgSentimentThreadBeginning\n\n\nfloat\n   avgSentimentThreadEnd\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\n\n\nSeveritySentimentHistoricMetricProvider :\n\n\n\n\nid\n  newsgroups.severity.averageSentiment\n\n\nid\n  newsgroups.severity.sentiment\n\n\nid\n  newsgroups.severity.sentimentAtThreadEnd\n\n\nid\n  newsgroups.severity.sentimentAtThreadBeggining\n\n\n\n\n\n\n\n\nThe sentiment related variables above all represent a \nPolarity\n value. A polarity value closer to: \n-1\n indicates negative sentiment, closer to \n0\n indicates neutral sentiment and closer to \n1\n indicates positive sentiment.\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.threads\n\n\n\n\nShort name\n: historic.newsgroups.threads\n\n\nFriendly name\n: Number of threads per day per newsgroup\n\n\n\n\nThis metric computes the number of threads per day for each newsgroup separately. The metric also computes average values for articles per thread, requests per thread, replies per thread, articles per user, requests per user and replies per user.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n,\n\norg.eclipse.scava.metricprovider.trans.newsgroups.activeusers\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsThreadsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfThreads\n\n\nfloat\n   averageArticlesPerThread\n\n\nfloat\n   averageRequestsPerThread\n\n\nfloat\n   averageRepliesPerThread\n\n\nfloat\n   averageArticlesPerUser\n\n\nfloat\n   averageRequestsPerUser\n\n\nfloat\n   averageRepliesPerUser\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nThreadsHistoricMetricProvider :\n\n\nid\n  newsgroups.threads\n\n\nid\n  newsgroups.articles-threadaverage\n\n\nid\n  newsgroups.articles-useraverage\n\n\nid\n  newsgroups.requests-threadaverage\n\n\nid\n  newsgroups.requests-useraverage\n\n\nid\n  newsgroups.replies-threadaverage\n\n\nid\n  newsgroups.replies-useraverage\n\n\nid\n  newsgroups.requestsreplies-threadaverage\n\n\nid\n  newsgroups.requestsreplies-useraverage\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.topics\n\n\n\n\nShort name\n: historic.newsgroups.topics\n\n\nFriendly name\n: Labels of newsgroup topics per newsgroup\n\n\n\n\nThis metric computes the labels of topics clusters in articles submitted by the community (users), for each newsgroup seperately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.topics\n\n\n\n\n\n\nReturns\n :  \nNewsgroupTopicsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupTopic\n\n\nList<NewsgrpTopic>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nNewsgroupTopic :\n\n\nString\n  newsgroupName\n\n\nList<String>\n    labels\n\n\nint\n numberOfDocuments\n\n\nList<Integer>\n articlesId\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads\n\n\n\n\nShort name\n: historic.newsgroups.unansweredthreads\n\n\nFriendly name\n: Number of unanswered threads per day per newsgroup\n\n\n\n\nThis metric computes the number of unanswered threads per day for each newsgroup separately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsUnansweredThreadsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfUnansweredThreads\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nUnansweredThreadsHistoricMetricProvider :\n\n\nid\n  newsgroups.unansweredThreads\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.newsgroups.users\n\n\n\n\nShort name\n: historic.newsgroups.users\n\n\nFriendly name\n: Number of users, active and inactive per day per newsgroup\n\n\n\n\nThis metric computes the number of users, including active and inactive users per day for each newsgroup separately.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.activeusers\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsUsersHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyNewsgroupData\n\n\nList<DailyNewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfUsers\n\n\nint\n numberOfActiveUsers\n\n\nint\n numberOfInactiveUsers\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nUsersHistoricMetricProvider :\n\n\nid\n  newsgroups.users\n\n\nid\n  newsgroups.activeusers\n\n\nid\n  newsgroups.inactiveusers\n\n\nid\n  newsgroups.activeinactiveusers\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Commits and Committers\n\n\nThe following Historic Metric Providers are related to the commits and committers of a project.\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.committersoverfile.historic\n\n\n\n\nShort name\n: giniCommittersOverFile.historic\n\n\nFriendly name\n: Historic giniCommittersOverFile\n\n\nHistoric version of\n: trans.rascal.activecommitters.committersoverfile\n\n\n\n\nCalculates the gini coefficient of committers per file\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersoverfile\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.percentageOfWeekendCommits.historic\n\n\n\n\nShort name\n: percentageOfWeekendCommits.historic\n\n\nFriendly name\n: Historic percentageOfWeekendCommits\n\n\nHistoric version of\n: trans.rascal.activecommitters.percentageOfWeekendCommits\n\n\n\n\nPercentage of commits made during the weekend\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.percentageOfWeekendCommits\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.commitsPerDeveloper.historic\n\n\n\n\nShort name\n: commitsPerDeveloper.historic\n\n\nFriendly name\n: Historic commitsPerDeveloper\n\n\nHistoric version of\n: trans.rascal.activecommitters.commitsPerDeveloper\n\n\n\n\nThe number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits,\nwhen combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.commitsPerDeveloper\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.numberOfActiveCommittersLongTerm.historic\n\n\n\n\nShort name\n: numberOfActiveCommittersLongTerm.historic\n\n\nFriendly name\n: Historic numberOfActiveCommittersLongTerm\n\n\nHistoric version of\n: trans.rascal.activecommitters.numberOfActiveCommittersLongTerm\n\n\n\n\nNumber of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.numberOfActiveCommittersLongTerm\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.numberOfActiveCommitters.historic\n\n\n\n\nShort name\n: numberOfActiveCommitters.historic\n\n\nFriendly name\n: Historic numberOfActiveCommitters\n\n\nHistoric version of\n: trans.rascal.activecommitters.numberOfActiveCommitters\n\n\n\n\nNumber of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.numberOfActiveCommitters\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.commitsToday.historic\n\n\n\n\nShort name\n: commitsToday.historic\n\n\nFriendly name\n: Historic commitsToday\n\n\nHistoric version of\n: rascal.generic.churn.commitsToday\n\n\n\n\nCounts the number of commits made today.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.commitsToday\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnToday.historic\n\n\n\n\nShort name\n: commitsToday.historic\n\n\nFriendly name\n: Historic commitsToday\n\n\nHistoric version of\n: rascal.generic.churn.churnToday\n\n\n\n\nCounts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnToday\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerCommitInTwoWeeks.historic\n\n\n\n\nShort name\n: churnPerCommitInTwoWeeks.historic\n\n\nFriendly name\n: Historic churnPerCommitInTwoWeeks\n\n\nHistoric version of\n: rascal.generic.churn.churnPerCommitInTwoWeeks\n\n\n\n\nThe ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnPerCommitInTwoWeeks\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.filesPerCommit.historic\n\n\n\n\nShort name\n: numberOfFilesPerCommit.historic\n\n\nFriendly name\n: Historic numberOfFilesPerCommit\n\n\nHistoric version of\n: rascal.generic.churn.filesPerCommit\n\n\n\n\nCounts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.filesPerCommit\n\n\nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerCommit.historic\n\n\n\n\nShort name\n: churnPerCommit.historic\n\n\nFriendly name\n: Historic churnPerCommit\n\n\nHistoric version of\n: rascal.generic.churn.churnPerCommit\n\n\n\n\nCount churn. Churn is the number lines added or deleted. We measure this per commit because the commit\nis a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnPerCommit\n\n\nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerCommitter.historic\n\n\n\n\nShort name\n: churnPerCommitter.historic\n\n\nFriendly name\n: Historic churnPerCommitter\n\n\nHistoric version of\n: rascal.generic.churn.churnPerCommitter\n\n\n\n\nCount churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnPerCommitter\n\n\nReturns\n: \nmap[str author, int churn]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.commitsInTwoWeeks.historic\n\n\n\n\nShort name\n: commitsInTwoWeeks.historic\n\n\nFriendly name\n: Historic commitsInTwoWeeks\n\n\nHistoric version of\n: rascal.generic.churn.commitsInTwoWeeks\n\n\n\n\nChurn in the last two weeks: aggregates the number of commits over a 14-day sliding window.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.commitsInTwoWeeks\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnInTwoWeeks.historic\n\n\n\n\nShort name\n: churnInTwoWeeks.historic\n\n\nFriendly name\n: Historic churnInTwoWeeks\n\n\nHistoric version of\n: rascal.generic.churn.churnInTwoWeeks\n\n\n\n\nChurn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnInTwoWeeks\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.commits.messages.topics\n\n\n\n\nShort name\n: historic.commits.messages.topics\n\n\nFriendly name\n: Labels of topics in commits messages analyzed in the last 30 days\n\n\n\n\nThis metric computes the labels of topic clusters in commits messages pushed by users in the last 30 days\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.commits.message.topics\n\n\n\n\n\n\nReturns\n :  \nCommitsMessagesTopicsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncommitMessageTopics\n\n\nList<CommitMessageTopic>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nCommitMessageTopic :\n\n\nString\n  repository\n\n\nString\n  labels\n\n\nint\n numberOfMessages\n\n\nList<String>\n    commitsMessageId\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nCommitsMessagesTopicsHistoricMetricProvider :\n\n\nid\n  commits.topics.messages\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Documentation\n\n\nThe following Historic Metric Providers are associated with documentation analyses.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.documentation.readability\n\n\n\n\nShort name\n: historic.documentation.readability\n\n\nFriendly name\n: Documentation readability Historic Metric\n\n\n\n\nHistoric metric that stores the evolution of the documentation readability. The higher the readability score, the harder to understand the text. \n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.documentation.readability\n\n\n\n\n\n\nReturns\n :  \nDocumentationReadabilityHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationReadability\n\n\nList<DocumentationHistoricReadability>\n\n\n\n\n\n\ndocumentationEntriesReadability\n\n\nList<DocumentationEntryHistoricReadability>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nDocumentationHistoricReadability :\n\n\n\n\nString\n  documentationId\n\n\nint\n numberOfDocumentationEntries\n\n\ndouble\n  averageDocumentationReadability\n\n\n\n\n\n\n\n\nDocumentationEntryHistoricReadability :\n\n\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\ndouble\n  readability\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nreadability :\n\n\nid\n  documentation.readability.entries\n\n\nid\n  documentation.readability\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.documentation.sentiment\n\n\n\n\nShort name\n: historic.documentation.sentiment\n\n\nFriendly name\n: Documentation sentiment polarity Historic Metric\n\n\n\n\nHistoric metric that stores the evolution of the documentation sentiment polarity. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.documentation.sentiment\n\n\n\n\n\n\nReturns\n :  \nDocumentationSentimentHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationSentiment\n\n\nList<DocumentationHistoricSentiment>\n\n\n\n\n\n\ndocumentationEntriesSentiment\n\n\nList<DocumentationEntryHistoricSentiment>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nDocumentationHistoricSentiment :\n\n\n\n\nString\n  documentationId\n\n\nint\n numberOfDocumentationEntries\n\n\ndouble\n  averageDocumentationSentiment\n\n\n\n\n\n\n\n\nDocumentationEntryHistoricSentiment :\n\n\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\nString\n  polarity\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nsentiment :\n\n\nid\n  documentation.sentiment.entries\n\n\nid\n  documentation.sentiment\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Generic Source Code\n\n\nThese metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in.\n\n\nBack to top\n\n\n\n\ntrans.rascal.clones.cloneLOCPerLanguage.historic\n\n\n\n\nShort name\n: cloneLOCPerLanguage.historic\n\n\nFriendly name\n: Historic cloneLOCPerLanguage\n\n\nHistoric version of\n: trans.rascal.clones.cloneLOCPerLanguage\n\n\n\n\nLines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric.\n\n\n\n\nDepends-on\n: \ntrans.rascal.clones.cloneLOCPerLanguage\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.readability.fileReadabilityQuartiles.historic\n\n\n\n\nShort name\n: fileReadabilityQ.historic\n\n\nFriendly name\n: Historic fileReadabilityQ\n\n\nHistoric version of\n: trans.rascal.readability.fileReadabilityQuartiles\n\n\n\n\nWe measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles\nrepresent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a\nlack of attention to readability.\n\n\n\n\nDepends-on\n: \ntrans.rascal.readability.fileReadabilityQuartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.commentLinesPerLanguage.historic\n\n\n\n\nShort name\n: commentLinesPerLanguage.historic\n\n\nFriendly name\n: Historic commentLinesPerLanguage\n\n\nHistoric version of\n: trans.rascal.comments.commentLinesPerLanguage\n\n\n\n\nNumber of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream.\n\n\n\n\nDepends-on\n: \ntrans.rascal.comments.commentLinesPerLanguage\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.commentedOutCodePerLanguage.historic\n\n\n\n\nShort name\n: commentedOutCodePerLanguage.historic\n\n\nFriendly name\n: Historic commentedOutCodePerLanguage\n\n\nHistoric version of\n: trans.rascal.comments.commentedOutCodePerLanguage\n\n\n\n\nLines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how\nmuch source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator.\n\n\n\n\nDepends-on\n: \ntrans.rascal.comments.commentedOutCodePerLanguage\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.headerPercentage.historic\n\n\n\n\nShort name\n: headerPercentage.historic\n\n\nFriendly name\n: Historic headerPercentage\n\n\nHistoric version of\n: trans.rascal.comments.headerPercentage\n\n\n\n\nPercentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling.\n\n\n\n\nDepends-on\n: \ntrans.rascal.comments.headerPercentage\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.LOC.genericLOCoverFiles.historic\n\n\n\n\nShort name\n: giniLOCOverFiles.historic\n\n\nFriendly name\n: Historic giniLOCOverFiles\n\n\nHistoric version of\n: trans.rascal.LOC.genericLOCoverFiles\n\n\n\n\nWe find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality.\n\n\n\n\nDepends-on\n: \ntrans.rascal.LOC.genericLOCoverFiles\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.LOC.locPerLanguage.historic\n\n\n\n\nShort name\n: locPerLanguage.historic\n\n\nFriendly name\n: Historic locPerLanguage\n\n\nHistoric version of\n: trans.rascal.LOC.locPerLanguage\n\n\n\n\nPhysical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language.\nThe metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written.\n\n\n\n\nDepends-on\n: \ntrans.rascal.LOC.locPerLanguage\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Java code\n\n\nThese metrics are related to the Java source code of analyzed projects.\n\n\nBack to top\n\n\n\n\nstyle.filesWithErrorProneness.historic\n\n\n\n\nShort name\n: filesWithErrorProneness.historic\n\n\nFriendly name\n: Historic filesWithErrorProneness\n\n\nHistoric version of\n: style.filesWithErrorProneness\n\n\n\n\nPercentage of files with error proneness\n\n\n\n\nDepends-on\n: \nstyle.filesWithErrorProneness\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.filesWithUnderstandabilityIssues.historic\n\n\n\n\nShort name\n: filesWithUnderstandabilityIssues.historic\n\n\nFriendly name\n: Historic filesWithUnderstandabilityIssues\n\n\nHistoric version of\n: style.filesWithUnderstandabilityIssues\n\n\n\n\nPercentage of files with understandability issues. This is a basic metric which can not be easily compared between projects.\n\n\n\n\nDepends-on\n: \nstyle.filesWithUnderstandabilityIssues\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfStyleViolations.historic\n\n\n\n\nShort name\n: spreadOfStyleViolations.historic\n\n\nFriendly name\n: Historic spreadOfStyleViolations\n\n\nHistoric version of\n: style.spreadOfStyleViolations\n\n\n\n\nBetween 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.spreadOfStyleViolations\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nstyle.filesWithInefficiencies.historic\n\n\n\n\nShort name\n: filesWithInefficiencies.historic\n\n\nFriendly name\n: Historic filesWithInefficiencies\n\n\nHistoric version of\n: style.filesWithInefficiencies\n\n\n\n\nPercentage of files with inefficiencies\n\n\n\n\nDepends-on\n: \nstyle.filesWithInefficiencies\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.filesWithStyleViolations.historic\n\n\n\n\nShort name\n: filesWithStyleViolations.historic\n\n\nFriendly name\n: Historic filesWithStyleViolations\n\n\nHistoric version of\n: style.filesWithStyleViolations\n\n\n\n\nPercentage of files with style violations\n\n\n\n\nDepends-on\n: \nstyle.filesWithStyleViolations\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfUnderstandabilityIssues.historic\n\n\n\n\nShort name\n: spreadOfUnderstandabilityIssues.historic\n\n\nFriendly name\n: Historic spreadOfUnderstandabilityIssues\n\n\nHistoric version of\n: style.spreadOfUnderstandabilityIssues\n\n\n\n\nBetween 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.spreadOfUnderstandabilityIssues\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfInefficiencies.historic\n\n\n\n\nShort name\n: spreadOfInefficiencies.historic\n\n\nFriendly name\n: Historic spreadOfInefficiencies\n\n\nHistoric version of\n: style.spreadOfInefficiencies\n\n\n\n\nBetween 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.spreadOfInefficiencies\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfErrorProneness.historic\n\n\n\n\nShort name\n: spreadOfErrorProneness.historic\n\n\nFriendly name\n: Historic spreadOfErrorProneness\n\n\nHistoric version of\n: style.spreadOfErrorProneness\n\n\n\n\nBetween 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.spreadOfErrorProneness\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nrascal.testability.java.TestOverPublicMethods.historic\n\n\n\n\nShort name\n: percentageOfTestedPublicMethods.historic\n\n\nFriendly name\n: Historic percentageOfTestedPublicMethods\n\n\nHistoric version of\n: rascal.testability.java.TestOverPublicMethods\n\n\n\n\nNumber of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we\ncompute how far from the ideal situation the project is.\n\n\n\n\nDepends-on\n: \nrascal.testability.java.TestOverPublicMethods\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nrascal.testability.java.NumberOfTestMethods.historic\n\n\n\n\nShort name\n: numberOfTestMethods.historic\n\n\nFriendly name\n: Historic numberOfTestMethods\n\n\nHistoric version of\n: rascal.testability.java.NumberOfTestMethods\n\n\n\n\nNumber of JUnit test methods\n\n\n\n\nDepends-on\n: \nrascal.testability.java.NumberOfTestMethods\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.testability.java.TestCoverage.historic\n\n\n\n\nShort name\n: estimateTestCoverage.historic\n\n\nFriendly name\n: Historic estimateTestCoverage\n\n\nHistoric version of\n: rascal.testability.java.TestCoverage\n\n\n\n\nThis is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate\nthis by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation,\nas compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator\nfor a lack in testing effort for the project.\n\n\n\n\nDepends-on\n: \nrascal.testability.java.TestCoverage\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.Ca-Java-Quartiles.historic\n\n\n\n\nShort name\n: Ca_Java_Q.historic\n\n\nFriendly name\n: Historic Ca_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.Ca-Java-Quartiles\n\n\n\n\nAfferent coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.Ca-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.CF-Java.historic\n\n\n\n\nShort name\n: CF_Java.historic\n\n\nFriendly name\n: Historic CF_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.CF-Java\n\n\n\n\nCoupling factor (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.CF-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.DAC-Java-Quartiles.historic\n\n\n\n\nShort name\n: DAC_Java_Q.historic\n\n\nFriendly name\n: Historic DAC_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.DAC-Java-Quartiles\n\n\n\n\nData abstraction coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.DAC-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MPC-Java-Quartiles.historic\n\n\n\n\nShort name\n: MPC_Java_Q.historic\n\n\nFriendly name\n: Historic MPC_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.MPC-Java-Quartiles\n\n\n\n\nMessage passing coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.MPC-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.PF-Java.historic\n\n\n\n\nShort name\n: PF_Java.historic\n\n\nFriendly name\n: Historic PF_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.PF-Java\n\n\n\n\nPolymorphism factor (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.PF-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.RFC-Java-Quartiles.historic\n\n\n\n\nShort name\n: RFC_Java_Q.historic\n\n\nFriendly name\n: Historic RFC_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.RFC-Java-Quartiles\n\n\n\n\nResponse for class quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.RFC-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.I-Java-Quartiles.historic\n\n\n\n\nShort name\n: I_Java_Q.historic\n\n\nFriendly name\n: Historic I_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.I-Java-Quartiles\n\n\n\n\nInstability quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.I-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MIF-Java-Quartiles.historic\n\n\n\n\nShort name\n: MIF_Java_Q.historic\n\n\nFriendly name\n: Historic MIF_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.MIF-Java-Quartiles\n\n\n\n\nMethod inheritance factor quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.MIF-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MHF-Java.historic\n\n\n\n\nShort name\n: MHF_Java.historic\n\n\nFriendly name\n: Historic MHF_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.MHF-Java\n\n\n\n\nMethod hiding factor (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.MHF-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.AHF-Java.historic\n\n\n\n\nShort name\n: AHF_Java.historic\n\n\nFriendly name\n: Historic AHF_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.AHF-Java\n\n\n\n\nAttribute hiding factor (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.AHF-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCOM-Java-Quartiles.historic\n\n\n\n\nShort name\n: LCOM_Java_Q.historic\n\n\nFriendly name\n: Historic LCOM_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.LCOM-Java-Quartiles\n\n\n\n\nLack of cohesion in methods quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.LCOM-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.A-Java.historic\n\n\n\n\nShort name\n: A_Java.historic\n\n\nFriendly name\n: Historic A_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.A-Java\n\n\n\n\nAbstractness (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.A-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.DIT-Java-Quartiles.historic\n\n\n\n\nShort name\n: DIT_Java_Q.historic\n\n\nFriendly name\n: Historic DIT_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.DIT-Java-Quartiles\n\n\n\n\nDepth of inheritance tree quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.DIT-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.TCC-Java-Quartiles.historic\n\n\n\n\nShort name\n: TCC_Java_Q.historic\n\n\nFriendly name\n: Historic TCC_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.TCC-Java-Quartiles\n\n\n\n\nTight class cohesion quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.TCC-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCOM4-Java-Quartiles.historic\n\n\n\n\nShort name\n: LCOM4_Java_Q.historic\n\n\nFriendly name\n: Historic LCOM4_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.LCOM4-Java-Quartiles\n\n\n\n\nLack of cohesion in methods 4 quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.LCOM4-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.SR-Java.historic\n\n\n\n\nShort name\n: SR_Java.historic\n\n\nFriendly name\n: Historic SR_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.SR-Java\n\n\n\n\nSpecialization ratio (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.SR-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.AIF-Java-Quartiles.historic\n\n\n\n\nShort name\n: AIF_Java_Q.historic\n\n\nFriendly name\n: Historic AIF_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.AIF-Java-Quartiles\n\n\n\n\nAttribute inheritance factor quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.AIF-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOC-Java-Quartiles.historic\n\n\n\n\nShort name\n: NOC_Java_Q.historic\n\n\nFriendly name\n: Historic NOC_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.NOC-Java-Quartiles\n\n\n\n\nNumber of children quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.NOC-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.RR-Java.historic\n\n\n\n\nShort name\n: RR_Java.historic\n\n\nFriendly name\n: Historic RR_Java\n\n\nHistoric version of\n: trans.rascal.OO.java.RR-Java\n\n\n\n\nReuse ratio (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.RR-Java\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCC-Java-Quartiles.historic\n\n\n\n\nShort name\n: LCC_Java_Q.historic\n\n\nFriendly name\n: Historic LCC_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.LCC-Java-Quartiles\n\n\n\n\nLoose class cohesion quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.LCC-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.Ce-Java-Quartiles.historic\n\n\n\n\nShort name\n: Ce_Java_Q.historic\n\n\nFriendly name\n: Historic Ce_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.Ce-Java-Quartiles\n\n\n\n\nEfferent coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.Ce-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOM-Java-Quartiles.historic\n\n\n\n\nShort name\n: NOM_Java_Q.historic\n\n\nFriendly name\n: Historic NOM_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.NOM-Java-Quartiles\n\n\n\n\nNumber of methods quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.NOM-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOA-Java-Quartiles.historic\n\n\n\n\nShort name\n: NOA_Java_Q.historic\n\n\nFriendly name\n: Historic NOA_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.NOA-Java-Quartiles\n\n\n\n\nNumber of attributes quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.NOA-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.CBO-Java-Quartiles.historic\n\n\n\n\nShort name\n: CBO_Java_Q.historic\n\n\nFriendly name\n: Historic CBO_Java_Q\n\n\nHistoric version of\n: trans.rascal.OO.java.CBO-Java-Quartiles\n\n\n\n\nCoupling between objects quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.CBO-Java-Quartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles.historic\n\n\n\n\nShort name\n: countUsesOfAdvancedLanguageFeaturesQ.historic\n\n\nFriendly name\n: Historic countUsesOfAdvancedLanguageFeaturesQ\n\n\nHistoric version of\n: trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles\n\n\n\n\nQuartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values.\n\n\n\n\nDepends-on\n: \ntrans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.CC.java.CCHistogramJava.historic\n\n\n\n\nShort name\n: CCHistogramJava.historic\n\n\nFriendly name\n: Historic CCHistogramJava\n\n\nHistoric version of\n: trans.rascal.CC.java.CCHistogramJava\n\n\n\n\nNumber of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis.\n\n\n\n\nDepends-on\n: \ntrans.rascal.CC.java.CCHistogramJava\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.CC.java.CCOverJavaMethods.historic\n\n\n\n\nShort name\n: giniCCOverMethodsJava.historic\n\n\nFriendly name\n: Historic giniCCOverMethodsJava\n\n\nHistoric version of\n: trans.rascal.CC.java.CCOverJavaMethods\n\n\n\n\nCalculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects.\n\n\n\n\nDepends-on\n: \ntrans.rascal.CC.java.CCOverJavaMethods\n\n\nReturns\n: \nreal\n\n\n\n\nHistoric Metric Providers for OSGi Dependencies\n\n\nThese metrics are related to OSGi dependencies declared in \nMANIFEST.MF\n files.\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.numberOSGiBundleDependencies.historic\n\n\n\n\nShort name\n: numberOSGiBundleDependencies.historic\n\n\nFriendly name\n: Historic numberOSGiBundleDependencies\n\n\nHistoric version of\n: trans.rascal.dependency.osgi.numberOSGiBundleDependencies\n\n\n\n\nRetrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.osgi.numberOSGiBundleDependencies\n\n\nReturns\n: \nint\n\n\n\n\nHistoric Metric Providers for Maven Dependencies\n\n\nThese metrics are related to Maven dependencies declared in \npom.xml\n files.\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.numberMavenDependencies.historic\n\n\n\n\nShort name\n: numberMavenDependencies.historic\n\n\nFriendly name\n: Historic numberMavenDependencies\n\n\nHistoric version of\n: trans.rascal.dependency.maven.numberMavenDependencies\n\n\n\n\nRetrieves the number of Maven dependencies.\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.maven.numberMavenDependencies\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Docker Dependencies\n\n\nThe following Historic Metric Provider is associated with Docker Dependencies\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.configuration.docker.dependencies\n\n\n\n\nShort name\n: historic.configuration.docker.dependencies\n\n\nFriendly name\n: Number of dependencies defined in Dockerfiles per day\n\n\n\n\nThis metric computes the number of the dependencies that are defined in the Dockerfiles of a project per day. It also computes additional information such as the number of each version of the dependencies (image/package).\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.docker.dependencies\n\n\n\n\nReturns\n :  \nDockerDependenciesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnumberOfDockerDependencies\n\n\nint\n\n\n\n\n\n\nnumberOfDockerPackageDependencies\n\n\nint\n\n\n\n\n\n\nnumberOfDockerImageDependencies\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nDockerDependenciesHistoricMetric :\n\n\nid\n  docker.dependencies\n\n\nid\n  docker.packageDependencies\n\n\nid\n  docker.imageDependencies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Puppet Dependencies\n\n\nThe following Historic Metric Provider is associated with Puppet Dependencies\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.configuration.puppet.dependencies\n\n\n\n\nShort name\n: historic.configuration.puppet.dependencies\n\n\nFriendly name\n: Number of dependencies defined in Puppet manifests per day\n\n\n\n\nThis metric computes the number of the dependencies that are defined in the Puppet manifests of a project per day.\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies\n\n\n\n\nReturns\n :  \nPuppetDependenciesHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnumberOfPuppetDependencies\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nDockerDependenciesHistoricMetric :\n\n\nid\n  puppet.dependencies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Docker Smells\n\n\nThe following Historic Metric Provider is associated with Docker Smells\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.configuration.docker.smells\n\n\n\n\nShort name\n: historic.configuration.docker.smells\n\n\nFriendly name\n: Number of smells detected in Dockerfiles per day\n\n\n\n\nThis metric computes the number of the smells that are detected in the Dockerfiles of a project per day. It also computes additional information such as the number of each type of the smell.\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.docker.smells\n\n\n\n\nReturns\n :  \nDockerSmellsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncumulativeNumberOfDockerSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperUpgradeSmells\n\n\nint\n\n\n\n\n\n\nnumberOfUnknownPackageVersionSmells\n\n\nint\n\n\n\n\n\n\nnumberOfUntaggedImageSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperSudoSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperCopySmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperFromSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperCmdSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMeaninglessSmells\n\n\nint\n\n\n\n\n\n\nnumberOfInvalidPortsSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperShellSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperEntrypointSmells\n\n\nint\n\n\n\n\n\n\nnumberOfDeprecatedInstructionSmells\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nDockerDependenciesHistoricMetric :\n\n\nid\n  docker.smells\n\n\nid\n  docker.smells.improperUpgradeSmells\n\n\nid\n  docker.smells.unknownPackageVersionSmells\n\n\nid\n  docker.smells.untaggedImageSmells\n\n\nid\n  docker.smells.improperSudoSmells\n\n\nid\n  docker.smells.improperCopySmells\n\n\nid\n  docker.smells.improperFromSmells\n\n\nid\n  docker.smells.improperCmdSmells\n\n\nid\n  docker.smells.meaninglessSmells\n\n\nid\n  docker.smells.invalidPortsSmells\n\n\nid\n  docker.smells.improperShellSmells\n\n\nid\n  docker.smells.improperEntrypointSmells\n\n\nid\n  docker.smells.deprecatedInstructionSmells\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nHistoric Metric Providers for Puppet Smells\n\n\nThe following Historic Metric Providers are associated with Puppet Smells\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.configuration.puppet.designsmells\n\n\n\n\nShort name\n: historic.configuration.puppet.designsmells\n\n\nFriendly name\n: Number of design smells detected in Puppet manifests per day\n\n\n\n\nThis metric computes the number of the design smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell.\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells\n\n\n\n\nReturns\n :  \nPuppetDesignSmellsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncumulativeNumberOfDesignSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMultifacetedSmells\n\n\nint\n\n\n\n\n\n\nnumberOfUnnecessarySmells\n\n\nint\n\n\n\n\n\n\nnumberOfImperativeSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMissAbSmells\n\n\nint\n\n\n\n\n\n\nnumberOfInsufficientSmells\n\n\nint\n\n\n\n\n\n\nnumberOfUnstructuredSmells\n\n\nint\n\n\n\n\n\n\nnumberOfTightSmells\n\n\nint\n\n\n\n\n\n\nnumberOfBrokenSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMissingDepSmells\n\n\nint\n\n\n\n\n\n\nnumberOfHairballSmells\n\n\nint\n\n\n\n\n\n\nnumberOfDeficientSmells\n\n\nint\n\n\n\n\n\n\nnumberOfWeakenSmells\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nDockerDependenciesHistoricMetric :\n\n\nid\n  puppet.design.smells\n\n\nid\n  puppet.design.multifacetedSmells\n\n\nid\n  puppet.design.unnecessarySmells\n\n\nid\n  puppet.design.imperativeSmells\n\n\nid\n  puppet.design.missAbSmells\n\n\nid\n  puppet.design.insufficientSmells\n\n\nid\n  puppet.design.unstructuredSmells\n\n\nid\n  puppet.design.tightSmells\n\n\nid\n  puppet.design.brokenSmells\n\n\nid\n  puppet.design.missingDepSmells\n\n\nid\n  puppet.design.hairballSmells\n\n\nid\n  puppet.design.deficientSmells\n\n\nid\n  puppet.design.weakenSmells\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.historic.configuration.puppet.implementationsmells\n\n\n\n\nShort name\n: historic.configuration.puppet.implementationsmells\n\n\nFriendly name\n: Number of implementation smells detected in Puppet manifests per day\n\n\n\n\nThis metric computes the number of the implementation smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell.\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells\n\n\n\n\nReturns\n :  \nPuppetImplementationSmellsHistoricMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncumulativeNumberOfImplementationSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMissingDefaultCaseSmells\n\n\nint\n\n\n\n\n\n\nnumberOfInconsistentNamingSmells\n\n\nint\n\n\n\n\n\n\nnumberOfDuplicateEntitySmells\n\n\nint\n\n\n\n\n\n\nnumberOfMisplacedAttributeSmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperAlignment\n\n\nint\n\n\n\n\n\n\nnumberOfInvalidPropertySmells\n\n\nint\n\n\n\n\n\n\nnumberOfImproperQuoteSmells\n\n\nint\n\n\n\n\n\n\nnumberOfLongStatementsSmells\n\n\nint\n\n\n\n\n\n\nnumberOfUnguardedVariableSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMissingDocSmells\n\n\nint\n\n\n\n\n\n\nnumberOfDeprecatedStatementsSmells\n\n\nint\n\n\n\n\n\n\nnumberOfIncompleteTasksSmells\n\n\nint\n\n\n\n\n\n\nnumberOfComplexExpressionSmells\n\n\nint\n\n\n\n\n\n\nnumberOfMissingElseSmells\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation Output Information\n :\n\n\n\n\nDockerDependenciesHistoricMetric :\n\n\nid\n  puppet.implementation.smells\n\n\nid\n  puppet.implementation.missingDefaultCaseSmells\n\n\nid\n  puppet.implementation.inconsistentNamingSmells\n\n\nid\n  puppet.implementation.duplicateEntitySmells\n\n\nid\n  puppet.implementation.misplacedAttributeSmells\n\n\nid\n  puppet.implementation.improperAlignmentSmells\n\n\nid\n  puppet.implementation.invalidPropertySmells\n\n\nid\n  puppet.implementation.improperQuoteSmells\n\n\nid\n  puppet.implementation.longStatementsSmells\n\n\nid\n  puppet.implementation.unguardedVariableSmells\n\n\nid\n  puppet.implementation.missingDocSmells\n\n\nid\n  puppet.implementation.deprecatedStatementsSmells\n\n\nid\n  puppet.implementation.incompleteTasksSmells\n\n\nid\n  puppet.implementation.complexExpressionSmells\n\n\nid\n  puppet.implementation.missingElseSmells\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers\n\n\nTransient metrics are used to calculate heuristics that are associated with a particular period in time, i.e. a single day. Transient Metrics are stored temporarily within the knowledge base and their output is passed as parameters in the calculation of other transient and historic metrics. Depending on the complexity, a transient metric can depend on the output from other tools, other transient metircs or have no dependencies at all.\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Bug Trackers\n\n\nThe following Transient Metric Providers are associated with Issue trackers.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.activeusers\n\n\n\n\nShort name\n: trans.bugs.activeusers\n\n\nFriendly name\n: Number of users with new bug comment in the last 15 days\n\n\n\n\nThis metric computes the number of users that submitted new bug comments in the last 15 days, for each bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n : \nBugsActiveUsersTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugs\n\n\nList<BugsData>\n\n\n\n\n\n\nusers\n\n\nList<User>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugData :\n\n\nString\n  bugTrackerId\n\n\nint\n activeUsers\n\n\nint\n   inactiveUsers\n\n\nint\n previousUsers\n\n\nint\n users\n\n\nint\n days\n\n\n\n\n\n\nUser :\n\n\nString\n  bugTrackerId\n\n\nString\n  userId\n\n\nString\n  lastActivityDate\n\n\nint\n comments\n\n\nint\n requests\n\n\nint\n replies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\nShort name\n: trans.bugs.bugmetadata\n\n\nFriendly name\n: Bug header metadata\n\n\n\n\nThis metric computes various metadata in bug header, i.e. priority, status, operation system and resolution. Other values computed by this metric includes average sentiment, content class and requests/replies.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n,\norg.eclipse.scava.metricprovider.trans.sentimentclassification\n, \norg.eclipse.scava.metricprovider.trans.detectingcode\n\n\n\n\n\n\nReturns\n :  \nBugsBugMetadataTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nBugData\n\n\nList<BugData>\n\n\n\n\n\n\nCommentData\n\n\nList<CommentData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nBugData :\n\n\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nString\n  status\n\n\nList<String>\n    resolution\n\n\nString\n  operatingSystem\n\n\nString\n  priority\n\n\nString\n  creationTime\n\n\nString\n  lastClosedTime\n\n\nString\n  startSentiment\n\n\nString\n  endSentiment\n\n\nfloat\n   averageSentiment\n\n\nint\n     commentSum\n\n\nint\n     sentimentSum\n\n\nString\n  firstCommentId\n\n\nString\n  lastCommentId\n\n\n\n\n\n\n\n\nCommentData :\n\n\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nString\n    commentId\n\n\nString\n    creationTime\n\n\nString\n    creator\n\n\nString\n    contentClass\n\n\nString\n    requestReplyPrediction\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.comments\n\n\n\n\nShort name\n: trans.bugs.comments\n\n\nFriendly name\n: Number of bug comments\n\n\n\n\nThis metric computes the number of bug comments, per bug tracker.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nBugsCommentsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerData\n\n\nList<BugTrackerData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerData:\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfComments\n\n\nint\n cumulativeNumberOfComments\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.contentclasses\n\n\n\n\nShort name\n: trans.bugs.contentclasses\n\n\nFriendly name\n: Content classes in bug comments\n\n\n\n\nThis metric computes the frequency and percentage of content Classes in bug comments, per bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsContentClassesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerData\n\n\nList<BugTrackerData>\n\n\n\n\n\n\ncontentClasses\n\n\nList<ContentClass>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerData :\n\n\nString\n    bugTrackerId\n\n\nint\n   numberOfComments\n\n\nContentClass :\n\n\nString\n    bugTrackerId\n\n\nString\n    classLabel\n\n\nint\n   numberOfComments\n\n\nfloat\n percentage\n\n\n\n\nNote\n : \nclassLabel\n could be one of the content classes shown in the hierarchical tree structure below. Where a node consists of sub-trees of children, only the child nodes are considered as \nclassLabel\n. For example, bug comments of type  \n1. Clarification\n can be labelled as either \n1.1\n or \n1.2\n. A node without sub-trees such as \n2. Suggestion of solution\n is considered \nclassLabel\n on its own.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies\n\n\n\n\nShort name\n: trans.bugs.dailyrequestsreplies\n\n\nFriendly name\n: Number of bug comments, requests and replies per day\n\n\n\n\nThis metric computes the number of bug comments, including those regarded as requests and replies each day, per bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nBugsDailyRequestsRepliesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndayComments\n\n\nList<DayComments>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDayComments :\n\n\nString\n  name\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfComments\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nfloat\n   percentageOfComments\n\n\nfloat\n   percentageOfRequests\n\n\nfloat\n   percentageOfReplies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.emotions\n\n\n\n\nShort name\n: trans.bugs.emotions\n\n\nFriendly name\n: Emotions in bug comments\n\n\n\n\nThis metric computes the emotional dimensions in bug comments, per bug tracker. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.emotionclassification\n\n\n\n\n\n\nReturns\n :  \nBugsEmotionsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerData\n\n\nList<BugTrackerData>\n\n\n\n\n\n\ndimensions\n\n\nList<EmotionDimension>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfComments\n\n\nint\n cumulativeNumberOfComments\n\n\n\n\n\n\nEmotionDimension :\n\n\nString\n  bugTrackerId\n\n\nString\n  emotionLabel (\nanger\n, \nfear\n, \njoy\n, \nsadness\n, \nlove\n, \nsurprise\n)\n\n\nint\n numberOfComments\n\n\nint\n cumulativeNumberOfComments\n\n\nfloat\n   percentage\n\n\nfloat\n   cumulativePercentage\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies\n\n\n\n\nShort name\n: trans.bugs.hourlyrequestsreplies\n\n\nFriendly name\n: Number of bug comments, requests and replies per hour\n\n\n\n\nThis metric computes the number of bug comments, including those regarded as requests and replies, every hour of the day, per bug tracker.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nBugsHourlyRequestsRepliesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nhourComments\n\n\nList<HourComments>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nHourComments :\n\n\nString\n  bugTrackerId\n\n\nString\n  hour\n\n\nint\n numberOfComments\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nfloat\n   percentageOfComments\n\n\nfloat\n   percentageOfRequests\n\n\nfloat\n   percentageOfReplies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.migrationissues\n\n\n\n\nShort name\n: trans.bugs.migrationissues\n\n\nFriendly name\n: Migration Issues Detection in Bug Trackers\n\n\n\n\nThis metric detects migration issues in Bug Tracking Systems.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.topics\n\n\n\n\n\n\nReturns\n :  \nBugTrackerMigrationIssueTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerMigrationIssues\n\n\nList<BugTrackerMigrationIssue>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerMigrationIssue :\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nString\n  summary\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas\n\n\n\n\nShort name\n: trans.bugs.migrationissuesmaracas\n\n\nFriendly name\n: Migration Issues Detection Maracas in Bug Trackers\n\n\n\n\nThis metric detects migration issues in Bug Tracking Systems along with data from Maracas.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.bugs.migrationissues\n, \norg.eclipse.scava.metricprovider.trans.migrationissuesmaracas\n, \norg.eclipse.scava.metricprovider.trans.plaintextprocessing\n\n\n\n\n\n\nReturns\n :  \nBugTrackerMigrationIssueMaracasTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerMigrationIssuesMaracas\n\n\nList<BugTrackerMigrationIssueMaracas>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerMigrationIssueMaracas :\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nList<String>\n changes\n\n\nList<Double>\n matchingPercentage\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.newbugs\n\n\n\n\nShort name\n: trans.bugs.newbugs\n\n\nFriendly name\n: Number of new bugs\n\n\n\n\nThis metric computes the number of new bugs over time, per bug tracker.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nBugsNewBugsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerData\n\n\ntype\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfBugs\n\n\nint\n cumulativeNumberOfBugs\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.patches\n\n\n\n\nShort name\n: trans.bugs.patches\n\n\nFriendly name\n: Number of patches per bug\n\n\n\n\nThis metric computes the number of patches submitted by the community (users) for each bug.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nBugsPatchesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerData\n\n\ntype\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerData :\n\n\nString\n  bugTrackerId\n\n\nint\n numberOfPatches\n\n\nint\n cumulativeNumberOfPatches\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.references\n\n\n\n\nShort name\n: trans.bugs.references\n\n\nFriendly name\n: Bugs References\n\n\n\n\nThis metrics search for references of commits or bugs within comments comming from bugs comments.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nBugsReferenceTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugs\n\n\nList<BugReferringTo>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugReferringTo :\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nString\n  commentId\n\n\nList<String>\n    bugsReferred    (URLs)\n\n\nList<String>\n    commitsReferred (URLs)\n\n\n\n\n\n\n\n\nNote\n :\n    When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.bugs.requestsreplies\n\n\n\n\nShort name\n: trans.bugs.requestreplies\n\n\nFriendly name\n: Bug statistics (answered?, response time)\n\n\n\n\nThis metric computes for each bug, whether it was  answered. If so, it computes the time taken to respond.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.bugmetadata\n\n\n\n\n\n\nReturns\n :  \nBugsRequestsRepliesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugs\n\n\nList<BugStatistics>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugStatistics :\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nboolean\n answered\n\n\nlong\n    responseDurationSec\n\n\nString\n  responseDate\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Newsgroups and Forums\n\n\nThe following Transient Metric Providers are associated with communication channels in general, either newsgroups or forums.\nDespite the name of the metrics are newsgroups, all the metrics are valid for communication channels.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.activeusers \n\n\n\n\nShort name\n: trans.newsgroups.activeusers\n\n\nFriendly name\n: Number of users with new comment in the last 15 days\n\n\n\n\nThis metric computes the number of users that submitted news comments in the last 15 days, per newsgroup.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsActiveUsersTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupData\n\n\nList<NewsgroupData>\n\n\n\n\n\n\nuser\n\n\nList<User>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nNewsgroupData :\n\n\n\n\nString\n  newsgroupName\n\n\nint\n activeUsers\n\n\nint\n inactiveUsers\n\n\nint\n previousUsers\n\n\nint\n users\n\n\nint\n days\n\n\n\n\n\n\n\n\nUser :\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  userId\n\n\nString\n  lastActiveDate\n\n\nint\n articles\n\n\nint\n requests\n\n\nint\n replies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.articles\n\n\n\n\nShort name\n: trans.newsgroups.articles\n\n\nFriendly name\n: Number of articles per newsgroup\n\n\n\n\nThis metric computes the number of articles, per newsgroup.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsArticlesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupData\n\n\nList<NewsgroupData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nNewsgroupData :\n\n\nString\n  newsgroupName\n\n\nint\n numberOfArticles\n\n\nint\n cumulativeNumberOfArticles\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.contentclasses\n\n\n\n\nShort name\n: trans.newsgroups.contentclasses\n\n\nFriendly name\n: Content classes in newsgroup articles\n\n\n\n\nThis metric computes the content classes in newgroup articles, per newsgroup.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsContentClassesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsGroupData\n\n\nList<NewsgroupData>\n\n\n\n\n\n\ncontentClass\n\n\nList< ContentClass>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nNewsGroupData:\n\n\n\n\nString\n  newsgroupName\n\n\nint\n numberOfArticles\n\n\n\n\n\n\n\n\nContentClass:\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  classLabel\n\n\nint\n numberOfArticles\n\n\nfloat\n   percentage\n\n\n\n\n\n\n\n\nNote\n : \nclassLabel\n could be one of the content classes shown in the hierarchical tree structure below. Where a node consists of sub-trees of children, only the child nodes are considered as \nclassLabel\n. For example, articles of type  \n1. Clarification\n can be labelled as either \n1.1\n or \n1.2\n. A node without sub-trees such as \n2. Suggestion of solution\n is considered \nclassLabel\n on its own.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies\n\n\n\n\nShort name\n: trans.newsgroups.dailyrequestsreplies\n\n\nFriendly name\n: Number of articles, requests and replies per day\n\n\n\n\nThis metric computes the number of articles, including those regarded as requests and replies for each day of the week, per newsgroup.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsDailyRequestsRepliesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndailyArticles\n\n\nList<DailyArticles>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDailyArticles :\n\n\nString\n  name\n\n\nint\n numberOfArticles\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nfloat\n   percentageOfArticles\n\n\nfloat\n   percentageOfRequests\n\n\nfloat\n   percentageOfReplies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.emotions\n\n\n\n\nShort name\n: trans.newsgroups.emotions\n\n\nFriendly name\n: Emotions in newsgroup articles\n\n\n\n\nThis metric computes the emotional dimensions in newsgroup articles, per newsgroup. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.emotionclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsEmotionsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupData\n\n\nList<NewsgroupData>\n\n\n\n\n\n\nemotionDimension\n\n\nList<EmotionDimension>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nNewsgroupData :\n\n\n\n\nString\n  newsgroupName\n\n\nint\n numberOfArticles\n\n\nint\n cumulativeNumberOfArticles\n\n\n\n\n\n\n\n\nEmotionDimension :\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  emotionLabel (\nanger\n, \nfear\n, \njoy\n, \nsadness\n, \nlove\n, \nsurprise\n)\n\n\nint\n numberOfArticles\n\n\nint\n cumulativeNumberOfArticles\n\n\nfloat\n   percentage\n\n\nfloat\n   cumulativePercentage\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.hourlyrequestsreplies\n\n\n\n\nShort name\n: trans.newsgroups.hourlyrequestsreplies\n\n\nFriendly name\n: Number of articles, requests and replies per hour\n\n\n\n\nThis metric computes the number of articles, including those regarded as requests and replies for each hour of the day, per newsgroup.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsHourlyRequestsRepliesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nhourArticles\n\n\nList<HourArticles>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nHourArticles :\n\n\nString\n  hour\n\n\nint\n numberOfArticles\n\n\nint\n numberOfRequests\n\n\nint\n numberOfReplies\n\n\nfloat\n   percentageOfArticles\n\n\nfloat\n   percentageOfRequests\n\n\nfloat\n   percentageOfReplies\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.migrationissues\n\n\n\n\nShort name\n: trans.newsgroups.migrationissues\n\n\nFriendly name\n: Migration Issues Detection in Communication Channels\n\n\n\n\nThis metric detects migration issues in Communication Channels articles.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.topics\n, \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsMigrationIssueTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupsMigrationIssues\n\n\nList<NewsgroupsMigrationIssue>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nNewsgroupsMigrationIssue :\n\n\nString\n  newsgroupName\n\n\nint\n threadId\n\n\nlong\n articleId\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas\n\n\n\n\nShort name\n: trans.newsgroups.migrationissuesmaracas\n\n\nFriendly name\n: Migration Issues Detection Maracas in Newsgroups\n\n\n\n\nThis metric detects migration issues in Newsgroups along with data from Maracas.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.newsgroups.migrationissues\n, \norg.eclipse.scava.metricprovider.trans.migrationissuesmaracas\n, \norg.eclipse.scava.metricprovider.trans.plaintextprocessing\n\n\n\n\n\n\nReturns\n :  \nBugTrackerMigrationIssueMaracasTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nnewsgroupsMigrationIssuesMaracas\n\n\nList<NewsgroupMigrationIssueMaracas>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nNewsgroupMigrationIssueMaracas :\n\n\nString\n  newsgrupName\n\n\nint\n threadId\n\n\nList<String>\n changes\n\n\nList<Double>\n matchingPercentage\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.sentiment\n\n\n\n\nShort name\n: trans.newsgroups.sentiment\n\n\nFriendly name\n: Average sentiment in newsgroup threads\n\n\n\n\nThe metric computes the average sentiment, including sentiment at the beginning and end of each thread, per newsgroup. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n, \norg.eclipse.scava.metricprovider.trans.sentimentclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsSentimentTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nthreadStatistics\n\n\nList<ThreadStatistics>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThreadStatistics :\n\n\nString\n    newsgroupName\n\n\nint\n   threadId\n\n\nfloat\n averageSentiment\n\n\nString\n    startSentiment\n\n\nString\n    endSentiment\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n\n\n\n\nShort name\n: trans.newsgroups.threads\n\n\nFriendly name\n: Assigns newsgroup articles to threads\n\n\n\n\nThis metric holds information for assigning newsgroup articles to threads. The threading algorithm is executed from scratch every time.\n\n\n\n\n\n\nDepends-on\n : \nNone\n,\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsThreadsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\narticleData\n\n\nList<ArticleData>\n\n\n\n\n\n\nthreadData\n\n\nList<ThreadData>\n\n\n\n\n\n\nnewsgroupData\n\n\nList<NewsgroupData>\n\n\n\n\n\n\ncurrentDate\n\n\nList<CurrentDate>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nArticleData :\n\n\n\n\nString\n  newsgroupName\n\n\nint\n articleNumber\n\n\nString\n  articlesId\n\n\nString\n  date\n\n\nString\n  from\n\n\nString\n  subject\n\n\nString\n  contentClass\n\n\nString\n  references\n\n\n\n\n\n\n\n\nThreadData :\n\n\n\n\nint\n threadId\n\n\n\n\n\n\n\n\nNewsgroupData :\n\n\n\n\nString\n  newsgroupName\n\n\nint\n threads\n\n\nint\n previousThreads\n\n\n\n\n\n\n\n\nCurrentDate :\n\n\n\n\nString\n  date\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies\n\n\n\n\nShort name\n: trans.newsgroups.threadsrequestsreplies\n\n\nFriendly name\n: Thread statistics (answered?, response time)\n\n\n\n\nThe metric computes for each thread whether it is answered. If so, it computes the response time.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n, \norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\n\n\nReturns\n :  \nNewsgroupsThreadsRequestsRepliesTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nthreadStatistics\n\n\nList<ThreadStatistics>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThreadStatistics :\n\n\nString\n  newsgroupName\n\n\nint\n threadId\n\n\nboolean\n firstRequest\n\n\nboolean\n answered\n\n\nlong\n    responseDurationSec\n\n\nString\n  responseDate\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Documentation\n\n\nThe following Transient Metric Providers are associated with documentation analyses.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.documentation\n\n\n\n\nShort name\n: trans.documentation\n\n\nFriendly name\n: Documentation processing\n\n\n\n\nThis metric process the files returned from the documentation readers and extracts the body (in format HTML or text)\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nDocumentationTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationEntries\n\n\nList<DocumentationEntry>\n\n\n\n\n\n\ndocumentation\n\n\nList<Documentation>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nDocumentationEntry :\n\n\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\nString\n  body\n\n\nString\n  originalFormatName\n\n\nString\n  originalFormatMime\n\n\nboolean\n htmlFormatted\n\n\n\n\n\n\n\n\nDocumentation :\n\n\n\n\nString\n  documentationId\n\n\nList<String>\n    entriesId\n\n\nList<String>\n    removedEntriesId\n\n\nString\n  lastUpdateDate\n\n\nString\n  lastRevisionAnalyzed\n\n\nString\n  nextUpdateDate\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.documentation.classification\n\n\n\n\nShort name\n: trans.documentation.classification\n\n\nFriendly name\n: Documentation classification\n\n\n\n\nThis metric determines which type of documentation is present. The possible types are: \nAPI\n, \nDevelopment\n, \nInstallation\n, \nStarted\n, \nUser\n.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.documentation\n, \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nDocumentationClassificationTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationEntriesClassification\n\n\nList<DocumentationEntryClassification>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDocumentationEntryClassification :\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\nList<String>\n    types\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.documentation.detectingcode\n\n\n\n\nShort name\n: trans.documentation.detectingcode\n\n\nFriendly name\n: Documentation detection of code\n\n\n\n\nThis metric process the plain text from documentation and detects the portions corresponding to code and natural language\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.documentation.plaintext\n\n\n\n\n\n\nReturns\n :  \nDocumentationDetectingCodeTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationEntriesDetectingCode\n\n\nList<DocumentationEntryDetectingCode>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDocumentationEntryDetectingCode :\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\nString\n  naturalLanguage\n\n\nString\n  code\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.documentation.plaintext\n\n\n\n\nShort name\n: trans.documentation.plaintext\n\n\nFriendly name\n: Documentation plain text processor\n\n\n\n\nThis metric process the body of each documentation entry and extracts the plain text\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.documentation\n\n\n\n\n\n\nReturns\n :  \nDocumentationPlainTextTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationEntriesPlainText\n\n\nList<DocumentationEntryPlainText>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDocumentationEntryPlainText :\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\nList<String>\n    plainText\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.documentation.readability\n\n\n\n\nShort name\n: trans.documentation.readability\n\n\nFriendly name\n: Documentation calculation of readability\n\n\n\n\nThis metric calculates the readability of each documentation entry. The higher the score, the more difficult to understand the text.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.documentation\n, \norg.eclipse.scava.metricprovider.trans.documentation.detectingcode\n\n\n\n\n\n\nReturns\n :  \nDocumentationReadabilityTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationEntriesReadability\n\n\nList<DocumentationEntryReadability>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDocumentationEntryReadability :\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\ndouble\n  readability\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.documentation.sentiment\n\n\n\n\nShort name\n: trans.documentation.sentiment\n\n\nFriendly name\n: Documentation Sentiment Analysis\n\n\n\n\nThis metric calculates the sentiment polarity of each documentation entry. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.documentation\n, \norg.eclipse.scava.metricprovider.trans.documentation.detectingcode\n\n\n\n\n\n\nReturns\n :  \nDocumentationSentimentTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndocumentationEntriesSentiment\n\n\nList<DocumentationEntrySentiment>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nDocumentationEntrySentiment :\n\n\nString\n  documentationId\n\n\nString\n  entryId\n\n\nString\n  polarity\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Natural Language Processing\n\n\nThe following Transient Metric Providers are associated with Natural Language Processing tools.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.detectingcode\n\n\n\n\nShort name\n: trans.detectingcode\n\n\nFriendly name\n: Distinguishes between code and natural language\n\n\n\n\nThis metric determines the parts of a bug comment or a newsgroup article that contains code or natural language.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.plaintextprocessing\n\n\n\n\n\n\nReturns\n :  \nDetectingCodeTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerComments\n\n\nList<BugTrackerCommentDetectingCode>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticleDetectingCode>\n\n\n\n\n\n\nforumPosts\n\n\nList<ForumPostsDetectingCode>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nBugTrackerCommentDetectingCode :\n\n\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nString\n  commentId\n\n\nString\n  naturalLanguage\n\n\nString\n  code\n\n\n\n\n\n\n\n\nNewsgroupArticleDetectingCode :\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  articleNumber\n\n\nString\n  naturalLanguage\n\n\nString\n  code\n\n\n\n\n\n\n\n\nForumPostsDetectingCode :\n\n\n\n\nString\n  forumId\n\n\nString\n  topicId\n\n\nString\n  postId\n\n\nString\n  naturalLanguage\n\n\nString\n  code\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.emotionclassification\n\n\n\n\nShort name\n: trans.emotionclassification\n\n\nFriendly name\n: Emotion classifier\n\n\n\n\nThis metric computes the emotions present in each bug comment, newsgroup article or forum post. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.detectingcode\n\n\n\n\n\n\nReturns\n :  \nEmotionClassificationTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerComments\n\n\nList<BugTrackerCommentEmotionClassification>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticleEmotionClassification>\n\n\n\n\n\n\nforumPosts\n\n\nList<ForumPostsEmotionClassification>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nBugTrackerCommentEmotionClassification :\n\n\n\n\nString\n  bugTrackerId\n\n\nString\n  bugId\n\n\nString\n  commentId\n\n\nString\n  emotions\n\n\n\n\n\n\n\n\nNewsgroupArticleEmotionClassification :\n\n\n\n\nString\n  newsgroupName\n\n\nString\n  articleNumber\n\n\nString\n  emotions\n\n\n\n\n\n\n\n\nForumPostsEmotionClassification :\n\n\n\n\nString\n  forumId\n\n\nString\n  topicId\n\n\nString\n  postId\n\n\nString\n  emotions\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.plaintextprocessing\n\n\n\n\nShort name\n: trans.plaintextprocessing\n\n\nFriendly name\n: Plain text processing\n\n\n\n\nThis metric preprocess each bug comment, newsgroup article or forum post into a split plain text format.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nPlainTextProcessingTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerComments\n\n\nList<BugTrackerCommentPlainTextProcessing>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticlePlainTextProcessing>\n\n\n\n\n\n\nforumPosts\n\n\nList<ForumPostsPlainTextProcessing>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerCommentPlainTextProcessing :\n\n\nString\n    bugTrackerId\n\n\nString\n    bugId\n\n\nString\n    commentId\n\n\nString\n    plainText\n\n\n\n\nboolean\n   hadReplies\n\n\n\n\n\n\nNewsgroupArticlePlainTextProcessing :\n\n\n\n\nString\n    newsgroupName\n\n\nString\n    articleNumber\n\n\nString\n    plainText\n\n\n\n\nboolean\n   hadReplies\n\n\n\n\n\n\nForumPostsPlainTextProcessing :\n\n\n\n\nString\n    forumId\n\n\nString\n    topicId\n\n\nString\n    postId\n\n\nString\n    plainText\n\n\nboolean\n   hadReplies\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.requestreplyclassification\n\n\n\n\nShort name\n: trans.requestreplyclassification\n\n\nFriendly name\n: Request/Reply classification\n\n\n\n\nThis metric computes if a bug comment, newsgroup article or forum post is a request of a reply.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.plaintextprocessing\n, \norg.eclipse.scava.metricprovider.trans.detectingcode\n\n\n\n\n\n\nReturns\n :  \nRequestReplyClassificationTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerComments\n\n\nList<BugTrackerComments>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticles>\n\n\n\n\n\n\nforumPosts\n\n\nList<ForumPosts>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerComments :\n\n\nString\n    bugTrackerId\n\n\nString\n    bugId\n\n\nString\n    commentId\n\n\nString\n    classificationResult\n\n\n\n\nString\n    date\n\n\n\n\n\n\nNewsgroupArticles :\n\n\n\n\nString\n    newsgroupName\n\n\nString\n    articleNumber\n\n\nString\n    classificationResult\n\n\n\n\nString\n    date\n\n\n\n\n\n\nForumPosts :\n\n\n\n\nString\n    forumId\n\n\nString\n    topicId\n\n\nString\n    postId\n\n\nString\n    classificationResult\n\n\nString\n    date\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.sentimentclassification\n\n\n\n\nShort name\n: trans.sentimentclassification\n\n\nFriendly name\n: Sentiment classification\n\n\n\n\nThis metric computes the sentiment of each bug comment, newsgroup article or forum post. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or 1 (positive sentiment).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.detectingcode\n\n\n\n\n\n\nReturns\n :  \nSentimentClassificationTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerComments\n\n\nList<BugTrackerCommentsSentimentClassification>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticlesSentimentClassification>\n\n\n\n\n\n\nforumPosts\n\n\nList<ForumPostSentimentClassification>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerCommentsSentimentClassification :\n\n\nString\n    bugTrackerId\n\n\nString\n    bugId\n\n\nString\n    commentId\n\n\n\n\nString\n    polarity (\nnegative (-1)\n, \nneutral (0)\n or \npositive (1)\n)\n\n\n\n\n\n\nNewsgroupArticlesSentimentClassification :\n\n\n\n\nString\n    newsgroupName\n\n\nString\n    articleNumber\n\n\n\n\nString\n    polarity (\nnegative (-1)\n, \nneutral (0)\n or \npositive (1)\n)\n\n\n\n\n\n\nForumPostSentimentClassification :\n\n\n\n\nString\n    forumId\n\n\nString\n    topicId\n\n\nString\n    postId\n\n\nString\n    polarity (\nnegative (-1)\n, \nneutral (0)\n or \npositive (1)\n)\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.severityclassification\n\n\n\n\nShort name\n: trans.severityclassification\n\n\nFriendly name\n: Severity classification\n\n\n\n\nThis metric computes the severity of each bug comment, newsgroup article or forum post. Severity could be blocker, critical, major, minor, enhancement,  normal). For bug comments, there is an additional severity level called \nunknown\n. A bug severity is considered \nunknown\n if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.detectingcode\n, \norg.eclipse.scava.metricprovider.trans.newsgroups.threads\n\n\n\n\n\n\nReturns\n :  \nSeverityClassificationTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerBugs\n\n\nList<BugTrackerBugsData>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticleData>\n\n\n\n\n\n\nnewsgroupThreads\n\n\nList<NewsgroupThreadData>\n\n\n\n\n\n\nforumPosts\n\n\nForumPostData>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerBugsData :\n\n\nString\n    bugTrackerId\n\n\nString\n    bugId\n\n\nString\n    severity\n\n\nint\n   unigrams\n\n\nint\n   bigrams\n\n\nint\n   trigrams\n\n\nint\n   quadgrams\n\n\nint\n   charTrigrams\n\n\nint\n   charQuadgrams\n\n\n\n\nint\n   charFivegrams\n\n\n\n\n\n\nNewsgroupArticleData :\n\n\n\n\nString\n    NewsgroupName\n\n\nlong\n  articleNumber\n\n\nint\n   unigrams\n\n\nint\n   bigrams\n\n\nint\n   trigrams\n\n\nint\n   quadgrams\n\n\nint\n   charTrigrams\n\n\nint\n   charQuadgrams\n\n\n\n\nint\n   charFivegrams\n\n\n\n\n\n\nNewsgroupThreadData :\n\n\n\n\nString\n    newsgroupName\n\n\nint\n   threadId\n\n\n\n\nString\n    severity\n\n\n\n\n\n\nBugTrackerBugsData :\n\n\n\n\nString\n    forumId\n\n\nString\n    topicId\n\n\nString\n    severity\n\n\nint\n   unigrams\n\n\nint\n   bigrams\n\n\nint\n   trigrams\n\n\nint\n   quadgrams\n\n\nint\n   charTrigrams\n\n\nint\n   charQuadgrams\n\n\nint\n   charFivegrams\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.topics\n\n\n\n\nShort name\n: trans.topics\n\n\nFriendly name\n: Topic clustering\n\n\n\n\nThis metric computes topic clusters for each bug comment, newsgroup article or forum post in the last 30 days.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.detectingcode\n\n\n\n\n\n\nReturns\n :  \nTopicsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nbugTrackerComments\n\n\nList<BugTrackerCommentsData>\n\n\n\n\n\n\nbugTrackerTopics\n\n\nList<BugTrackerTopic>\n\n\n\n\n\n\nnewsgroupArticles\n\n\nList<NewsgroupArticlesData>\n\n\n\n\n\n\nnewsgroupTopics\n\n\nList<NewsgroupTopic>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugTrackerCommentsData :\n\n\nString\n    bugTrackerId\n\n\nString\n    bugId\n\n\nString\n    commentId\n\n\nString\n    subject\n\n\nString\n    text\n\n\n\n\nString\n    date\n\n\n\n\n\n\nNewsgroupArticlesData :\n\n\n\n\nString\n    newsgroupName\n\n\nlong\n  articleNumber\n\n\nString\n    subject\n\n\nString\n    text\n\n\n\n\nString\n    date\n\n\n\n\n\n\nBugTrackerTopic :\n\n\n\n\nString\n    bugTrackerId\n\n\nList<String>\n  labels\n\n\nint\n   numberOfDocuments\n\n\n\n\nList<String>\n commentsId\n\n\n\n\n\n\nNewsgroupTopic :\n\n\n\n\nString\n    newsgroupName\n\n\nList<String>\n  labels\n\n\nint\n   numberOfDocuments\n\n\nList<Long>\n articlesId\n\n\n\n\n\n\nTransient Metric Providers for Commits and Committers\n\n\nThese metrics are related to the commits and committers of a project.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.commits.message.plaintext\n\n\n\n\nShort name\n: trans.commits.message.plaintext\n\n\nFriendly name\n: Commits message plain text\n\n\n\n\nThis metric preprocess each commit message to get a split plain text version.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nCommitsMessagePlainTextTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncommitsMessagesPlainText\n\n\nList<CommitMessagePlainText>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nCommitMessagePlainText :\n\n\nString\n  repository (URL)\n\n\nString\n  revision (Commit SHA)\n\n\nList<String>\n    plainText\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.commits.messagereferences\n\n\n\n\nShort name\n: trans.commits.messagereferences\n\n\nFriendly name\n: Commits Messages References\n\n\n\n\nThis metrics search for references of commits or bugs within the messages of commits. In order to detect bugs references, it is necessary to use at the same time one Bug Tracker, as the retrieval of references are based on patterns defined by bug trackers. If multiple or zero Bug Trackers are defined in the project, the metric will only search for commits (alphanumeric strings of 40 characters).\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nCommitsMessageReferenceTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncommitsMessagesReferringTo\n\n\nList<CommitMessageReferringTo>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nBugReferringTo :\n\n\nString\n  repository (URL)\n\n\nString\n  revision (Commit SHA)\n\n\nList<String>\n    bugsReferred    (URLs)\n\n\nList<String>\n    commitsReferred (URLs)\n\n\n\n\n\n\n\n\nNote\n :\n    When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.commits.message.topics\n\n\n\n\nShort name\n: trans.commits.message.topics\n\n\nFriendly name\n: Commits Messages Topic Clustering\n\n\n\n\nThis metric computes topic clusters for each commit message.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.commits.message.plaintext\n\n\n\n\n\n\nReturns\n :  \nCommitsMessageTopicsTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ncommitsMessages\n\n\nList<CommitMessage>\n\n\n\n\n\n\ncommitsTopics\n\n\nList<CommitsTopic>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nCommitMessage :\n\n\n\n\nString\n  repository (URL)\n\n\nString\n  revision (Commit SHA)\n\n\nString\n  subject\n\n\nString\n  message\n\n\nDate\n    date\n\n\n\n\n\n\n\n\nCommitsTopic :\n\n\n\n\nString\n  repository (URL)\n\n\nList<String>\n    labels\n\n\nint\n numberOfMessages\n\n\nList<String>\n    commitsMessageId\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.activeCommitters\n\n\n\n\nShort name\n: activeCommitters\n\n\nFriendly name\n: Committers of last two weeks\n\n\n\n\nA list of committers who have been active the last two weeks. This metric is meant for downstream processing.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersToday\n\n\nReturns\n: \nrel[datetime,set[str]]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.committersoverfile\n\n\n\n\nShort name\n: giniCommittersOverFile\n\n\nFriendly name\n: Committers over file\n\n\n\n\nCalculates the gini coefficient of committers per file\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.countCommittersPerFile\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.countCommittersPerFile\n\n\n\n\nShort name\n: countCommittersPerFile\n\n\nFriendly name\n: Number of committers per file\n\n\n\n\nCount the number of committers that have touched a file.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersPerFile\n\n\nReturns\n: \nmap[loc file, int numberOfCommitters]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.firstLastCommitDatesPerDeveloper\n\n\n\n\nShort name\n: firstLastCommitDates\n\n\nFriendly name\n: First and last commit dates per developer\n\n\n\n\nCollects per developer the first and last dates on which he or she contributed code. This basic metric is used downstream for other metrics, but\nit is also used to drill down on the membership of specific individuals of the development team.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersToday\n\n\nReturns\n: \nmap[str, tuple[datetime,datetime]]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.developmentTeam\n\n\n\n\nShort name\n: developmentTeam\n\n\nFriendly name\n: Development team\n\n\n\n\nLists the names of people who have been contributing code at least once in the history of the project.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersToday\n\n\nReturns\n: \nset[str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.percentageOfWeekendCommits\n\n\n\n\nShort name\n: percentageOfWeekendCommits\n\n\nFriendly name\n: Percentage of weekend commits\n\n\n\n\nPercentage of commits made during the weekend\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.commitsPerWeekDay\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.maximumActiveCommittersEver\n\n\n\n\nShort name\n: maximumActiveCommittersEver\n\n\nFriendly name\n: Maximum active committers ever\n\n\n\n\nWhat is the maximum number of committers who have been active together in any two week period?\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.numberOfActiveCommitters.historic\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.developmentTeamEmails\n\n\n\n\nShort name\n: developmentTeamEmails\n\n\nFriendly name\n: Development team\n\n\n\n\nLists the names of people who have been contributing code at least once in the history of the project.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersEmailsToday\n\n\nReturns\n: \nset[str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.developmentDomainNames\n\n\n\n\nShort name\n: developmentDomainNames\n\n\nFriendly name\n: Development team domain names\n\n\n\n\nLists the domain names of email addresses of developers if such information is present.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.developmentTeamEmails\n\n\nReturns\n: \nset[str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.committersPerFile\n\n\n\n\nShort name\n: committersPerFile\n\n\nFriendly name\n: Committers per file\n\n\n\n\nRegister which committers have contributed to which files\n\n\n\n\nDepends-on\n: - \nReturns\n: \nrel[loc,str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.longerTermActiveCommitters\n\n\n\n\nShort name\n: longerTermActiveCommitters\n\n\nFriendly name\n: Committers of last year\n\n\n\n\nCommitters who have been active the last 12 months. This metric is meant for downstream processing.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.committersToday\n\n\nReturns\n: \nrel[datetime,set[str]]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.commitsPerDeveloper\n\n\n\n\nShort name\n: commitsPerDeveloper\n\n\nFriendly name\n: Number of commits per developer\n\n\n\n\nThe number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits,\nwhen combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.committersAge\n\n\n\n\nShort name\n: ageOfCommitters\n\n\nFriendly name\n: Developer experience in project\n\n\n\n\nMeasures in days the amount of time between the first and last contribution of each developer.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.firstLastCommitDatesPerDeveloper\n\n\nReturns\n: \nrel[str,int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.committersToday\n\n\n\n\nShort name\n: committersToday\n\n\nFriendly name\n: Active committers\n\n\n\n\nWho have been active today?\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.projectAge\n\n\n\n\nShort name\n: projectAge\n\n\nFriendly name\n: Age of the project (nr of days between first and last commit)\n\n\n\n\nAge of the project (nr of days between first and last commit)\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.firstLastCommitDatesPerDeveloper\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.commitsPerWeekDay\n\n\n\n\nShort name\n: commitsPerWeekDay\n\n\nFriendly name\n: Commits per week day\n\n\n\n\nOn which day of the week do commits take place?\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.committersEmailsToday\n\n\n\n\nShort name\n: committersEmailsToday\n\n\nFriendly name\n: Active committers\n\n\n\n\nWho have been active today?\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.sizeOfDevelopmentTeam\n\n\n\n\nShort name\n: sizeOfDevelopmentTeam\n\n\nFriendly name\n: Size of development team\n\n\n\n\nHow many people have ever contributed code to this project?\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.developmentTeam\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.numberOfActiveCommittersLongTerm\n\n\n\n\nShort name\n: numberOfActiveCommittersLongTerm\n\n\nFriendly name\n: Number of active committers long term\n\n\n\n\nNumber of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.longerTermActiveCommitters\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.activecommitters.numberOfActiveCommitters\n\n\n\n\nShort name\n: numberOfActiveCommitters\n\n\nFriendly name\n: Number of active committers\n\n\n\n\nNumber of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days.\n\n\n\n\nDepends-on\n: \ntrans.rascal.activecommitters.activeCommitters\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.commitsToday\n\n\n\n\nShort name\n: commitsToday\n\n\nFriendly name\n: Number of commits today\n\n\n\n\nCounts the number of commits made today.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnToday\n\n\n\n\nShort name\n: commitsToday\n\n\nFriendly name\n: Churn of today\n\n\n\n\nCounts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerCommitInTwoWeeks\n\n\n\n\nShort name\n: churnPerCommitInTwoWeeks\n\n\nFriendly name\n: Churn per commit in two weeks\n\n\n\n\nThe ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnInTwoWeeks\n\n\nrascal.generic.churn.commitsInTwoWeeks\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnActivity\n\n\n\n\nShort name\n: churnActivity\n\n\nFriendly name\n: Churn over the last two weeks\n\n\n\n\nChurn in the last two weeks: collects the lines of code added and deleted over a 14-day sliding window.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnToday\n\n\nReturns\n: \nrel[datetime,int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.commitActivity\n\n\n\n\nShort name\n: commitActivity\n\n\nFriendly name\n: Commits in last two weeks\n\n\n\n\nNumber of commits in the last two weeks: collects commit activity over a 14-day sliding window.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.commitsToday\n\n\nReturns\n: \nrel[datetime,int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.coreCommittersChurn\n\n\n\n\nShort name\n: coreCommittersChurn\n\n\nFriendly name\n: Churn per core committer\n\n\n\n\nFind out about the committers what their total number of added and deleted lines for this system.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnPerCommitter\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.filesPerCommit\n\n\n\n\nShort name\n: numberOfFilesPerCommit\n\n\nFriendly name\n: Number of files per commit\n\n\n\n\nCounts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerCommit\n\n\n\n\nShort name\n: churnPerCommit\n\n\nFriendly name\n: Counts number of lines added and deleted per commit.\n\n\n\n\nCount churn. Churn is the number lines added or deleted. We measure this per commit because the commit\nis a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerCommitter\n\n\n\n\nShort name\n: churnPerCommitter\n\n\nFriendly name\n: Churn per committer\n\n\n\n\nCount churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[str author, int churn]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnPerFile\n\n\n\n\nShort name\n: churnPerFile\n\n\nFriendly name\n: Churn per file\n\n\n\n\nChurn per file counts the number of files added and deleted for a single file. This is a basic metric to indicate hotspots in the design of the system which is changed often. This metric is used further downstream.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc file, int churn]\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.commitsInTwoWeeks\n\n\n\n\nShort name\n: commitsInTwoWeeks\n\n\nFriendly name\n: Number of commits in the last two weeks\n\n\n\n\nChurn in the last two weeks: aggregates the number of commits over a 14-day sliding window.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.commitActivity\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.generic.churn.churnInTwoWeeks\n\n\n\n\nShort name\n: churnInTwoWeeks\n\n\nFriendly name\n: Sum of churn in the last two weeks\n\n\n\n\nChurn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window.\n\n\n\n\nDepends-on\n: \nrascal.generic.churn.churnActivity\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Generic Source Code\n\n\nThese metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in.\n\n\nBack to top\n\n\n\n\ntrans.rascal.readability.fileReadability\n\n\n\n\nShort name\n: fileReadability\n\n\nFriendly name\n: File readability\n\n\n\n\nCode readability per file, measured by use of whitespace measures deviations from common usage of whitespace in source code, such\nas spaces after commas. This is a basic collection metric which is used further downstream.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.readability.fileReadabilityQuartiles\n\n\n\n\nShort name\n: fileReadabilityQ\n\n\nFriendly name\n: File readability quartiles\n\n\n\n\nWe measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles\nrepresent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a\nlack of attention to readability.\n\n\n\n\nDepends-on\n: \ntrans.rascal.readability.fileReadability\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.headerCounts\n\n\n\n\nShort name\n: headerCounts\n\n\nFriendly name\n: Number of appearances of estimated unique headers\n\n\n\n\nIn principle it is expected for the files in a project to share the same license. The license text in the header of each file may differ slightly due to different copyright years and or lists of contributors. The heuristic allows for slight differences. The metric produces the number of different types of header files found. A high number is a contra-indicator, meaning either a confusing licensing scheme or the source code of many different projects is included in the code base of the analyzed system.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nlist[int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.commentedOutCode\n\n\n\n\nShort name\n: commentedOutCode\n\n\nFriendly name\n: Lines of commented out code per file\n\n\n\n\nLines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how\nmuch source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.commentLOC\n\n\n\n\nShort name\n: commentLOC\n\n\nFriendly name\n: Number of lines containing comments per file\n\n\n\n\nNumber of lines containing comments per file is a basic metric used for downstream processing. This metric does not consider the difference\nbetween natural language comments and commented out code.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.commentLinesPerLanguage\n\n\n\n\nShort name\n: commentLinesPerLanguage\n\n\nFriendly name\n: Number of lines containing comments per language (excluding headers)\n\n\n\n\nNumber of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream.\n\n\n\n\nDepends-on\n: \ntrans.rascal.comments.commentLOC\n\n\ntrans.rascal.comments.headerLOC\n\n\ntrans.rascal.comments.commentedOutCode\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.commentedOutCodePerLanguage\n\n\n\n\nShort name\n: commentedOutCodePerLanguage\n\n\nFriendly name\n: Lines of commented out code per language\n\n\n\n\nLines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how\nmuch source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator.\n\n\n\n\nDepends-on\n: \ntrans.rascal.comments.commentedOutCode\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.headerLOC\n\n\n\n\nShort name\n: headerLOC\n\n\nFriendly name\n: Header size per file\n\n\n\n\nHeader size per file is a basic metric counting the size of the comment at the start of each file. It is used for further processing downstream.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.matchingLicenses\n\n\n\n\nShort name\n: matchingLicenses\n\n\nFriendly name\n: Used licenses (from selected list of known licenses)\n\n\n\n\nWe match against a list of known licenses to find out which are used in the current project\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[str]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.comments.headerPercentage\n\n\n\n\nShort name\n: headerPercentage\n\n\nFriendly name\n: Percentage of files with headers.\n\n\n\n\nPercentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling.\n\n\n\n\nDepends-on\n: \ntrans.rascal.comments.headerLOC\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.LOC.genericLOC\n\n\n\n\nShort name\n: countLoc\n\n\nFriendly name\n: Language independent physical lines of code\n\n\n\n\nPhysical lines of code simply counts the number of newline characters (OS independent) in a source code file.\nThe metric can be used to compare the volume between two systems.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.LOC.genericLOCoverFiles\n\n\n\n\nShort name\n: giniLOCOverFiles\n\n\nFriendly name\n: Spread of code over files\n\n\n\n\nWe find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.LOC.locPerLanguage\n\n\n\n\nShort name\n: locPerLanguage\n\n\nFriendly name\n: Physical lines of code per language\n\n\n\n\nPhysical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language.\nThe metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written.\n\n\n\n\nDepends-on\n: \ntrans.rascal.LOC.genericLOC\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.clones.cloneLOCPerLanguage\n\n\n\n\nShort name\n: cloneLOCPerLanguage\n\n\nFriendly name\n: Lines of code in Type I clones larger than 6 lines, per language\n\n\n\n\nLines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[str, int]\n\n\n\n\n\n\nTransient Metric Providers for Java Code\n\n\nThese metrics are related to the Java source code of analyzed projects.\n\n\nBack to top\n\n\n\n\nstyle.filesWithErrorProneness\n\n\n\n\nShort name\n: filesWithErrorProneness\n\n\nFriendly name\n: Files with style violations which make the code error prone. This is basic metric which can not be easily compared between projects.\n\n\n\n\nPercentage of files with error proneness\n\n\n\n\nDepends-on\n: \nstyle.errorProneness\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.understandability\n\n\n\n\nShort name\n: understandability\n\n\nFriendly name\n: Inefficient code\n\n\n\n\nPercentage of the projects files with coding style violations which indicate the code may be hard to read and understand,\nbut not necessarily more error prone.\n\n\n\n\nDepends-on\n: \nstyle.styleViolations\n\n\nReturns\n: \nTable\n\n\n\n\nBack to top\n\n\n\n\nstyle.inefficiencies\n\n\n\n\nShort name\n: inefficiencies\n\n\nFriendly name\n: Inefficient code\n\n\n\n\nPercentage of the projects files with coding style violations which indicate common inefficient ways of doing things in Java.\n\n\n\n\nDepends-on\n: \nstyle.styleViolations\n\n\nReturns\n: \nTable\n\n\n\n\nBack to top\n\n\n\n\nstyle.filesWithUnderstandabilityIssues\n\n\n\n\nShort name\n: filesWithUnderstandabilityIssues\n\n\nFriendly name\n: Files with style violations which make the code harder to understand\n\n\n\n\nPercentage of files with understandability issues. This is a basic metric which can not be easily compared between projects.\n\n\n\n\nDepends-on\n: \nstyle.understandability\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.errorProneness\n\n\n\n\nShort name\n: errorProneness\n\n\nFriendly name\n: Error proneness\n\n\n\n\nPercentage of the projects files with coding style violations which indicate error prone code. This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation.\n  Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects.\n  Other metrics further downstream do aggregate this information.\n\n\n\n\nDepends-on\n: \nstyle.styleViolations\n\n\nReturns\n: \nTable\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfStyleViolations\n\n\n\n\nShort name\n: spreadOfStyleViolations\n\n\nFriendly name\n: Spread of style violations over files\n\n\n\n\nBetween 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.styleViolations\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nstyle.filesWithInefficiencies\n\n\n\n\nShort name\n: filesWithInefficiencies\n\n\nFriendly name\n: Files with style violations which indicate inefficiencies. This is a basic metric which can not be easily compared between projects.\n\n\n\n\nPercentage of files with inefficiencies\n\n\n\n\nDepends-on\n: \nstyle.inefficiencies\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.filesWithStyleViolations\n\n\n\n\nShort name\n: filesWithStyleViolations\n\n\nFriendly name\n: Counts the number of files with any kind of style violation. This metric can not be easily compared between projects.\n\n\n\n\nPercentage of files with style violations\n\n\n\n\nDepends-on\n: \nstyle.styleViolations\n\n\nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfUnderstandabilityIssues\n\n\n\n\nShort name\n: spreadOfUnderstandabilityIssues\n\n\nFriendly name\n: Spread of understandability issues over files\n\n\n\n\nBetween 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.understandability\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfInefficiencies\n\n\n\n\nShort name\n: spreadOfInefficiencies\n\n\nFriendly name\n: Spread of inefficiencies over files\n\n\n\n\nBetween 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.inefficiencies\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nstyle.styleViolations\n\n\n\n\nShort name\n: styleViolations\n\n\nFriendly name\n: All style violations\n\n\n\n\nThis is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation.\n  Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects.\n  Other metrics further downstream do aggregate this information.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nTable\n\n\n\n\nBack to top\n\n\n\n\nstyle.spreadOfErrorProneness\n\n\n\n\nShort name\n: spreadOfErrorProneness\n\n\nFriendly name\n: Spread of error proneness style violations over files\n\n\n\n\nBetween 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.\n\n\n\n\nDepends-on\n: \nstyle.errorProneness\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nrascal.testability.java.TestOverPublicMethods\n\n\n\n\nShort name\n: percentageOfTestedPublicMethods\n\n\nFriendly name\n: Number of JUnit tests averaged over the total number of public methods\n\n\n\n\nNumber of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we\ncompute how far from the ideal situation the project is.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nrascal.testability.java.NumberOfTestMethods\n\n\n\n\nShort name\n: numberOfTestMethods\n\n\nFriendly name\n: Number of JUnit test methods. This is an intermediate absolute metric used to compute others. The bare metric is hard to compare between projects.\n\n\n\n\nNumber of JUnit test methods\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\nrascal.testability.java.TestCoverage\n\n\n\n\nShort name\n: estimateTestCoverage\n\n\nFriendly name\n: Static Estimation of test coverage\n\n\n\n\nThis is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate\nthis by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation,\nas compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator\nfor a lack in testing effort for the project.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MIF-Java\n\n\n\n\nShort name\n: MIF_Java\n\n\nFriendly name\n: Method inheritance factor (Java)\n\n\n\n\nMethod inheritance factor (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.Ca-Java-Quartiles\n\n\n\n\nShort name\n: Ca_Java_Q\n\n\nFriendly name\n: Afferent coupling quartiles (Java)\n\n\n\n\nAfferent coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.Ca-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.DAC-Java\n\n\n\n\nShort name\n: DAC_Java\n\n\nFriendly name\n: Data abstraction coupling (Java)\n\n\n\n\nData abstraction coupling (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.CF-Java\n\n\n\n\nShort name\n: CF_Java\n\n\nFriendly name\n: Coupling factor (Java)\n\n\n\n\nCoupling factor (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.I-Java\n\n\n\n\nShort name\n: I_Java\n\n\nFriendly name\n: Instability (Java)\n\n\n\n\nInstability (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.Ce-Java\n\n\ntrans.rascal.OO.java.Ca-Java\n\n\nReturns\n: \nmap[loc, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.DAC-Java-Quartiles\n\n\n\n\nShort name\n: DAC_Java_Q\n\n\nFriendly name\n: Data abstraction coupling quartiles (Java)\n\n\n\n\nData abstraction coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.DAC-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MPC-Java-Quartiles\n\n\n\n\nShort name\n: MPC_Java_Q\n\n\nFriendly name\n: Message passing coupling quartiles (Java)\n\n\n\n\nMessage passing coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.MPC-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOM-Java\n\n\n\n\nShort name\n: NOM_Java\n\n\nFriendly name\n: Number of methods (Java)\n\n\n\n\nNumber of methods (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCOM-Java\n\n\n\n\nShort name\n: LCOM_Java\n\n\nFriendly name\n: Lack of cohesion in methods (Java)\n\n\n\n\nLack of cohesion in methods (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.CBO-Java\n\n\n\n\nShort name\n: CBO_Java\n\n\nFriendly name\n: Coupling between objects (Java)\n\n\n\n\nCoupling between objects (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.Ce-Java\n\n\n\n\nShort name\n: Ce_Java\n\n\nFriendly name\n: Efferent coupling (Java)\n\n\n\n\nEfferent coupling (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.PF-Java\n\n\n\n\nShort name\n: PF_Java\n\n\nFriendly name\n: Polymorphism factor (Java)\n\n\n\n\nPolymorphism factor (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.RFC-Java-Quartiles\n\n\n\n\nShort name\n: RFC_Java_Q\n\n\nFriendly name\n: Response for class quartiles (Java)\n\n\n\n\nResponse for class quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.RFC-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.I-Java-Quartiles\n\n\n\n\nShort name\n: I_Java_Q\n\n\nFriendly name\n: Instability quartiles (Java)\n\n\n\n\nInstability quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.I-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.RFC-Java\n\n\n\n\nShort name\n: RFC_Java\n\n\nFriendly name\n: Response for class (Java)\n\n\n\n\nResponse for class (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCC-Java\n\n\n\n\nShort name\n: LCC_Java\n\n\nFriendly name\n: Loose class cohesion (Java)\n\n\n\n\nLoose class cohesion (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MIF-Java-Quartiles\n\n\n\n\nShort name\n: MIF_Java_Q\n\n\nFriendly name\n: Method inheritance factor quartiles (Java)\n\n\n\n\nMethod inheritance factor quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.MIF-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.DIT-Java\n\n\n\n\nShort name\n: DIT_Java\n\n\nFriendly name\n: Depth of inheritance tree (Java)\n\n\n\n\nDepth of inheritance tree (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MHF-Java\n\n\n\n\nShort name\n: MHF_Java\n\n\nFriendly name\n: Method hiding factor (Java)\n\n\n\n\nMethod hiding factor (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.TCC-Java\n\n\n\n\nShort name\n: TCC_Java\n\n\nFriendly name\n: Tight class cohesion (Java)\n\n\n\n\nTight class cohesion (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.AHF-Java\n\n\n\n\nShort name\n: AHF_Java\n\n\nFriendly name\n: Attribute hiding factor (Java)\n\n\n\n\nAttribute hiding factor (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCOM-Java-Quartiles\n\n\n\n\nShort name\n: LCOM_Java_Q\n\n\nFriendly name\n: Lack of cohesion in methods quartiles (Java)\n\n\n\n\nLack of cohesion in methods quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.LCOM-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.Ca-Java\n\n\n\n\nShort name\n: Ca_Java\n\n\nFriendly name\n: Afferent coupling (Java)\n\n\n\n\nAfferent coupling (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.A-Java\n\n\n\n\nShort name\n: A_Java\n\n\nFriendly name\n: Abstractness (Java)\n\n\n\n\nAbstractness (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.DIT-Java-Quartiles\n\n\n\n\nShort name\n: DIT_Java_Q\n\n\nFriendly name\n: Depth of inheritance tree quartiles (Java)\n\n\n\n\nDepth of inheritance tree quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.DIT-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.TCC-Java-Quartiles\n\n\n\n\nShort name\n: TCC_Java_Q\n\n\nFriendly name\n: Tight class cohesion quartiles (Java)\n\n\n\n\nTight class cohesion quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.TCC-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCOM4-Java-Quartiles\n\n\n\n\nShort name\n: LCOM4_Java_Q\n\n\nFriendly name\n: Lack of cohesion in methods 4 quartiles (Java)\n\n\n\n\nLack of cohesion in methods 4 quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.LCOM4-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCOM4-Java\n\n\n\n\nShort name\n: LCOM4_Java\n\n\nFriendly name\n: Lack of cohesion in methods 4 (Java)\n\n\n\n\nLack of cohesion in methods 4 (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.SR-Java\n\n\n\n\nShort name\n: SR_Java\n\n\nFriendly name\n: Specialization ratio (Java)\n\n\n\n\nSpecialization ratio (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.AIF-Java-Quartiles\n\n\n\n\nShort name\n: AIF_Java_Q\n\n\nFriendly name\n: Attribute inheritance factor quartiles (Java)\n\n\n\n\nAttribute inheritance factor quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.AIF-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOC-Java-Quartiles\n\n\n\n\nShort name\n: NOC_Java_Q\n\n\nFriendly name\n: Number of children quartiles (Java)\n\n\n\n\nNumber of children quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.NOC-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOC-Java\n\n\n\n\nShort name\n: NOC_Java\n\n\nFriendly name\n: Number of children (Java)\n\n\n\n\nNumber of children (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.AIF-Java\n\n\n\n\nShort name\n: AIF_Java\n\n\nFriendly name\n: Attribute inheritance factor (Java)\n\n\n\n\nAttribute inheritance factor (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.RR-Java\n\n\n\n\nShort name\n: RR_Java\n\n\nFriendly name\n: Reuse ratio (Java)\n\n\n\n\nReuse ratio (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.LCC-Java-Quartiles\n\n\n\n\nShort name\n: LCC_Java_Q\n\n\nFriendly name\n: Loose class cohesion quartiles (Java)\n\n\n\n\nLoose class cohesion quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.LCC-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOA-Java\n\n\n\n\nShort name\n: NOA_Java\n\n\nFriendly name\n: Number of attributes (Java)\n\n\n\n\nNumber of attributes (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.Ce-Java-Quartiles\n\n\n\n\nShort name\n: Ce_Java_Q\n\n\nFriendly name\n: Efferent coupling quartiles (Java)\n\n\n\n\nEfferent coupling quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.Ce-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOM-Java-Quartiles\n\n\n\n\nShort name\n: NOM_Java_Q\n\n\nFriendly name\n: Number of methods quartiles (Java)\n\n\n\n\nNumber of methods quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.NOM-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.NOA-Java-Quartiles\n\n\n\n\nShort name\n: NOA_Java_Q\n\n\nFriendly name\n: Number of attributes quartiles (Java)\n\n\n\n\nNumber of attributes quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.NOA-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.CBO-Java-Quartiles\n\n\n\n\nShort name\n: CBO_Java_Q\n\n\nFriendly name\n: Coupling between objects quartiles (Java)\n\n\n\n\nCoupling between objects quartiles (Java)\n\n\n\n\nDepends-on\n: \ntrans.rascal.OO.java.CBO-Java\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.OO.java.MPC-Java\n\n\n\n\nShort name\n: MPC_Java\n\n\nFriendly name\n: Message passing coupling (Java)\n\n\n\n\nMessage passing coupling (Java)\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.LOC.java.LOCoverJavaClass\n\n\n\n\nShort name\n: giniLOCOverClassJava\n\n\nFriendly name\n: Distribution of physical lines of code over Java classes, interfaces and enums\n\n\n\n\nThe distribution of physical lines of code over Java classes, interfaces and enums explains how complexity is distributed over the design elements of a system.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles\n\n\n\n\nShort name\n: countUsesOfAdvancedLanguageFeaturesQ\n\n\nFriendly name\n: Usage of advanced Java features quartiles\n\n\n\n\nQuartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values.\n\n\n\n\nDepends-on\n: \ntrans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava\n\n\nReturns\n: \nmap[str, real]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava\n\n\n\n\nShort name\n: countUsesOfAdvancedLanguageFeatures\n\n\nFriendly name\n: Usage of advanced Java features\n\n\n\n\nUsage of advanced Java features (wildcards, union types and anonymous classes), reported per file and line number of the occurrence. This metric is for downstream processing by other metrics.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc file, int count]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.CC.java.CCHistogramJava\n\n\n\n\nShort name\n: CCHistogramJava\n\n\nFriendly name\n: Number of Java methods per CC risk factor\n\n\n\n\nNumber of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis.\n\n\n\n\nDepends-on\n: \ntrans.rascal.CC.java.CCJava\n\n\nReturns\n: \nmap[str, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.CC.java.CCOverJavaMethods\n\n\n\n\nShort name\n: giniCCOverMethodsJava\n\n\nFriendly name\n: CC over Java methods\n\n\n\n\nCalculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects.\n\n\n\n\nDepends-on\n: \ntrans.rascal.CC.java.CCJava\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.CC.java.CCJava\n\n\n\n\nShort name\n: getCC\n\n\nFriendly name\n: McCabe's Cyclomatic Complexity Metric (Java)\n\n\n\n\nCyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases\nyou would need to test the method. A high number indicates also a lot of work to understand the method.  This metric is a basic metric for further processing downstream. It is not easily compared between projects.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nmap[loc, int]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.CC.java.WMCJava\n\n\n\n\nShort name\n: getWMC\n\n\nFriendly name\n: Weighted Method Count (Java)\n\n\n\n\nCyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases\nyou would need to test the method. A high number indicates also a lot of work to understand the method. The weighted method count for a class is the sum\nof the cyclomatic complexity measures of all methods in the class. This metric is a basic metric for further processing downstream. It is not easily compared between projects.\n\n\n\n\nDepends-on\n: \ntrans.rascal.CC.java.CCJava\n\n\nReturns\n: \nmap[loc class, int wmcCount]\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for OSGi Dependencies\n\n\nThese metrics are related to OSGi dependencies declared in \nMANIFEST.MF\n files.\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.numberRequiredPackagesInSourceCode\n\n\n\n\nShort name\n: numberRequiredPackagesInSourceCode\n\n\nFriendly name\n: Number required packages in source code\n\n\n\n\nRetrieves the number of required packages found in the project source code.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.allOSGiPackageDependencies\n\n\n\n\nShort name\n: allOSGiPackageDependencies\n\n\nFriendly name\n: All OSGi package dependencies\n\n\n\n\nRetrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.unversionedOSGiRequiredBundles\n\n\n\n\nShort name\n: unversionedOSGiRequiredBundles\n\n\nFriendly name\n: Unversioned OSGi required bundles\n\n\n\n\nRetrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header).\nIf returned value != {} there is a smell in the Manifest.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.unusedOSGiImportedPackages\n\n\n\n\nShort name\n: unusedOSGiImportedPackages\n\n\nFriendly name\n: Unused OSGi imported packages\n\n\n\n\nRetrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell).\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.numberOSGiSplitImportedPackages\n\n\n\n\nShort name\n: numberOSGiSplitImportedPackages\n\n\nFriendly name\n: Number OSGi split imported packages\n\n\n\n\nRetrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.ratioUnusedOSGiImportedPackages\n\n\n\n\nShort name\n: ratioUnusedOSGiImportedPackages\n\n\nFriendly name\n: Ratio of unused OSGi imported packages\n\n\n\n\nRetrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages.\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.osgi.unusedOSGiImportedPackages\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.allOSGiBundleDependencies\n\n\n\n\nShort name\n: allOSGiBundleDependencies\n\n\nFriendly name\n: All OSGi bundle dependencies\n\n\n\n\nRetrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.unversionedOSGiExportedPackages\n\n\n\n\nShort name\n: unversionedOSGiExportedPackages\n\n\nFriendly name\n: Unversioned OSGi exported packages\n\n\n\n\nRetrieves the set of unversioned OSGi exported packages (declared in the Export-Package header).\nIf returned value != {} there is a smell in the Manifest.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.numberOSGiSplitExportedPackages\n\n\n\n\nShort name\n: numberOSGiSplitExportedPackages\n\n\nFriendly name\n: Number OSGi split exported packages\n\n\n\n\nRetrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.allOSGiDynamicImportedPackages\n\n\n\n\nShort name\n: allOSGiDynamicImportedPackages\n\n\nFriendly name\n: All OSGi dynamically imported packages\n\n\n\n\nRetrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.numberOSGiBundleDependencies\n\n\n\n\nShort name\n: numberOSGiBundleDependencies\n\n\nFriendly name\n: Number all OSGi bundle dependencies\n\n\n\n\nRetrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.ratioUnversionedOSGiImportedPackages\n\n\n\n\nShort name\n: ratioUnversionedOSGiImportedPackages\n\n\nFriendly name\n: Ratio unversioned OSGi imported packages\n\n\n\n\nRetrieves the ratio of unversioned OSGi imported packages.\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.osgi.unversionedOSGiImportedPackages\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.unversionedOSGiImportedPackages\n\n\n\n\nShort name\n: unversionedOSGiImportedPackages\n\n\nFriendly name\n: Unversioned OSGi imported packages\n\n\n\n\nRetrieves the set of unversioned OSGi imported packages (declared in the Import-Package header).\nIf returned value != {} there is a smell in the Manifest.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.numberOSGiPackageDependencies\n\n\n\n\nShort name\n: numberOSGiPackageDependencies\n\n\nFriendly name\n: Number of all OSGi package dependencies\n\n\n\n\nRetrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.ratioUnversionedOSGiRequiredBundles\n\n\n\n\nShort name\n: ratioUnversionedOSGiRequiredBundles\n\n\nFriendly name\n: Ratio unversioned OSGi required bundles\n\n\n\n\nRetrieves the ratio of unversioned OSGi required bundles.\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.osgi.unversionedOSGiRequiredBundles\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.usedOSGiUnimportedPackages\n\n\n\n\nShort name\n: usedOSGiUnimportedPackages\n\n\nFriendly name\n: Used OSGi unimported packages\n\n\n\n\nRetrieves the set of used but unimported packages. This metric does not consider packages implicitly imported through the Bundle-Require header.\nIf set != {} then developers may be depending on the execution environment (smell).\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.ratioUnversionedOSGiExportedPackages\n\n\n\n\nShort name\n: ratioUnversionedOSGiExportedPackages\n\n\nFriendly name\n: Ratio of unversioned OSGi exported packages\n\n\n\n\nRetrieves the ratio of unversioned OSGi exported packages.\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.osgi.unversionedOSGiExportedPackages\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.osgi.ratioUsedOSGiImportedPackages\n\n\n\n\nShort name\n: ratioUsedOSGiImportedPackages\n\n\nFriendly name\n: Ratio of used OSGi imported packages\n\n\n\n\nRetrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Maven dependencies\n\n\nThese metrics are related to Maven dependencies declared in \npom.xml\n files.\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.numberRequiredPackagesInSourceCode\n\n\n\n\nShort name\n: numberRequiredPackagesInSourceCode\n\n\nFriendly name\n: Number required packages in source code\n\n\n\n\nRetrieves the number of required packages found in the project source code.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.ratioOptionalMavenDependencies\n\n\n\n\nShort name\n: ratioOptionalMavenDependencies\n\n\nFriendly name\n: Ratio optional Maven dependencies\n\n\n\n\nRetrieves the ratio of optional Maven dependencies.\n\n\n\n\nDepends-on\n: \ntrans.rascal.dependency.maven.allOptionalMavenDependencies\n\n\nReturns\n: \nreal\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.numberUniqueMavenDependencies\n\n\n\n\nShort name\n: numberUniqueMavenDependencies\n\n\nFriendly name\n: Number unique Maven dependencies\n\n\n\n\nRetrieves the number of unique Maven dependencies.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.allOptionalMavenDependencies\n\n\n\n\nShort name\n: allOptionalMavenDependencies\n\n\nFriendly name\n: All optional Maven dependencies\n\n\n\n\nRetrieves all the optional Maven dependencies.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.isUsingTycho\n\n\n\n\nShort name\n: isUsingTycho\n\n\nFriendly name\n: Is using Tycho\n\n\n\n\nChecks if the current project is a Tycho project.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nbool\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.numberMavenDependencies\n\n\n\n\nShort name\n: numberMavenDependencies\n\n\nFriendly name\n: Number Maven dependencies\n\n\n\n\nRetrieves the number of Maven dependencies.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nint\n\n\n\n\nBack to top\n\n\n\n\ntrans.rascal.dependency.maven.allMavenDependencies\n\n\n\n\nShort name\n: allMavenDependencies\n\n\nFriendly name\n: All Maven dependencies\n\n\n\n\nRetrieves all the Maven dependencies.\n\n\n\n\nDepends-on\n: - \nReturns\n: \nset[loc]\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Docker Dependencies\n\n\nThis metric is related to Docker dependencies declared in Dockerfiles.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.docker.dependencies\n\n\n\n\nShort name\n: trans.configuration.docker.dependencies\n\n\nFriendly name\n: Dependencies declared in Dockerfiles\n\n\n\n\nRetrieves the names of the dependencies that are declared in the Dockerfiles of a project and additional information such as their version and type.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nDockerDependency\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndependencyName\n\n\nString\n\n\n\n\n\n\ndependencyVersion\n\n\nString\n\n\n\n\n\n\ntype\n\n\nString\n\n\n\n\n\n\nsubType\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Puppet Dependencies\n\n\nThis metric is related to Puppet dependencies declared in Puppet manifests.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies\n\n\n\n\nShort name\n: trans.configuration.puppet.dependencies\n\n\nFriendly name\n: Dependencies declared in Puppet manifests\n\n\n\n\nRetrieves the names of the dependencies that are declared in the Puppet manifests of a project and additional information such as their version and type.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nPuppetDependency\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\ndependencyName\n\n\nString\n\n\n\n\n\n\ndependencyVersion\n\n\nString\n\n\n\n\n\n\ntype\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Docker Smells\n\n\nThis metric is related to Docker smells detected in Dockerfiles.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.docker.smells\n\n\n\n\nShort name\n: trans.configuration.docker.smells\n\n\nFriendly name\n: Smells detected in Dockerfiles\n\n\n\n\nDetects the smells in the Dockerfiles of a project and additional information such as their reason, the file and the line that each smells is detected.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nSmell\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nsmellName\n\n\nString\n\n\n\n\n\n\nreason\n\n\nString\n\n\n\n\n\n\ncode\n\n\nString\n\n\n\n\n\n\nfileName\n\n\nString\n\n\n\n\n\n\nline\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Puppet Smells\n\n\nThese metrics are related to Puppet smells detected in Puppet manifests.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells\n\n\n\n\nShort name\n: trans.configuration.puppet.designsmells\n\n\nFriendly name\n: Design smells detected in Puppet manifests\n\n\n\n\nDetects the design smells in the Puppet manifests of a project and additional information such as their reason and the file that each smells is detected.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nSmell\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nsmellName\n\n\nString\n\n\n\n\n\n\nreason\n\n\nString\n\n\n\n\n\n\nfileName\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells\n\n\n\n\nShort name\n: trans.configuration.puppet.implementationsmells\n\n\nFriendly name\n: Implementation smells detected in Puppet manifests\n\n\n\n\nDetects the implementation smells in the Puppet manifests of a project and additional information such as their reason, the file and the line that each smells is detected.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nSmell\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nsmellName\n\n\nString\n\n\n\n\n\n\nreason\n\n\nString\n\n\n\n\n\n\nfileName\n\n\nString\n\n\n\n\n\n\nline\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Docker Antipatterns\n\n\nThis metric is related to Docker antipatterns detected in Dockerfiles.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.docker.antipatterns\n\n\n\n\nShort name\n: trans.configuration.docker.antipatterns\n\n\nFriendly name\n: Antipatterns detected in Dockerfiles\n\n\n\n\nDetects the antipatterns in the Dockerfiles of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nDockerAntipattern\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nsmellName\n\n\nString\n\n\n\n\n\n\nreason\n\n\nString\n\n\n\n\n\n\ncode\n\n\nString\n\n\n\n\n\n\nfileName\n\n\nString\n\n\n\n\n\n\nline\n\n\nString\n\n\n\n\n\n\ncommit\n\n\nString\n\n\n\n\n\n\ndate\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Puppet Antipatterns\n\n\nThese metrics are related to Puppet antipatterns detected in Puppet manifests.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.puppet.designantipatterns\n\n\n\n\nShort name\n: trans.configuration.puppet.designantipatterns\n\n\nFriendly name\n: Design antipatterns detected in Puppet manifests\n\n\n\n\nDetects the design antipatterns in the Puppet manifests of a project and additional information such as their reason, the file that each antipattern is detected and the commit and date that this antipattern is related.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nDesignAntipattern\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nsmellName\n\n\nString\n\n\n\n\n\n\nreason\n\n\nString\n\n\n\n\n\n\nfileName\n\n\nString\n\n\n\n\n\n\ncommit\n\n\nString\n\n\n\n\n\n\ndate\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.puppet.implementationantipatterns\n\n\n\n\nShort name\n: trans.configuration.puppet.implementationantipatterns\n\n\nFriendly name\n: Implementation antipatterns detected in Puppet manifests\n\n\n\n\nDetects the implementation antipatterns in the Puppet manifests of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nSmell\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nsmellName\n\n\nString\n\n\n\n\n\n\nreason\n\n\nString\n\n\n\n\n\n\nfileName\n\n\nString\n\n\n\n\n\n\nline\n\n\nString\n\n\n\n\n\n\ncommit\n\n\nString\n\n\n\n\n\n\ndate\n\n\nString\n\n\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.projects.relations\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransient Metric Providers for Indexing\n\n\nThese metrics facilitate data indexing unto the platform.\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for Projects Relations\n\n\nThis metric is related to the relations between projects that are analysed at the platform.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.configuration.projects.relations\n\n\n\n\nShort name\n: trans.configuration.projects.relations\n\n\nFriendly name\n: Relations between projects\n\n\n\n\nDetects the relations between projects that are already analysed at the platform by determining if a project is used as dependency by another project.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nProjectRelation\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nrelationName\n\n\nString\n\n\n\n\n\n\ndependencyType\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for New Versions\n\n\nThese metrics are related to the new version of the dependencies of the projects that are analysed at the platform.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newversion.docker\n\n\n\n\nShort name\n: trans.newversion.docker\n\n\nFriendly name\n: New versions of Docker dependencies\n\n\n\n\nDetects the new versions of dependencies of Docker based projects.\n\n\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.docker.dependencies\n\n\n\n\n\n\nReturns\n: \nNewDockerVersion\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\npackageName\n\n\nString\n\n\n\n\n\n\noldVersion\n\n\nString\n\n\n\n\n\n\nnewVersion\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newversion.puppet\n\n\n\n\nShort name\n: trans.newversion.puppet\n\n\nFriendly name\n: New versions of Puppet dependencies\n\n\n\n\nDetects the new versions of dependencies of Puppet based projects.\n\n\n\n\n\n\nDepends-on\n: \norg.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies\n\n\n\n\n\n\nReturns\n: \nNewPuppetVersion\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\npackageName\n\n\nString\n\n\n\n\n\n\noldVersion\n\n\nString\n\n\n\n\n\n\nnewVersion\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newversion.osgi\n\n\n\n\nShort name\n: trans.newversion.osgi\n\n\nFriendly name\n: New versions of OSGi dependencies\n\n\n\n\nDetects the new versions of dependencies of OSGi based projects.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nNewOsgiVersion\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\npackageName\n\n\nString\n\n\n\n\n\n\noldVersion\n\n\nString\n\n\n\n\n\n\nnewVersion\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.newversion.maven\n\n\n\n\nShort name\n: trans.newversion.maven\n\n\nFriendly name\n: New versions of Maven dependencies\n\n\n\n\nDetects the new versions of dependencies of Maven based projects.\n\n\n\n\n\n\nDepends-on\n: \nNone\n\n\n\n\n\n\nReturns\n: \nNewMavenVersion\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\npackageName\n\n\nString\n\n\n\n\n\n\noldVersion\n\n\nString\n\n\n\n\n\n\nnewVersion\n\n\nString\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\nShort name\n: index preparation transmetric\n\n\nFriendly name\n: index preparation\n\n\n\n\nThis identifies the metric(s) that have been chosen to be executed by the user in preparation for indexing (note: This is required to enable the indexing capabilities of the platform to be dynamic.\n\n\n\n\n\n\nDepends-on\n : \nNone\n\n\n\n\n\n\nReturns\n :  \nIndexPrepTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nexecutedMetricProviders\n\n\nList<ExecutedMetricProviders>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nExecutedMetricProviders :\n\n\nList<String>\n    metricIdentifiers\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.bugs\n\n\n\n\nShort name\n: bug indexing metric\n\n\nFriendly name\n: bug tracking system indexer\n\n\n\n\nThis metric prepares and indexes documents relating to bug tracking systems.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nBugsIndexingMetric\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.commits\n\n\n\n\nShort name\n: metricprovider.indexing.commits\n\n\nFriendly name\n: Commits indexer\n\n\n\n\nThis metric prepares and indexes documents relating to commits.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nCommitsIndexingMetric\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.communicationchannels\n\n\n\n\nShort name\n: communication channels indexing metric\n\n\nFriendly name\n: communication channels indexer\n\n\n\n\nThis metric prepares and indexes documents relating to communication channels.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n\n\n\n\n\n\nReturns\n :  \nCommunicationChannelsIndexingMetric\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.indexing.documentation\n\n\n\n\nShort name\n: metricprovider.indexing.documentation\n\n\nFriendly name\n: Documentation indexer\n\n\n\n\nThis metric prepares and indexes documents relating to documentation.\n\n\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.indexing.preparation\n, \norg.eclipse.scava.metricprovider.trans.documentation\n\n\n\n\n\n\nReturns\n :  \nDocumentationIndexingMetric\n\n\n\n\n\n\nBack to top\n\n\n\n\nTransient Metric Providers for API\n\n\nThese transient metrics are related to the analysis and evolution of API\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.metricprovider.trans.migrationissuesmaracas\n\n\n\n\nShort name\n: trans.migrationissuesmaracas\n\n\nFriendly name\n: Migration Issues Detection using Maracas\n\n\n\n\nThis metric convert the changes found by Maracas into Regex useful for other metrics.\n\n\n\n\n\n\nDepends-on\n : \ntrans.rascal.api.changedMethods\n\n\n\n\n\n\nReturns\n :  \nMigrationIssueMaracasTransMetric\n which contains:\n\n\n\n\n\n\n\n\nVariable\n\n\nType\n\n\n\n\n\n\n\n\n\n\nmaracasMeasurements\n\n\nList<MaracasMeasurement>\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nMaracasMeasurement :\n\n\nList<String>\n  regex\n\n\nString\n    change\n\n\nint\n   lastUpdateDate\n\n\n\n\nBack to top\n\n\n\n\nFactoids\n\n\nFactoids are plugins used to present data that has been mined and analysed using one or more historic and/or transient metric providers.\n\n\nBack to top\n\n\n\n\nFactoids for Bug Trackers\n\n\nThese factoids are related to bug tracking systems.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.channelusage\n\n\n\n\nShort name\n: factoid.bugs.channelusage\n\n\nFriendly name\n: Bug Tracker Usage data\n\n\n\n\nThis plugin generates the factoid regarding usage data for bug trackers. For example, the total number of new bugs, comments or patches per year.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.newbugs\n, \norg.eclipse.scava.metricprovider.historic.bugs.comments\n, \norg.eclipse.scava.metricprovider.historic.bugs.patches\n\n\n\n\nAdditional Information\n :\n\n\n\n\n\n\nStar rating information :\n\n\n\n\n4 star\n  \nnumber of bugs or comments\n > \nworking days in a year (250)\n.\n\n\n3 star\n  \n2\n x \nnumber of bugs or comments\n > \nworking days in a year (250)\n.\n\n\n2 star\n  \n4\n x \nnumber of bugs or comments\n > \nworking days in a year (250)\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nNote :\n\n\n\n\nIf the analyses is not up to a year (\n365\n days), the proportion of working days is used. We consider that \n250\n working days exist within a year.\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.emotion\n\n\n\n\nShort name\n: factoid.bugs.emotion\n\n\nFriendly name\n: Bug Tracker Emotions\n\n\n\n\nThis plugin generates the factoid regarding emotions for bug trackers. For example, the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.newbugs\n, \norg.eclipse.scava.metricprovider.historic.bugs.comments\n, \norg.eclipse.scava.metricprovider.historic.bugs.patches\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \npositive emotion percentage\n > \n80\n OR \nnegative emotion percentage\n < \n35\n.\n\n\n3 star\n  \npositive emotion percentage\n > \n65\n OR \nnegative emotion percentage\n < \n50\n.\n\n\n2 star\n  \npositive emotion percentage\n > \n50\n OR \nnegative emotion percentage\n < \n65\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.hourly\n\n\n\n\nShort name\n: factoid.bugs.hourly\n\n\nFriendly name\n: Bug Tracker hourly data\n\n\n\n\nThis plugin generates the factoid regarding hourly statistics for bug trackers. For example, the percentage of bugs, comments etc.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \nmaximum percentage of hourly comments\n > \n2\n x \nuniform percentage of comments per hour (100/24)\n.\n\n\n3 star\n  \nmaximum percentage of hourly comments\n > \n4\n x \nuniform percentage of comments per hour (100/24)\n.\n\n\n2 star\n  \nmaximum percentage of hourly comments\n > \n6\n x \nuniform percentage of comments per hour (100/24)\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.responsetime\n\n\n\n\nShort name\n: factoid.bugs.responsetime\n\n\nFriendly name\n: Bug Tracker Response Time\n\n\n\n\nThis plugin generates the factoid regarding response time for bug trackers. This could be a cummulative average, yearly average etc.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.responsetime\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \nZero(0)\n < \nyearly average response time\n < \neight hours milliseconds (8 x 60 x 60 x 1000)\n.\n\n\n3 star\n  \nZero(0)\n < \nyearly average response time\n < \nday milliseconds (3 x eight hour milliseconds)\n.\n\n\n2 star\n  \nZero(0)\n < \nyearly average response time\n < \nweek milliseconds (7 x week milliseconds)\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.sentiment\n\n\n\n\nShort name\n: factoid.bugs.sentiment\n\n\nFriendly name\n: Bug Tracker Sentiment\n\n\n\n\nThis plugin generates the factoid regarding sentiment for bug trackers. For example, the average sentiment in all bug trackers associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.sentiment\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \naverage sentiment\n > \n0.5\n OR \nthread end sentiment\n - \nthread begining sentiment\n > \n0.25\n AND \nthread begining sentiment\n > \n0.15\n.\n\n\n3 star\n  \naverage sentiment\n > \n0.25\n OR \nthread end sentiment\n - \nthread begining sentiment\n > \n0.125\n AND \nthread begining sentiment\n > \n0.0\n.\n\n\n2 star\n  \naverage sentiment\n > \n0\n OR \nthread end sentiment\n - \nthread begining sentiment\n > \n0\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.severity\n\n\n\n\nShort name\n: factoid.bugs.severity\n\n\nFriendly name\n: Bug Tracker Severity\n\n\n\n\nThis plugin generates the factoid regarding severity for bug trackers. For example, the number of bugs per severity level, the average sentiment for each severity etc. There are 8 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered \nunknown\n if there is not enough information for the classifier to make a decision. Also,  \nblocker\n, \ncritical\n and \nmajor\n are regarded as serious bugs.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.severity\n, \norg.eclipse.scava.metricprovider.historic.bugs.severitybugstatus\n, \norg.eclipse.scava.metricprovider.historic.bugs.severityresponsetime\n, \norg.eclipse.scava.metricprovider.historic.bugs.severitysentiment\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n1 star\n    \npercentage of serious bugs\n > \n50\n.\n\n\n2 star\n    \npercentage of serious bugs\n > \n25\n.\n\n\n3 star\n    \npercentage of serious bugs\n > \n12.5\n.\n\n\n4 star\n    \notherwise\n (i.e., fewer percentage of serious bugs).\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.size\n\n\n\n\nShort name\n: factoid.bugs.size\n\n\nFriendly name\n: Bug Tracker Size\n\n\n\n\nThis plugin generates the factoid regarding bug size for bug trackers. For example, the cumulative number of bug comments or patches.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.newbugs\n, \norg.eclipse.scava.metricprovider.historic.bugs.comments\n, \norg.eclipse.scava.metricprovider.historic.bugs.patches\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \nnumber of bugs or parches\n > \n1000\n OR \nnumber of comments\n > \n10000\n.\n\n\n3 star\n  \n2\n x \nnumber of bugs or parches\n > \n1000\n OR \n2\n x \nnumber of comments\n > \n10000\n.\n\n\n2 star\n  \n4\n x \nnumber of bugs or parches\n > \n1000\n OR \n4\n x \nnumber of comments\n > \n10000\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.status\n\n\n\n\nShort name\n: factoid.bugs.status\n\n\nFriendly name\n: Bug Tracker Status\n\n\n\n\nThis plugin generates the factoid regarding bug status for bug trackers. For example, the number of fixed bugs, duplicate bugs etc. There are 7 bug status labels (resolved, nonResolved, fixed, worksForMe, wontFix, invalid and duplicate).\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.status\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \nperventage of resolved bug\n > \n75\n.\n\n\n3 star\n  \nperventage of resolved bug\n > \n50\n.\n\n\n2 star\n  \nperventage of resolved bug\n > \n25\n.\n\n\n1 star\n  \notherwise\n (i.e., very few resolved bugs)\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.threadlength\n\n\n\n\nShort name\n: factoid.bugs.threadlength\n\n\nFriendly name\n: Bug Tracker Thread Length\n\n\n\n\nThis plugin generates the factoid regarding bug thread length for bug trackers. For example, the average length of discussion associated to bugs.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.bugs\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \nZero(0)\n < \naverage comments\n < \n5\n.\n\n\n3 star\n  \nZero(0)\n < \naverage comments\n < \n10\n.\n\n\n2 star\n  \nZero(0)\n < \naverage comments\n < \n20\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.users\n\n\n\n\nShort name\n: factoid.bugs.users\n\n\nFriendly name\n: Bug Tracker Users\n\n\n\n\nThis plugin generates the factoid regarding users for bug trackers. For example, the average number of users associated to a project in a bug tracking system.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.bugs.users\n, \norg.eclipse.scava.metricprovider.historic.bugs.bugs\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \ndaily new users in last month\n > \n8\n x \n0.25\n OR \ndaily active users in last month\n > \n8\n x \n2.5\n OR \ndaily new users in last year\n > \n4\n x \n0.25\n OR daily active users in last year\n > \n4\n x \n2.5*.\n\n\n3 star\n  \ndaily new users in last month\n > \n4\n x \n0.25\n OR \ndaily active users in last month\n > \n4\n x \n2.5\n OR \ndaily new users in last year\n > \n2\n x \n0.25\n OR daily active users in last year\n > \n2\n x \n2.5*.\n\n\n2 star\n  \ndaily new users in last month\n > \n2\n x \n0.25\n OR \ndaily active users in last month\n > \n2\n x \n2.5\n OR \ndaily new users in last year\n > \n0.25\n OR \ndaily active users in last year\n > \n2.5\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.bugs.weekly\n\n\n\n\nShort name\n: factoid.bugs.weekly\n\n\nFriendly name\n: Bug Tracker Weekly\n\n\n\n\nThis plugin generates the factoid regarding weekly user engagements for bug trackers. For example, the average number of bug comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n  \nmaximum percentage of weekly comments\n < \n2\n x \nuniform percentage of comments per week (100/7)\n.\n\n\n3 star\n  \nmaximum percentage of weekly comments\n < \n3\n x \nuniform percentage of comments per week (100/7)\n.\n\n\n2 star\n  \nmaximum percentage of weekly comments\n < \n4\n x \nuniform percentage of comments per week (100/7)\n.\n\n\n1 star\n  \notherwise\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nFactoids for Newsgroups and Forums\n\n\nThese factoids are related to communication channels.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.channelusage\n\n\n\n\nShort name\n: factoid.newsgroups.channelusage\n\n\nFriendly name\n: Newsgroup Channel Usage\n\n\n\n\nThis plugin generates the factoid regarding usage data for newsgroups. For example, the total number of new articles or threads per year.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.articles\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.newthreads\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nnumber of articles\n OR \nthreads\n > \nworking days in a year (250\n).\n\n\n3 star\n    \n2\n x \nnumber of articles\n OR \nthreads\n > \nworking days in a year (250)\n.\n\n\n2 star\n    \n4\n x \nnumber of articles\n OR \nthreads\n > \nworking days in a year (250)\n.\n\n\n\n\n1 star\n    \notherwise\n\n\n\n\n\n\nNote :\n\n\n\n\nIf the analyses is not up to a year (\n365\n days), the proportion of working days is used. We consider that \n250\n working days exist within a year.\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.emotion\n\n\n\n\nShort name\n: factoid.newsgroups.emotion\n\n\nFriendly name\n: Newsgroup Channel Emotion\n\n\n\n\nThis plugin generates the factoid regarding emotions for newsgroups, such as the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.emotions\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \npositive emotion percentage\n > \n80\n OR \nnegative emotion percentage\n < \n35\n.\n\n\n3 star\n    \npositive emotion percentage\n > \n65\n OR \nnegative emotion percentage\n < \n50\n.\n\n\n2 star\n    \npositive emotion percentage\n > \n50\n OR \nnegative emotion percentage\n < \n65\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.hourly\n\n\n\n\nShort name\n: factoid.newsgroups.hourly\n\n\nFriendly name\n: Newsgroup Channel hourly data\n\n\n\n\nThis plugin generates the factoid regarding hourly data for newsgroups, such as the percentage of articles, threads etc.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nmaximum percentage of hourly articles\n > \n2\n x \nuniform percentage of articles per hour (100/24)\n.\n\n\n3 star\n    \nmaximum percentage of hourly articles\n > \n4\n x \nuniform percentage of articles per hour (100/24)\n.\n\n\n2 star\n    \nmaximum percentage of hourly articles\n > \n6\n x \nuniform percentage of articles per hour (100/24)\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.responsetime\n\n\n\n\nShort name\n: factoid.newsgroups.responsetime\n\n\nFriendly name\n: Newsgroup Channel Response Time\n\n\n\n\nThis plugin generates the factoid regarding response time for newsgroups. This could be a cummulative average, yearly average etc.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.responsetime\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nZero(0)\n < \nyearly average response time\n < \neight hours in milliseconds (8 x 60 x 60 x 1000)\n.\n\n\n3 star\n    \nZero(0)\n < \nyearly average response time\n < \nday in milliseconds (3 x eight hours in milliseconds)\n.\n\n\n2 star\n    \nZero(0)\n < \nyearly average response time\n < \nweek in milliseconds (7 x day in milliseconds)\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.sentiment\n\n\n\n\nShort name\n: factoid.newsgroups.sentiment\n\n\nFriendly name\n: Newsgroup Channel Sentiment\n\n\n\n\nThis plugin generates the factoid regarding sentiments for newsgroups. For example, the average sentiment in all newsgroup channel associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.sentiment\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \naverage sentiment\n > \n0.5\n OR \nthread end sentiment\n - \nthread begining sentiment\n > \n0.25\n AND \nthread begining sentiment\n > \n0.15\n.\n\n\n3 star\n    \naverage sentiment\n > \n0.25\n OR \nthread end sentiment\n - \nthread begining sentiment\n > \n0.125\n AND \nthread begining sentiment\n > \n0.0\n.\n\n\n2 star\n    \naverage sentiment\n > \n0\n OR \nthread end sentiment\n - \nthread begining sentiment\n > \n0\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.severity\n\n\n\n\nShort name\n: factoid.newsgroups.severity\n\n\nFriendly name\n: Newsgroup Channel Severity\n\n\n\n\nThis plugin generates the factoid regarding severity for newsgroups. For example, the number of articles per severity level, the average sentiment for each severity etc. There are 7 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial). Note:  \nblocker\n, \ncritical\n and \nmajor\n are regarded as serious bugs.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.severity\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n1 star\n    \npercentage of serious bugs\n > \n50\n.\n\n\n2 star\n    \npercentage of serious bugs\n > \n25\n.\n\n\n3 star\n    \npercentage of serious bugs\n > \n12.5\n.\n\n\n4 star\n    \notherwise\n (i.e., fewer percentage of serious bugs).\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.size\n\n\n\n\nShort name\n: factoid.newsgroups.size\n\n\nFriendly name\n: Newsgroup Channel Size\n\n\n\n\nThis plugin generates the factoid regarding thread or article size for newsgroups. For example, the cummulative number of threads.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.articles\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.newthreads\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nnumber of threads\n > \n1000\n OR \nnumber of articles\n > \n10000\n.\n\n\n3 star\n    \n2\n x \nnumber of threads\n > \n1000\n OR \n2\n x \nnumber of articles\n > \n10000\n.\n\n\n2 star\n    \n4\n x \nnumber of threads\n > \n1000\n OR \n4\n x \nnumber of articles\n > \n10000\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.status\n\n\n\n\nShort name\n: factoid.newsgroups.status\n\n\nFriendly name\n: Newsgroup Channel Status\n\n\n\n\nThis plugin generates the factoid regarding thread or article status for newsgroups. For example, the number of requests and replies, unanswered threads etc.  \n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nperventage of replies\n > \n75\n.\n\n\n3 star\n    \nperventage of replies\n > \n50\n.\n\n\n2 star\n    \nperventage of replies\n > \n25\n.\n\n\n1 star\n    \notherwise\n (i.e., very few replies)\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.threadlength\n\n\n\n\nShort name\n: factoid.newsgroups.threadlength\n\n\nFriendly name\n: Newsgroup Channel Thread Length\n\n\n\n\nThis plugin generates the factoid regarding thread length for newsgroups. For example, the average length of discussion per day, month etc.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.threads\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nZero(0)\n < \naverage comments\n < \n5\n.\n\n\n3 star\n    \nZero(0)\n < \naverage comments\n < \n10\n.\n\n\n2 star\n    \nZero(0)\n < \naverage comments\n < \n20\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.users\n\n\n\n\nShort name\n: factoid.newsgroups.users\n\n\nFriendly name\n: Newsgroup Channel Users\n\n\n\n\nThis plugin generates the factoid regarding users for newsgroups. For example, the average number of users associated to a project in a newsgroup channel.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.newsgroups.users\n, \norg.eclipse.scava.metricprovider.historic.newsgroups.threads\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \ndaily new users in last month\n > \n8\n x \n0.25\n OR \ndaily active users in last month\n > \n8\n x \n2.5\n OR \ndaily new users in last year\n > \n4\n x \n0.25\n OR \ndaily active users in last year\n > \n4\n x \n2.5\n.\n\n\n3 star\n    \ndaily new users in last month\n > \n4\n x \n0.25\n OR \ndaily active users in last month\n > \n4\n x \n2.5\n OR \ndaily new users in last year\n > \n2\n x \n0.25\n OR \ndaily active users in last year\n > \n2\n x \n2.5\n.\n\n\n2 star\n    \ndaily new users in last month\n > \n2\n x \n0.25\n OR \ndaily active users in last month > 2 x 2.5\n OR \ndaily new users in last year\n > \n0.25\n OR \ndaily active users in last year\n > \n2.5\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.newsgroups.weekly\n\n\n\n\nShort name\n: factoid.newsgroups.weekly\n\n\nFriendly name\n: Newsgroup Channel Weekly\n\n\n\n\nThis plugin generates the factoid regarding weekly user engagement for newsgroups. For example, the average number of comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nmaximum percentage of weekly articles\n < \n2\n x \nuniform percentage of articles per week (100/7)\n.\n\n\n3 star\n    \nmaximum percentage of weekly articles\n < \n3\n x \nuniform percentage of articles per week (100/7)\n.\n\n\n2 star\n    \nmaximum percentage of weekly articles\n < \n4\n x \nuniform percentage of articles per week (100/7)\n.\n\n\n1 star\n    \notherwise\n\n\n\n\nBack to top\n\n\n\n\nFactoids for Documentation\n\n\nThese factoids are related to documentation.\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.documentation.entries\n\n\n\n\nShort name\n: factoid.documentation.entries\n\n\nFriendly name\n: Documentation Entries\n\n\n\n\nThis plugin generates the factoid regarding which sections have been found and which are missing in the documentation. This can help understanding which sections should be added or better indicated to have a better documentation.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.trans.documentation.classification\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nAll five types of documentation sections have been found\n.\n\n\n3 star\n    \nThree or four types of documentation sections have been found\n.\n\n\n2 star\n    \nTwo or one types of documentation sections have been found\n.\n\n\n1 star\n    \nNo sections have been found\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.factoid.documentation.sentiment\n\n\n\n\nShort name\n: factoid.documentation.sentiment\n\n\nFriendly name\n: Documentation Sentiment\n\n\n\n\nThis plugin generates the factoid regarding sentiment for documentation.\n\n\n\n\nDepends-on\n : \norg.eclipse.scava.metricprovider.historic.documentation.sentiment.DocumentationSentimentHistoricMetricProvider\n\n\n\n\nAdditional Information\n :\n\n\n\n\nStar rating information :\n\n\n4 star\n    \nAverage sentiment greater than 0.5\n.\n\n\n3 star\n    \nAverage sentiment between 0.25 and 0.5\n.\n\n\n2 star\n    \nAverage sentiment between 0 and 0.25\n.\n\n\n1 star\n    \nAverage sentiment equal or lesser than 0\n\n\n\n\nBack to top",
            "title": "Home"
        },
        {
            "location": "/user-guide/metrics/#metrics-reference-guide",
            "text": "This guide describes the historic and transient metric providers, as well as factoids, provided by the Scava platform.   Historic Metric Providers  for:  Bug Trackers  Newsgroups and Forums  Commits and Committers  Documentation  Generic Source Code  Java Code  OSGi Dependencies  Maven Dependencies  Docker Dependencies  Puppet Dependencies    Docker Smells  Puppet Smells    Transient Metric Providers  for:  Bug Trackers  Newsgroups and forums  Documentation  Natural Language Processing  Commits and Committers  Generic Source Code  Java Code  OSGi Dependencies  Maven Dependencies  Docker Dependencies  Puppet Dependencies    Docker Smells  Puppet Smells  Docker Antipatterns  Puppet Antipatterns  Projects Relations  New Versions  Indexing  API    Factoids  for:  Bug Trackers  Newsgroups and Forums  Documentation",
            "title": "Metrics Reference Guide"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers",
            "text": "Historic metrics maintain a record of various heuristics associated with a specific open source project over its lifetime. They typically depend on the results from one or more transient metrics and are typically displayed in the Scava dashboards.",
            "title": "Historic Metric Providers"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-bug-trackers",
            "text": "The following Historic Metric Providers are associated with Issue trackers  Back to top",
            "title": "Historic Metric Providers for Bug Trackers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsbugs",
            "text": "Short name : historic.bugs.bugs  Friendly name : Number of bugs per day per bug tracker   This metric computes the number of bugs per day for each bug tracker seperately. It also computes additional information such as average comments per bug, average comments per user, average requests and/or replies per user and bug.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata ,  org.eclipse.scava.metricprovider.trans.bugs.activeusers    Returns  :   BugsBugsHistoricMetric  which contains:     Variable  Type      bugTrackers  List<DailyBugTrackerData>    numberOfBugs  int    averageCommentsPerBug  float    averageRequestsPerBug  float    averageRepliesPerBug  float    averageCommentsPerUser  float    averageRequestsPerUser  float    averageRepliesPerUser  float       Additional Information  :   DailyBugTrackerData :  String   bugTrackerId  int  numberOfBugs     Visualisation Output Information  :   BugsHistoricMetricProvider :  id   bugs.bugs  id   bugs.comments-bugaverage  id   bugs.comments-useraverage  id   bugs.requests-bugaverage  id   bugs.requests-useraverage  id   bugs.replies-bugaverage  id   bugs.replies-useraverage  id   bugs.requestsreplies-useraverage  id   bugs.requestsreplies-bugaverage     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.bugs"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugscomments",
            "text": "Short name : historic.bugs.comments  Friendly name : Number of bug comments per day per bug tracker   This metric computes the number of bug comments submitted by the community (users) per day for each bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.comments    Returns  :   BugsCommentsHistoricMetric  which contains:     Variable  Type      Bugs  List<DailyBugData>    numberOfComments  int    cumulativeNumberOfComments  int       Additional Information  :   DailyBugData :  String   bugTrackerID  int  numberOfComments  int  cumulativeNumberOfComments     Visualisation Output Information  :   CommentsHistoricMetricProvider :  id   bugs.comments  id   bugs.cumulativeComments     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.comments"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsemotions",
            "text": "Short name : historic.bugs.emotions  Friendly name : Number of emotions per day per bug tracker   This metric computes the emotional dimensions present in bug comments submitted by the community (users) per day for each bug tracker. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise).    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.emotions    Returns  :   BugsEmotionsHistoricMetric  which contains:     Variable  Type      bugData  List<BugData>    Dimensions  List<Dimensions>       Additional Information  :    BugData :   String  bugTrackerID  int     numberOfComments  int     cumulativeNumberOfComments     Dimensions :   String    bugTrackerId  String   emotionLabel ( anger ,  fear ,  joy ,  sadness ,  love ,  surprise )  int     numberOfComments  int     cumulativeNumberOfComments  float   percentage  float   cumulativePercentage     Visualisation Output Information  :   EmotionsHistoricMetricProvider :  id   bugs.emotions.cumulativeComments  id   bugs.emotions.cumulativeCommentPercentages  id   bugs.emotions.comments  id   bugs.emotions.commentPercentages     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.emotions"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsmigrationissues",
            "text": "Short name : historic.bugs.migrationissues  Friendly name : Migration Issues Detection in Bug Trackers per day per bug tracker   This metric stores how many migration issues have been found per day for each bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.migrationissues    Returns  :   BugTrackerMigrationIssueHistoricMetric  which contains:     Variable  Type      dailyBugTrackerMigrationData  List<DailyBugTrackerMigrationData>       Additional Information  :   DailyBugTrackerMigrationData :  String   bugTrackerId  List<String>  bugsId;  int  numberOfBugs     Visualisation Output Information  :   BugTrackerMigrationIssueHistoricMetricProvider :  id   bugs.dailymigrationissues     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.migrationissues"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsmigrationissuesmaracas",
            "text": "Short name : historic.bugs.migrationissuesmaracas  Friendly name : Migration Issues Detection along with Maracas in Bug Trackers per day per bug tracker   This metric stores how many migration issues have been found containing changes detected with  MARACAS  per day for each bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas    Returns  :   BugTrackerMigrationIssueMaracasHistoricMetric  which contains:     Variable  Type      dailyBugTrackerMigrationMaracasData  List<DailyBugTrackerMigrationMaracasData>    bugTrackerMigrationMaracasData  List<BugTrackerMigrationMaracasData>       Additional Information  :   DailyBugTrackerMigrationMaracasData :  String   bugTrackerId  List<String>  bugsId;  int  numberOfIssues    BugTrackerMigrationMaracasData :  String   bugTrackerId  String  bugId;  List<String>  changesAndMatchingPercentage     Visualisation Output Information  :   BugTrackerMigrationIssueMaracasHistoricMetricProvider :  id   bugs.dailymigrationissuesmaracas  id   bugs.migrationissuesmaracas.changes     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.migrationissuesmaracas"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsnewbugs",
            "text": "Short name : historic.bugs.newbugs  Friendly name : Number of new bugs per day per bug tracker   This metric computes the number of new bugs reported by the community (users) per day for each bug tracker. A small number of bug reports can indicate either a bug-free, robust project or a project with a small/inactive user community.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.newbugs    Returns  :   BugsNewBugsHistoricMetric  which contains:     Variable  Type      dailyBugData  List<DailyBugData>    numberOfBugs  int    cumulativeNumberOfBugs  int       Additional Information  :   DailyBugData :  String   bugTrackerId  int  numberOfBugs  int      cumulativeNumberOfBugs     Visualisation Output Information  :   NewUsersHistoricMetricProvider :  id   bugs.cumulativeNewUsers  id   bugs.newUsers     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.newbugs"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsnewusers",
            "text": "Short name : historic.bugs.newusers  Friendly name : Number of new users per day per bug tracker   This metric computes the number of new users per day  for each bug tracker seperately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.activeusers    Returns  :   BugsNewUsersHistoricMetric  which contains:     Variable  Type      bugTrackers  List<DailyBugTrackerData>    numberOfNewUsers  int    cumulativeNumberOfNewUsers  int       Additional Information  :   DailyBugTrackerData :  String   bugTrackerId  int  numberOfNewUsers  int      cumulativeNumberOfNewUsers     Visualisation Output Information  :   NewUsersHistoricMetricProvider :  id   bugs.cumulativeNewUsers  id   bugs.newUsers     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.newusers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsopentime",
            "text": "Short name : historic.bugs.opentime  Friendly name : Average duration to close an open bug   This metric computes the average duration between creating and closing bugs. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   OpenTimeHistoricMetricProvider  which contains:     Variable  Type      avgBugOpenTime  String    avgBugOpenTimeInDays  double    bugsConsidered  int       Visualisation Output Information  :   OpenTimeHistoricMetricProvider :  id   bugs.bugOpenTime  id   bugs.bugOpenTime-bugs     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.opentime"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugspatches",
            "text": "Short name : historic.bugs.patches  Friendly name : Number of bug patches per day   This class computes the number of bug patches per day, for each bug tracker seperately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.patches    Returns  :   PatchesHistoricMetricProvider  which contains:     Variable  Type      numberOfPatches  int    cumulativeNumberOfPatches  int    bugs  List<DailyBugData>       Additional Information  :   DailyBugData :  String   bugTrackerId  int  numberOfPatches  int  cumulativeNumberOfPatches     Visualisation Output Information  :   PatchesHistoricMetricProvider :  id   bugs.cumulativePatches  id   bugs.patches     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.patches"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsrequestsreplies",
            "text": "Short name : historic.bugs.requestsreplies  Friendly name : Number of request and replies in bug comments per bug tracker   This metric computes the number of requests and replies realting to comments posted to bugs by the community (users) per day  for each bug tracker seperately.    Depends-on  : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsRequestsRepliesHistoricMetric  which contains:     Variable  Type      Bugs  List<DailyBugTrackerData>    numberOfRequests  int    numberOfReplies  int    cumulativeNumberOfRequests  int    cumulativeNumberOfReplies  int       Additional Information  :   DailyBugTrackerData :  String   bugTrackerId  int  numberOfRequests  int  numberOfReplies  int  cumulativeNumberOfRequests  int  cumulativeNumberOfReplies     Visualisation Output Information  :   RequestsRepliesHistoricMetricProvider :  id   bugs.replies  id   bugs.cumulativereplies  id   bugs.requests  id   bugs.cumulativerequests  id   bugs.requestsreplies  id   bugs.cumulativerequestsreplies     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.requestsreplies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsrequestsrepliesaverage",
            "text": "Short name : historic.bugs.requestsreplies.average  Friendly name : Average number of requests and replies in bug comments per bug tracker   This metric computes the average number of bug comments considered as request and reply for each bug tracker per day.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.activeusers    Returns  :   BugsRequestsRepliesHistoricMetric  which contains:     Variable  Type      bugs  List<DailyBugTrackerData>    numberOfRequests  int    numberOfReplies  int    cumulativeNumberOfRequests  int    cumulativeNumberOfReplies  int       Additional Information  :   DailyBugTrackerData :  String   bugTrackerId  int  numberOfRequests  int  numberOfReplies  int  cumulativeNumberOfRequests  int  cumulativeNumberOfReplies     Visualisation Output Information  :   RequestsRepliesHistoricMetricProvider :  id   bugs.requests-averageperday  id   bugs.requestsreplies-averageperday  id   bugs.comments-averageperday  id   bugs.replies-averageperday     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.requestsreplies.average"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsresponsetime",
            "text": "Short name : historic.bugs.responsetime  Friendly name : Average response time to open bugs per bug tracker   This metric computes the average time in which the community (users) responds to open bugs per day for each bug tracker seperately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.requestsreplies    Returns  :   BugsResponseTimeHistoricMetric  which contains:       Variable  Type      bugTrackerId  String    avgResponseTimeFormatted  String    cumulativeAvgResponseTimeFormatted  String    avgResponseTime  float    cumulativeAvgResponseTime  float    bugsConsidered  int    cumulativeBugsConsidered  int     Visualisation Output Information  :   ResponseTimeHistoricMetricProvider :  id   bugs.averageResponseTime  id   bugs.cumulativeAverageResponseTime  id   bugs.cumulativeAverageResponseTime-bugs  id   bugs.averageResponseTime-bugs     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.responsetime"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugssentiment",
            "text": "Short name : historic.bugs.sentiment  Friendly name : Overall sentiment per bug tracker   This metric computes the overall sentiment per bug tracker up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score for each bug contributes equally, regardless of it's size.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsSentimentHistoricMetric  which contains:     Variable  Type      overallAverageSentiment  float    overallSentimentAtThreadBeggining  float    overallSentimentAtThreadEnd  float       Visualisation Output Information  :    SentimentHistoricMetricProvider :   id   bugs.averageSentiment  id   bugs.sentimentAtThreadEnd  id   bugs.sentimentAtThreadBeggining  id   bugs.sentiment     The sentiment related variables above all represent a  Polarity  value. A polarity value closer to:  -1  indicates negative sentiment, closer to  0  indicates neutral sentiment and closer to  1  indicates positive sentiment.    Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.sentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseverity",
            "text": "Short name : historic.bugs.severity  Friendly name : Number of bugs per severity level per bug tracker   This metric computes the number of severity levels for bugs submitted by the community (users) every day for each bug tracker. Specifically, it calculates the number and percentage of bugs that have been categorised into 1 of 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered  unknown  if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.      Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification    Returns  :   BugsSeveritiesHistoricMetric  which contains:     Variable  Type      bugData  List<BugData>    severityLevels  List<ServerityLevel>       Additional Information  :   BugData :  String   bugTrackerId  int  numberOfBugs    SeverityLevel :  String   bugTrackerId  String   severityLevel   ( blocker , critical , major , minor , enhancement ,  normal ,  trivial ,  unknown )  int  numberOfBugs  int  percentage     Visualisation Output Information  :   SeverityHistoricMetricProvider :  id   bugs.severity  id   bugs.severity.percentages     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.severity"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseveritybugstatus",
            "text": "Short name : historic.bugs.severitybugstatus  Friendly name : Number of each bug status per bug severity level   This metric computes the total number and percentage of each bug status per severity level, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate) and 8 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered  unknown  if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.    Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification ,  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsSeverityBugStatusHistoricMetric  which contains:     Variable  Type      severityLevels  List<SeverityLevel>       Additional Information  :   SeverityLevel :  String   severityLevel   ( blocker , critical , major , minor , enhancement ,  normal ,  trivial ,  unknown )  int  numberOfBugs  int  numberOfWontFixBugs  int  numberOfWorksForMeBugs  int  numberOfNonResolvedClosedBugs  int  numberOfInvalidBugs  int  numberOfFixedBugs  int  numberOfDuplicateBugs  float    percentageOfResolvedClosedBugs  float    percentageOfWontFixBugs  float    percentageOfWorksForMeBugs  float    percentageOfNonResolvedClosedBugs  float    percentageOfInvalidBugs  float    percentageOfFixedBugs  float    percentageOfDuplicateBugs     Visualisation Output  :   SeverityBugStatusHistoricMetricProvider :  id   bugs.severity.duplicateBugs  id   bugs.severity.duplicateBugs.percentages  id   bugs.severity.fixedBugs  id   bugs.severity.fixedBugs.percentages  id   bugs.severity.invalidBugs  id   bugs.severity.invalidBugs.percentages  id   bugs.severity.nonResolvedClosedBugs  id   bugs.severity.nonResolvedClosedBugs.percentages  id   bugs.severity.resolvedClosedBugs  id   bugs.severity.resolvedClosedBugs.percentages  id   bugs.severity.wontFixBugs  id   bugs.severity.wontFixBugs.percentages  id   bugs.severity.worksForMeBugs  id   bugs.severity.worksForMeBugs.percentages     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseverityresponsetime",
            "text": "Short name : historic.bugs.severityresponsetime  Friendly name : Average response time to bugs per severity level per day   This metric computes the average time in which the community (users) responds to open bugs per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 8 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered  unknown  if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.    Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification ,  org.eclipse.scava.metricprovider.trans.bugs.requestsreplies    Returns  :   BugsSeverityBugStatusHistoricMetric  which contains:     Variable  Type      severityLevels  List<SeverityLevel>       Additional Information  :   SeverityLevel :  String   severityLevel   ( blocker , critical , major , minor , enhancement ,  normal ,  trivial ,  unknown )  String   avgResponseTimeFormatted  int  numberOfBugs  long     avgResponseTime     Visualisation Output Information  :   SeverityResponseTimeHistoricMetricProvider :  id   bugs.severity.averageResponseTime     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseveritysentiment",
            "text": "Short name : historic.bugs.severitysentiment  Friendly name : Average sentiment per bugs severity level per day   This metric computes for each bug severity level, the average sentiment, sentiment at the begining and end of bug comments posted by the community (users) every day for each bug tracker. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered  unknown  if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.    Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification ,  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsSeverityBugStatusHistoricMetric  which contains:     Variable  Type      severityLevels  List<SeverityLevel>       Additional Information  :   SeverityLevel :  String   severityLevel   ( blocker , critical , major , minor , enhancement ,  normal ,  trivial ,  unknown )  int  numberOfBugs  float    averageSentiment  float    sentimentAtThreadBeggining  float    sentimentAtThreadEnd     Visualisation Output Information  :    SeveritySentimentHistoricMetricProvider :   id   bugs.severity.sentiment  id   bugs.severity.averageSentiment  id   bugs.severity.sentimentAtThreadBeggining  id   bugs.severity.sentimentAtThreadEnd     The sentiment related variables above all represent a  Polarity  value. A polarity value closer to:  -1  indicates negative sentiment, closer to  0  indicates neutral sentiment and closer to  1  indicates positive sentiment.    Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.severitysentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsstatus",
            "text": "Short name : historic.bugs.status  Friendly name : Number of bugs per bug status per day   This metric computes the total number of bugs that corresponds to each bug status, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate).    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsStatusHistoricMetric  which contains:     Variable  Type      numberOfBugs  long    numberOfResolvedClosedBugs  int    numberOfWontFixBugs  int    numberOfWorksForMeBugs  int    numberOfNonResolvedClosedBugs  int    numberOfInvalidBugs  int    numberOfFixedBugs  int    numberOfDuplicateBugs  int       Visualisation Output Information  :   StatusHistoricMetricProvider :  id   bugs.duplicateBugs  id   bugs.fixedBugs  id   bugs.invalidBugs  id   bugs.nonResolvedClosedBugs  id   bugs.wontFixBugs  id   bugs.worksForMeBugs  id   bugs.resolvedClosedBugs     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.status"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugstopics",
            "text": "Short name : historic.bugs.topics  Friendly name : Labels of topic clusters in bug comments per bug tracker   This metric computes the labels of topic clusters extracted from bug comments submitted by the community (users), per bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.topics    Returns  :   BugsTopicsHistoricMetric  which contains:     Variable  Type      bugTopics  List<BugTopic>       Additional Information  :   SeverityLevel :  String   bugTrackerId  List<String>     labels  float    numberOfDocuments  List<String>  commentsId     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.topics"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsunansweredbugs",
            "text": "Short name : historic.bugs.unansweredbugs  Friendly name : Number of unanswered bugs per day   This metric computes the number of unanswered bugs per day.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.requestsreplies    Returns  :   BugsUnansweredBugsHistoricMetric  which contains:     Variable  Type      numberOfUnansweredBugs  int       Visualisation Output Information  :   UnansweredThreadsHistoricMetricProvider :  id   bugs.unansweredBugs     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.unansweredbugs"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsusers",
            "text": "Short name : historic.bugs.users  Friendly name : Number of users, active and inactive per day per bug tracker   This metric computes the number of users, number of active and inactive users per day for each bug tracker separately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.activeusers    Returns  :   BugsUsersHistoricMetric  which contains:     Variable  Type      bugTrackers  List<DailyBugTrackingData>    numberOfUsers  int    numberOfActiveUsers  int    numberOfInactiveUsers  int       Additional Information  :   DailyBugTrackingData :  String   bugTrackerId  int  numberOfUsers  int  numberOfActiveUsers  int  numberOfInactiveUsers     Visualisation Output Information  :   UsersHistoricMetricProvider :  id   bugs.users  id   bugs.activeusers  id   bugs.inactiveusers     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.bugs.users"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-newsgroups-and-forums",
            "text": "The following Historic Metric Providers are associated with newsgroups.  Back to top",
            "title": "Historic Metric Providers for Newsgroups and Forums"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsarticles",
            "text": "Short name : historic.newsgroups.articles  Friendly name : Number of articles per day per news group   This metric computes the number of articles submitted by the community (users) per day for each newsgroup separately    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.articles    Returns  :   NewsgroupsArticlesHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfArticles  int  cummulativeNumberOfArticles     Visualisation Output Information  :   ArticlesHistoricMetricProvider :  id   newsgroups.articles  id   newsgroups.cumulativeArticles     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.articles"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsemotions",
            "text": "Short name : historic.newsgroups.emotions  Friendly name : Number of emotions per day per newsgroup   This metric computes the emotional dimensions present in newsgroup comments submitted by the community (users) per day for each newsgroup. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise).    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.emotions    Returns  :   NewsgroupsEmotionsHistoricMetric  which contains:     Variable  Type      newsgroupsData  List<Newsgroups>    emotionDimension  List<Emotion>       Additional Information  :    NewsgroupsData :   String   newsgroupName  int  numberOfArticles  int  cummulativeNumberOfArticles     EmotionDimension :   String   newsgroupName  String   emotionLabel ( anger ,  fear ,  joy ,  sadness ,  love ,  surprise )  int  numberOfArticles  int  cumulativeNumberOfArticles  float    percentage  float    cumulativePercentage     Visualisation Output Information  :   EmotionsHistoricMetricProvider :  id   newsgroups.emotions.articlePercentages  id   newsgroups.emotions.cumulativeArticles  id   newsgroups.emotions.cumulativeArticlePercentages  id   newsgroups.emotions.articles     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.emotions"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsmigrationissues",
            "text": "Short name : historic.newsgroups.migrationissues  Friendly name : Migration Issues Detection in articles per day per newsgroup   This metric detects migration issues in articles per day for each newgroup.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues    Returns  :   NewsgroupsMigrationIssueHistoricMetric  which contains:     Variable  Type      dailyNewsgroupsMigrationData  List<DailyNewsgroupsMigrationData>       Additional Information  :   DailyBugTrackerMigrationData :  String   newsgroupName  List<Integer>  threadsId  int  numberOfIssues     Visualisation Output Information  :   NewsgroupsMigrationIssueHistoricMetricProvider :  id   newsgroups.dailymigrationissues     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.migrationissues"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsmigrationissuesmaracas",
            "text": "Short name : historic.newsgroups.migrationissuesmaracas  Friendly name : Migration Issues Detection along with Maracas in articles per day per newsgroup   This metric stores how many migration issues have been found containing changes detected with  MARACAS  per day for each newsgroup.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas    Returns  :   NewsgroupMigrationIssueMaracasHistoricMetric  which contains:     Variable  Type      dailyNewsgroupMigrationMaracasData  List<DailyNewsgroupMigrationMaracasData>    newsgroupMigrationMaracasData  List<NewsgroupMigrationMaracasData>       Additional Information  :   DailyNewsgroupMigrationMaracasData :  String   newsgroupName  List<Integer>  threadsId;  int  numberOfIssues    NewsgroupMigrationMaracasData :  String   newsgroupName  int threadId;  List<String>  changesAndMatchingPercentage     Visualisation Output Information  :   NewsgroupMigrationIssueMaracasHistoricMetricProvider :  id   newsgroups.dailymigrationissuesmaracas  id   newsgroups.migrationissuesmaracas.changes     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.migrationissuesmaracas"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsnewthreads",
            "text": "Short name : historic.newsgroups.newthreads  Friendly name : Number of new threads per day per newsgroup   This metric computes the number of new threads submitted by the community (users) per day for each newsgroup separately    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threads    Returns  :   NewsgroupsNewThreadsHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfNewThreads  int  cummulativeNumberOfNewThreads     Visualisation Output Information  :   NewThreadsHistoricMetricProvider :  id   newsgroups.newThreads  id   newsgroups.cumulativeNewThreads     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.newthreads"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsnewusers",
            "text": "Short name : historic.newsgroups.newusers  Friendly name : Number of new users per day per newsgroup   This metric computes the number of new users per day for each newsgroup seperately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.activeusers    Returns  :   NewsgroupsNewUsersHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfNewUsers  int  cummulativeNumberOfNewUsers     Visualisation Output Information  :   NewUsersHistoricMetricProvider:  id   newsgroups.cumulativeNewUsers  id   newsgroups.newUsers     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.newusers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsrequestsreplies",
            "text": "Short name : historic.newsgroups.requestsreplies  Friendly name : Number of requests and replies in articles per day   This metric computes the number of requests and replies in newsgroup articles submitted by the community (users) per day for each newsgroup separately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   NewsgroupsRequestsRepliesHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfRequests  int  numberOfReplies  int  cumulativeNumberOfRequests  int  cumulativeNumberOfReplies     Visualisation Output Information  :   RequestsRepliesHistoricMetricProvider :  id   newsgroups.requests  id   newsgroups.cumulativerequests  id   newsgroups.replies  id   newsgroups.cumulativereplies  id   newsgroups.requestsreplies  id   newsgroups.cumulativerequestsreplies     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsrequestsrepliesaverage",
            "text": "Short name : historic.newsgroups.requestsreplies.average  Friendly name : Average number of articles, requests and replies per day   This metric computes the average number of newsgroup articles, including the number of requests and replies within the newsgroup articles per day.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.activeusers    Returns  :   NewsgroupsRequestsRepliesAverageHistoricMetric  which contains:     Variable  Type      averageArticlesPerDay  float    averageRequestsPerDay  float    averageRepliesPerDay  float       Visualisation Output Information  :   RequestsRepliesAverageHistoricMetricProvider :  id   newsgroups.requestsreplies-averageperday  id   newsgroups.requests-averageperday  id   newsgroups.replies-averageperday  id   newsgroups.comments-averageperday     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsresponsetime",
            "text": "Short name : historic.newsgroups.responsetime  Friendly name : Average response time to threads per day per newsgroup   This metric computes the average time in which the community responds to open threads per day for each newsgroup separately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies    Returns  :   NewsgroupsResponseTimeHistoricMetric  which contains:     Variable  Type      newsgroupName  String    avgResponseTime  long    avgResponseTimeFormatted  String    threadsConsidered  int    cumulativeAvgResponseTimeFormatted  String    cumulativeThreadsConsidered  int       Visualisation Output Information  :   ResponseTimeHistoricMetricProvider :  id   newsgroups.averageResponseTime  id   newsgroups.cumulativeAverageResponseTime  id   newsgroups.cumulativeAverageResponseTime-threads",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.responsetime"
        },
        {
            "location": "/user-guide/metrics/#id-newsgroupsaverageresponsetime-threads",
            "text": "Back to top",
            "title": "id  newsgroups.averageResponseTime-threads"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupssentiment",
            "text": "Short name : historic.newsgroups.sentiment  Friendly name : Overall sentiment of newsgroup articles   This metric computes the overall sentiment per newsgroup repository up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score of each thread contributes equally, irrespective of its size.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.sentiment    Returns  :   NewsgroupsSentimentHistoricMetric  which contains:     Variable  Type      overallAverageSentiment  float    overallSentimentAtThreadBegining  float    overallSentimentAtThreadEnd  float       Visualisation Output Information  :    SentimentHistoricMetricProvider :   id   newsgroups.averageSentiment  id   newsgroups.sentimentAtThreadEnd  id   newsgroups.sentimentAtThreadBeggining  id   newsgroups.sentiment     The sentiment related variables above all represent a  Polarity  value. A polarity value closer to:  -1  indicates negative sentiment, closer to  0  indicates neutral sentiment and closer to  1  indicates positive sentiment.    Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.sentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseverity",
            "text": "Short name : historic.newsgroups.severity  Friendly name : Number of each severity level in newsgroup threads per day   This metric computes the number of each severity levels in threads submitted every day, per newsgroup. There are 7 severity  levels (blocker, critical, major, minor, enhancement,  normal, trivial).      Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification    Returns  :   NewsgroupsSeveritiesHistoricMetric  which contains:     Variable  Type      newsgroupData  List<Newsgroups>    severityLevel  List<SeverityLevel>       Additional Information  :    NewsgroupData :   String   newsgroupName  int  numberThreads     SeverityLevel :   String   newsgroupName  String   severityLabel ( blocker ,  critical ,  major ,  minor ,  enhancement ,   normal ,  trivial )  int  numberOfThreads  float    percentage     Visualisation Output Information  :   SeverityHistoricMetricProvider :  id   newsgroups.severity  id   newsgroups.severity.percentages     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.severity"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseverityresponsetime",
            "text": "Short name : historic.newsgroups.severityresponsetime  Friendly name : Average response time to threads per severity level per day   This metric computes the average time in which the community (users) responds to open threads per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 7 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial).  Average response time to threads per severity level  This metric computes the average response time for newsgroup threads submitted every day, based on their severity levels.    Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies    Returns  :   NewsgroupsSeverityResponseTimeHistoricMetric  which contains:     Variable  Type      severityLevel  List<SeverityLevel>       Additional Information  :   SeverityLevel :  String   severityLabel ( blocker ,  critical ,  major ,  minor ,  enhancement ,   normal ,  trivial )  int  numberOfThreads  long     avgResponseTime  String   avgResponseTimeFormatted     Visualisation Output Information  :   SeverityResponseTimeHistoricMetricProvider :  id   newsgroups.severity.averageResponseTime     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseveritysentiment",
            "text": "Short name : historic.newsgroups.severitysentiment  Friendly name : Average sentiment in threads per severity level per day   This metric computes the average sentiment, the sentiment at the beginning of threads and the sentiment at the end of threads; for each severity level in newsgroup threads submitted every day. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). Note: there are 7 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial).    Depends-on  :  org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.sentiment    Returns  :   NewsgroupsSeveritySentimentHistoricMetric  which contains:     Variable  Type      severityLevel  List<SeverityLevel>       Additional Information  :   SeverityLevel :  String   severityLabel ( blocker ,  critical ,  major ,  minor ,  enhancement ,   normal ,  trivial )  int  numberOfThreads  float    avgSentiment  float    avgSentimentThreadBeginning  float    avgSentimentThreadEnd     Visualisation Output Information  :    SeveritySentimentHistoricMetricProvider :   id   newsgroups.severity.averageSentiment  id   newsgroups.severity.sentiment  id   newsgroups.severity.sentimentAtThreadEnd  id   newsgroups.severity.sentimentAtThreadBeggining     The sentiment related variables above all represent a  Polarity  value. A polarity value closer to:  -1  indicates negative sentiment, closer to  0  indicates neutral sentiment and closer to  1  indicates positive sentiment.    Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsthreads",
            "text": "Short name : historic.newsgroups.threads  Friendly name : Number of threads per day per newsgroup   This metric computes the number of threads per day for each newsgroup separately. The metric also computes average values for articles per thread, requests per thread, replies per thread, articles per user, requests per user and replies per user.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.newsgroups.activeusers    Returns  :   NewsgroupsThreadsHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfThreads  float    averageArticlesPerThread  float    averageRequestsPerThread  float    averageRepliesPerThread  float    averageArticlesPerUser  float    averageRequestsPerUser  float    averageRepliesPerUser     Visualisation Output Information  :   ThreadsHistoricMetricProvider :  id   newsgroups.threads  id   newsgroups.articles-threadaverage  id   newsgroups.articles-useraverage  id   newsgroups.requests-threadaverage  id   newsgroups.requests-useraverage  id   newsgroups.replies-threadaverage  id   newsgroups.replies-useraverage  id   newsgroups.requestsreplies-threadaverage  id   newsgroups.requestsreplies-useraverage     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.threads"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupstopics",
            "text": "Short name : historic.newsgroups.topics  Friendly name : Labels of newsgroup topics per newsgroup   This metric computes the labels of topics clusters in articles submitted by the community (users), for each newsgroup seperately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.topics    Returns  :   NewsgroupTopicsHistoricMetric  which contains:     Variable  Type      newsgroupTopic  List<NewsgrpTopic>       Additional Information  :   NewsgroupTopic :  String   newsgroupName  List<String>     labels  int  numberOfDocuments  List<Integer>  articlesId     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.topics"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsunansweredthreads",
            "text": "Short name : historic.newsgroups.unansweredthreads  Friendly name : Number of unanswered threads per day per newsgroup   This metric computes the number of unanswered threads per day for each newsgroup separately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies    Returns  :   NewsgroupsUnansweredThreadsHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfUnansweredThreads     Visualisation Output Information  :   UnansweredThreadsHistoricMetricProvider :  id   newsgroups.unansweredThreads     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsusers",
            "text": "Short name : historic.newsgroups.users  Friendly name : Number of users, active and inactive per day per newsgroup   This metric computes the number of users, including active and inactive users per day for each newsgroup separately.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.activeusers    Returns  :   NewsgroupsUsersHistoricMetric  which contains:     Variable  Type      dailyNewsgroupData  List<DailyNewsgroupData>       Additional Information  :   DailyNewsgroupData :  String   newsgroupName  int  numberOfUsers  int  numberOfActiveUsers  int  numberOfInactiveUsers     Visualisation Output Information  :   UsersHistoricMetricProvider :  id   newsgroups.users  id   newsgroups.activeusers  id   newsgroups.inactiveusers  id   newsgroups.activeinactiveusers     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.newsgroups.users"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-commits-and-committers",
            "text": "The following Historic Metric Providers are related to the commits and committers of a project.  Back to top",
            "title": "Historic Metric Providers for Commits and Committers"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommittersoverfilehistoric",
            "text": "Short name : giniCommittersOverFile.historic  Friendly name : Historic giniCommittersOverFile  Historic version of : trans.rascal.activecommitters.committersoverfile   Calculates the gini coefficient of committers per file   Depends-on :  trans.rascal.activecommitters.committersoverfile  Returns :  real   Back to top",
            "title": "trans.rascal.activecommitters.committersoverfile.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterspercentageofweekendcommitshistoric",
            "text": "Short name : percentageOfWeekendCommits.historic  Friendly name : Historic percentageOfWeekendCommits  Historic version of : trans.rascal.activecommitters.percentageOfWeekendCommits   Percentage of commits made during the weekend   Depends-on :  trans.rascal.activecommitters.percentageOfWeekendCommits  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.percentageOfWeekendCommits.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommitsperdeveloperhistoric",
            "text": "Short name : commitsPerDeveloper.historic  Friendly name : Historic commitsPerDeveloper  Historic version of : trans.rascal.activecommitters.commitsPerDeveloper   The number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits,\nwhen combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well.   Depends-on :  trans.rascal.activecommitters.commitsPerDeveloper  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.activecommitters.commitsPerDeveloper.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersnumberofactivecommitterslongtermhistoric",
            "text": "Short name : numberOfActiveCommittersLongTerm.historic  Friendly name : Historic numberOfActiveCommittersLongTerm  Historic version of : trans.rascal.activecommitters.numberOfActiveCommittersLongTerm   Number of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days.   Depends-on :  trans.rascal.activecommitters.numberOfActiveCommittersLongTerm  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.numberOfActiveCommittersLongTerm.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersnumberofactivecommittershistoric",
            "text": "Short name : numberOfActiveCommitters.historic  Friendly name : Historic numberOfActiveCommitters  Historic version of : trans.rascal.activecommitters.numberOfActiveCommitters   Number of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days.   Depends-on :  trans.rascal.activecommitters.numberOfActiveCommitters  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.numberOfActiveCommitters.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurncommitstodayhistoric",
            "text": "Short name : commitsToday.historic  Friendly name : Historic commitsToday  Historic version of : rascal.generic.churn.commitsToday   Counts the number of commits made today.   Depends-on :  rascal.generic.churn.commitsToday  Returns :  int   Back to top",
            "title": "rascal.generic.churn.commitsToday.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurntodayhistoric",
            "text": "Short name : commitsToday.historic  Friendly name : Historic commitsToday  Historic version of : rascal.generic.churn.churnToday   Counts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends.   Depends-on :  rascal.generic.churn.churnToday  Returns :  int   Back to top",
            "title": "rascal.generic.churn.churnToday.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnpercommitintwoweekshistoric",
            "text": "Short name : churnPerCommitInTwoWeeks.historic  Friendly name : Historic churnPerCommitInTwoWeeks  Historic version of : rascal.generic.churn.churnPerCommitInTwoWeeks   The ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns.   Depends-on :  rascal.generic.churn.churnPerCommitInTwoWeeks  Returns :  int   Back to top",
            "title": "rascal.generic.churn.churnPerCommitInTwoWeeks.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnfilespercommithistoric",
            "text": "Short name : numberOfFilesPerCommit.historic  Friendly name : Historic numberOfFilesPerCommit  Historic version of : rascal.generic.churn.filesPerCommit   Counts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream.   Depends-on :  rascal.generic.churn.filesPerCommit  Returns :  map[loc, int]   Back to top",
            "title": "rascal.generic.churn.filesPerCommit.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnpercommithistoric",
            "text": "Short name : churnPerCommit.historic  Friendly name : Historic churnPerCommit  Historic version of : rascal.generic.churn.churnPerCommit   Count churn. Churn is the number lines added or deleted. We measure this per commit because the commit\nis a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity.   Depends-on :  rascal.generic.churn.churnPerCommit  Returns :  map[loc, int]   Back to top",
            "title": "rascal.generic.churn.churnPerCommit.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnpercommitterhistoric",
            "text": "Short name : churnPerCommitter.historic  Friendly name : Historic churnPerCommitter  Historic version of : rascal.generic.churn.churnPerCommitter   Count churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing.   Depends-on :  rascal.generic.churn.churnPerCommitter  Returns :  map[str author, int churn]   Back to top",
            "title": "rascal.generic.churn.churnPerCommitter.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurncommitsintwoweekshistoric",
            "text": "Short name : commitsInTwoWeeks.historic  Friendly name : Historic commitsInTwoWeeks  Historic version of : rascal.generic.churn.commitsInTwoWeeks   Churn in the last two weeks: aggregates the number of commits over a 14-day sliding window.   Depends-on :  rascal.generic.churn.commitsInTwoWeeks  Returns :  int   Back to top",
            "title": "rascal.generic.churn.commitsInTwoWeeks.historic"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnintwoweekshistoric",
            "text": "Short name : churnInTwoWeeks.historic  Friendly name : Historic churnInTwoWeeks  Historic version of : rascal.generic.churn.churnInTwoWeeks   Churn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window.   Depends-on :  rascal.generic.churn.churnInTwoWeeks  Returns :  int   Back to top",
            "title": "rascal.generic.churn.churnInTwoWeeks.historic"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoriccommitsmessagestopics",
            "text": "Short name : historic.commits.messages.topics  Friendly name : Labels of topics in commits messages analyzed in the last 30 days   This metric computes the labels of topic clusters in commits messages pushed by users in the last 30 days    Depends-on  :  org.eclipse.scava.metricprovider.trans.commits.message.topics    Returns  :   CommitsMessagesTopicsHistoricMetric  which contains:     Variable  Type      commitMessageTopics  List<CommitMessageTopic>       Additional Information  :   CommitMessageTopic :  String   repository  String   labels  int  numberOfMessages  List<String>     commitsMessageId     Visualisation Output Information  :   CommitsMessagesTopicsHistoricMetricProvider :  id   commits.topics.messages     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.commits.messages.topics"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-documentation",
            "text": "The following Historic Metric Providers are associated with documentation analyses.  Back to top",
            "title": "Historic Metric Providers for Documentation"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricdocumentationreadability",
            "text": "Short name : historic.documentation.readability  Friendly name : Documentation readability Historic Metric   Historic metric that stores the evolution of the documentation readability. The higher the readability score, the harder to understand the text.     Depends-on  :  org.eclipse.scava.metricprovider.trans.documentation.readability    Returns  :   DocumentationReadabilityHistoricMetric  which contains:     Variable  Type      documentationReadability  List<DocumentationHistoricReadability>    documentationEntriesReadability  List<DocumentationEntryHistoricReadability>       Additional Information  :    DocumentationHistoricReadability :   String   documentationId  int  numberOfDocumentationEntries  double   averageDocumentationReadability     DocumentationEntryHistoricReadability :   String   documentationId  String   entryId  double   readability     Visualisation Output Information  :   readability :  id   documentation.readability.entries  id   documentation.readability     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.documentation.readability"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricdocumentationsentiment",
            "text": "Short name : historic.documentation.sentiment  Friendly name : Documentation sentiment polarity Historic Metric   Historic metric that stores the evolution of the documentation sentiment polarity. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)    Depends-on  :  org.eclipse.scava.metricprovider.trans.documentation.sentiment    Returns  :   DocumentationSentimentHistoricMetric  which contains:     Variable  Type      documentationSentiment  List<DocumentationHistoricSentiment>    documentationEntriesSentiment  List<DocumentationEntryHistoricSentiment>       Additional Information  :    DocumentationHistoricSentiment :   String   documentationId  int  numberOfDocumentationEntries  double   averageDocumentationSentiment     DocumentationEntryHistoricSentiment :   String   documentationId  String   entryId  String   polarity     Visualisation Output Information  :   sentiment :  id   documentation.sentiment.entries  id   documentation.sentiment     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.documentation.sentiment"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-generic-source-code",
            "text": "These metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in.  Back to top",
            "title": "Historic Metric Providers for Generic Source Code"
        },
        {
            "location": "/user-guide/metrics/#transrascalclonesclonelocperlanguagehistoric",
            "text": "Short name : cloneLOCPerLanguage.historic  Friendly name : Historic cloneLOCPerLanguage  Historic version of : trans.rascal.clones.cloneLOCPerLanguage   Lines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric.   Depends-on :  trans.rascal.clones.cloneLOCPerLanguage  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.clones.cloneLOCPerLanguage.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalreadabilityfilereadabilityquartileshistoric",
            "text": "Short name : fileReadabilityQ.historic  Friendly name : Historic fileReadabilityQ  Historic version of : trans.rascal.readability.fileReadabilityQuartiles   We measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles\nrepresent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a\nlack of attention to readability.   Depends-on :  trans.rascal.readability.fileReadabilityQuartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.readability.fileReadabilityQuartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentscommentlinesperlanguagehistoric",
            "text": "Short name : commentLinesPerLanguage.historic  Friendly name : Historic commentLinesPerLanguage  Historic version of : trans.rascal.comments.commentLinesPerLanguage   Number of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream.   Depends-on :  trans.rascal.comments.commentLinesPerLanguage  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.comments.commentLinesPerLanguage.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentscommentedoutcodeperlanguagehistoric",
            "text": "Short name : commentedOutCodePerLanguage.historic  Friendly name : Historic commentedOutCodePerLanguage  Historic version of : trans.rascal.comments.commentedOutCodePerLanguage   Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how\nmuch source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator.   Depends-on :  trans.rascal.comments.commentedOutCodePerLanguage  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.comments.commentedOutCodePerLanguage.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentsheaderpercentagehistoric",
            "text": "Short name : headerPercentage.historic  Friendly name : Historic headerPercentage  Historic version of : trans.rascal.comments.headerPercentage   Percentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling.   Depends-on :  trans.rascal.comments.headerPercentage  Returns :  real   Back to top",
            "title": "trans.rascal.comments.headerPercentage.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascallocgenericlocoverfileshistoric",
            "text": "Short name : giniLOCOverFiles.historic  Friendly name : Historic giniLOCOverFiles  Historic version of : trans.rascal.LOC.genericLOCoverFiles   We find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality.   Depends-on :  trans.rascal.LOC.genericLOCoverFiles  Returns :  real   Back to top",
            "title": "trans.rascal.LOC.genericLOCoverFiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalloclocperlanguagehistoric",
            "text": "Short name : locPerLanguage.historic  Friendly name : Historic locPerLanguage  Historic version of : trans.rascal.LOC.locPerLanguage   Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language.\nThe metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written.   Depends-on :  trans.rascal.LOC.locPerLanguage  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.LOC.locPerLanguage.historic"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-java-code",
            "text": "These metrics are related to the Java source code of analyzed projects.  Back to top",
            "title": "Historic Metric Providers for Java code"
        },
        {
            "location": "/user-guide/metrics/#stylefileswitherrorpronenesshistoric",
            "text": "Short name : filesWithErrorProneness.historic  Friendly name : Historic filesWithErrorProneness  Historic version of : style.filesWithErrorProneness   Percentage of files with error proneness   Depends-on :  style.filesWithErrorProneness  Returns :  int   Back to top",
            "title": "style.filesWithErrorProneness.historic"
        },
        {
            "location": "/user-guide/metrics/#stylefileswithunderstandabilityissueshistoric",
            "text": "Short name : filesWithUnderstandabilityIssues.historic  Friendly name : Historic filesWithUnderstandabilityIssues  Historic version of : style.filesWithUnderstandabilityIssues   Percentage of files with understandability issues. This is a basic metric which can not be easily compared between projects.   Depends-on :  style.filesWithUnderstandabilityIssues  Returns :  int   Back to top",
            "title": "style.filesWithUnderstandabilityIssues.historic"
        },
        {
            "location": "/user-guide/metrics/#stylespreadofstyleviolationshistoric",
            "text": "Short name : spreadOfStyleViolations.historic  Friendly name : Historic spreadOfStyleViolations  Historic version of : style.spreadOfStyleViolations   Between 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.spreadOfStyleViolations  Returns :  real   Back to top",
            "title": "style.spreadOfStyleViolations.historic"
        },
        {
            "location": "/user-guide/metrics/#stylefileswithinefficiencieshistoric",
            "text": "Short name : filesWithInefficiencies.historic  Friendly name : Historic filesWithInefficiencies  Historic version of : style.filesWithInefficiencies   Percentage of files with inefficiencies   Depends-on :  style.filesWithInefficiencies  Returns :  int   Back to top",
            "title": "style.filesWithInefficiencies.historic"
        },
        {
            "location": "/user-guide/metrics/#stylefileswithstyleviolationshistoric",
            "text": "Short name : filesWithStyleViolations.historic  Friendly name : Historic filesWithStyleViolations  Historic version of : style.filesWithStyleViolations   Percentage of files with style violations   Depends-on :  style.filesWithStyleViolations  Returns :  int   Back to top",
            "title": "style.filesWithStyleViolations.historic"
        },
        {
            "location": "/user-guide/metrics/#stylespreadofunderstandabilityissueshistoric",
            "text": "Short name : spreadOfUnderstandabilityIssues.historic  Friendly name : Historic spreadOfUnderstandabilityIssues  Historic version of : style.spreadOfUnderstandabilityIssues   Between 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.spreadOfUnderstandabilityIssues  Returns :  real   Back to top",
            "title": "style.spreadOfUnderstandabilityIssues.historic"
        },
        {
            "location": "/user-guide/metrics/#stylespreadofinefficiencieshistoric",
            "text": "Short name : spreadOfInefficiencies.historic  Friendly name : Historic spreadOfInefficiencies  Historic version of : style.spreadOfInefficiencies   Between 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.spreadOfInefficiencies  Returns :  real   Back to top",
            "title": "style.spreadOfInefficiencies.historic"
        },
        {
            "location": "/user-guide/metrics/#stylespreadoferrorpronenesshistoric",
            "text": "Short name : spreadOfErrorProneness.historic  Friendly name : Historic spreadOfErrorProneness  Historic version of : style.spreadOfErrorProneness   Between 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.spreadOfErrorProneness  Returns :  real   Back to top",
            "title": "style.spreadOfErrorProneness.historic"
        },
        {
            "location": "/user-guide/metrics/#rascaltestabilityjavatestoverpublicmethodshistoric",
            "text": "Short name : percentageOfTestedPublicMethods.historic  Friendly name : Historic percentageOfTestedPublicMethods  Historic version of : rascal.testability.java.TestOverPublicMethods   Number of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we\ncompute how far from the ideal situation the project is.   Depends-on :  rascal.testability.java.TestOverPublicMethods  Returns :  real   Back to top",
            "title": "rascal.testability.java.TestOverPublicMethods.historic"
        },
        {
            "location": "/user-guide/metrics/#rascaltestabilityjavanumberoftestmethodshistoric",
            "text": "Short name : numberOfTestMethods.historic  Friendly name : Historic numberOfTestMethods  Historic version of : rascal.testability.java.NumberOfTestMethods   Number of JUnit test methods   Depends-on :  rascal.testability.java.NumberOfTestMethods  Returns :  int   Back to top",
            "title": "rascal.testability.java.NumberOfTestMethods.historic"
        },
        {
            "location": "/user-guide/metrics/#rascaltestabilityjavatestcoveragehistoric",
            "text": "Short name : estimateTestCoverage.historic  Friendly name : Historic estimateTestCoverage  Historic version of : rascal.testability.java.TestCoverage   This is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate\nthis by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation,\nas compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator\nfor a lack in testing effort for the project.   Depends-on :  rascal.testability.java.TestCoverage  Returns :  real   Back to top",
            "title": "rascal.testability.java.TestCoverage.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaca-java-quartileshistoric",
            "text": "Short name : Ca_Java_Q.historic  Friendly name : Historic Ca_Java_Q  Historic version of : trans.rascal.OO.java.Ca-Java-Quartiles   Afferent coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.Ca-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.Ca-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavacf-javahistoric",
            "text": "Short name : CF_Java.historic  Friendly name : Historic CF_Java  Historic version of : trans.rascal.OO.java.CF-Java   Coupling factor (Java)   Depends-on :  trans.rascal.OO.java.CF-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.CF-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavadac-java-quartileshistoric",
            "text": "Short name : DAC_Java_Q.historic  Friendly name : Historic DAC_Java_Q  Historic version of : trans.rascal.OO.java.DAC-Java-Quartiles   Data abstraction coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.DAC-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.DAC-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavampc-java-quartileshistoric",
            "text": "Short name : MPC_Java_Q.historic  Friendly name : Historic MPC_Java_Q  Historic version of : trans.rascal.OO.java.MPC-Java-Quartiles   Message passing coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.MPC-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.MPC-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavapf-javahistoric",
            "text": "Short name : PF_Java.historic  Friendly name : Historic PF_Java  Historic version of : trans.rascal.OO.java.PF-Java   Polymorphism factor (Java)   Depends-on :  trans.rascal.OO.java.PF-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.PF-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavarfc-java-quartileshistoric",
            "text": "Short name : RFC_Java_Q.historic  Friendly name : Historic RFC_Java_Q  Historic version of : trans.rascal.OO.java.RFC-Java-Quartiles   Response for class quartiles (Java)   Depends-on :  trans.rascal.OO.java.RFC-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.RFC-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavai-java-quartileshistoric",
            "text": "Short name : I_Java_Q.historic  Friendly name : Historic I_Java_Q  Historic version of : trans.rascal.OO.java.I-Java-Quartiles   Instability quartiles (Java)   Depends-on :  trans.rascal.OO.java.I-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.I-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavamif-java-quartileshistoric",
            "text": "Short name : MIF_Java_Q.historic  Friendly name : Historic MIF_Java_Q  Historic version of : trans.rascal.OO.java.MIF-Java-Quartiles   Method inheritance factor quartiles (Java)   Depends-on :  trans.rascal.OO.java.MIF-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.MIF-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavamhf-javahistoric",
            "text": "Short name : MHF_Java.historic  Friendly name : Historic MHF_Java  Historic version of : trans.rascal.OO.java.MHF-Java   Method hiding factor (Java)   Depends-on :  trans.rascal.OO.java.MHF-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.MHF-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaahf-javahistoric",
            "text": "Short name : AHF_Java.historic  Friendly name : Historic AHF_Java  Historic version of : trans.rascal.OO.java.AHF-Java   Attribute hiding factor (Java)   Depends-on :  trans.rascal.OO.java.AHF-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.AHF-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcom-java-quartileshistoric",
            "text": "Short name : LCOM_Java_Q.historic  Friendly name : Historic LCOM_Java_Q  Historic version of : trans.rascal.OO.java.LCOM-Java-Quartiles   Lack of cohesion in methods quartiles (Java)   Depends-on :  trans.rascal.OO.java.LCOM-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.LCOM-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaa-javahistoric",
            "text": "Short name : A_Java.historic  Friendly name : Historic A_Java  Historic version of : trans.rascal.OO.java.A-Java   Abstractness (Java)   Depends-on :  trans.rascal.OO.java.A-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.A-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavadit-java-quartileshistoric",
            "text": "Short name : DIT_Java_Q.historic  Friendly name : Historic DIT_Java_Q  Historic version of : trans.rascal.OO.java.DIT-Java-Quartiles   Depth of inheritance tree quartiles (Java)   Depends-on :  trans.rascal.OO.java.DIT-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.DIT-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavatcc-java-quartileshistoric",
            "text": "Short name : TCC_Java_Q.historic  Friendly name : Historic TCC_Java_Q  Historic version of : trans.rascal.OO.java.TCC-Java-Quartiles   Tight class cohesion quartiles (Java)   Depends-on :  trans.rascal.OO.java.TCC-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.TCC-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcom4-java-quartileshistoric",
            "text": "Short name : LCOM4_Java_Q.historic  Friendly name : Historic LCOM4_Java_Q  Historic version of : trans.rascal.OO.java.LCOM4-Java-Quartiles   Lack of cohesion in methods 4 quartiles (Java)   Depends-on :  trans.rascal.OO.java.LCOM4-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.LCOM4-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavasr-javahistoric",
            "text": "Short name : SR_Java.historic  Friendly name : Historic SR_Java  Historic version of : trans.rascal.OO.java.SR-Java   Specialization ratio (Java)   Depends-on :  trans.rascal.OO.java.SR-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.SR-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaaif-java-quartileshistoric",
            "text": "Short name : AIF_Java_Q.historic  Friendly name : Historic AIF_Java_Q  Historic version of : trans.rascal.OO.java.AIF-Java-Quartiles   Attribute inheritance factor quartiles (Java)   Depends-on :  trans.rascal.OO.java.AIF-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.AIF-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanoc-java-quartileshistoric",
            "text": "Short name : NOC_Java_Q.historic  Friendly name : Historic NOC_Java_Q  Historic version of : trans.rascal.OO.java.NOC-Java-Quartiles   Number of children quartiles (Java)   Depends-on :  trans.rascal.OO.java.NOC-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.NOC-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavarr-javahistoric",
            "text": "Short name : RR_Java.historic  Friendly name : Historic RR_Java  Historic version of : trans.rascal.OO.java.RR-Java   Reuse ratio (Java)   Depends-on :  trans.rascal.OO.java.RR-Java  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.RR-Java.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcc-java-quartileshistoric",
            "text": "Short name : LCC_Java_Q.historic  Friendly name : Historic LCC_Java_Q  Historic version of : trans.rascal.OO.java.LCC-Java-Quartiles   Loose class cohesion quartiles (Java)   Depends-on :  trans.rascal.OO.java.LCC-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.LCC-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavace-java-quartileshistoric",
            "text": "Short name : Ce_Java_Q.historic  Friendly name : Historic Ce_Java_Q  Historic version of : trans.rascal.OO.java.Ce-Java-Quartiles   Efferent coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.Ce-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.Ce-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanom-java-quartileshistoric",
            "text": "Short name : NOM_Java_Q.historic  Friendly name : Historic NOM_Java_Q  Historic version of : trans.rascal.OO.java.NOM-Java-Quartiles   Number of methods quartiles (Java)   Depends-on :  trans.rascal.OO.java.NOM-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.NOM-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanoa-java-quartileshistoric",
            "text": "Short name : NOA_Java_Q.historic  Friendly name : Historic NOA_Java_Q  Historic version of : trans.rascal.OO.java.NOA-Java-Quartiles   Number of attributes quartiles (Java)   Depends-on :  trans.rascal.OO.java.NOA-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.NOA-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavacbo-java-quartileshistoric",
            "text": "Short name : CBO_Java_Q.historic  Friendly name : Historic CBO_Java_Q  Historic version of : trans.rascal.OO.java.CBO-Java-Quartiles   Coupling between objects quartiles (Java)   Depends-on :  trans.rascal.OO.java.CBO-Java-Quartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.CBO-Java-Quartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascaladvancedfeaturesjavaadvancedlanguagefeaturesjavaquartileshistoric",
            "text": "Short name : countUsesOfAdvancedLanguageFeaturesQ.historic  Friendly name : Historic countUsesOfAdvancedLanguageFeaturesQ  Historic version of : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles   Quartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values.   Depends-on :  trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalccjavacchistogramjavahistoric",
            "text": "Short name : CCHistogramJava.historic  Friendly name : Historic CCHistogramJava  Historic version of : trans.rascal.CC.java.CCHistogramJava   Number of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis.   Depends-on :  trans.rascal.CC.java.CCHistogramJava  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.CC.java.CCHistogramJava.historic"
        },
        {
            "location": "/user-guide/metrics/#transrascalccjavaccoverjavamethodshistoric",
            "text": "Short name : giniCCOverMethodsJava.historic  Friendly name : Historic giniCCOverMethodsJava  Historic version of : trans.rascal.CC.java.CCOverJavaMethods   Calculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects.   Depends-on :  trans.rascal.CC.java.CCOverJavaMethods  Returns :  real",
            "title": "trans.rascal.CC.java.CCOverJavaMethods.historic"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-osgi-dependencies",
            "text": "These metrics are related to OSGi dependencies declared in  MANIFEST.MF  files.  Back to top",
            "title": "Historic Metric Providers for OSGi Dependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosginumberosgibundledependencieshistoric",
            "text": "Short name : numberOSGiBundleDependencies.historic  Friendly name : Historic numberOSGiBundleDependencies  Historic version of : trans.rascal.dependency.osgi.numberOSGiBundleDependencies   Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).   Depends-on :  trans.rascal.dependency.osgi.numberOSGiBundleDependencies  Returns :  int",
            "title": "trans.rascal.dependency.osgi.numberOSGiBundleDependencies.historic"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-maven-dependencies",
            "text": "These metrics are related to Maven dependencies declared in  pom.xml  files.  Back to top",
            "title": "Historic Metric Providers for Maven Dependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavennumbermavendependencieshistoric",
            "text": "Short name : numberMavenDependencies.historic  Friendly name : Historic numberMavenDependencies  Historic version of : trans.rascal.dependency.maven.numberMavenDependencies   Retrieves the number of Maven dependencies.   Depends-on :  trans.rascal.dependency.maven.numberMavenDependencies  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.maven.numberMavenDependencies.historic"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-docker-dependencies",
            "text": "The following Historic Metric Provider is associated with Docker Dependencies  Back to top",
            "title": "Historic Metric Providers for Docker Dependencies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationdockerdependencies",
            "text": "Short name : historic.configuration.docker.dependencies  Friendly name : Number of dependencies defined in Dockerfiles per day   This metric computes the number of the dependencies that are defined in the Dockerfiles of a project per day. It also computes additional information such as the number of each version of the dependencies (image/package).   Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies   Returns  :   DockerDependenciesHistoricMetric  which contains:     Variable  Type      numberOfDockerDependencies  int    numberOfDockerPackageDependencies  int    numberOfDockerImageDependencies  int       Visualisation Output Information  :   DockerDependenciesHistoricMetric :  id   docker.dependencies  id   docker.packageDependencies  id   docker.imageDependencies     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.configuration.docker.dependencies"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-puppet-dependencies",
            "text": "The following Historic Metric Provider is associated with Puppet Dependencies  Back to top",
            "title": "Historic Metric Providers for Puppet Dependencies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationpuppetdependencies",
            "text": "Short name : historic.configuration.puppet.dependencies  Friendly name : Number of dependencies defined in Puppet manifests per day   This metric computes the number of the dependencies that are defined in the Puppet manifests of a project per day.   Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies   Returns  :   PuppetDependenciesHistoricMetric  which contains:     Variable  Type      numberOfPuppetDependencies  int       Visualisation Output Information  :   DockerDependenciesHistoricMetric :  id   puppet.dependencies     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.configuration.puppet.dependencies"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-docker-smells",
            "text": "The following Historic Metric Provider is associated with Docker Smells  Back to top",
            "title": "Historic Metric Providers for Docker Smells"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationdockersmells",
            "text": "Short name : historic.configuration.docker.smells  Friendly name : Number of smells detected in Dockerfiles per day   This metric computes the number of the smells that are detected in the Dockerfiles of a project per day. It also computes additional information such as the number of each type of the smell.   Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.docker.smells   Returns  :   DockerSmellsHistoricMetric  which contains:     Variable  Type      cumulativeNumberOfDockerSmells  int    numberOfImproperUpgradeSmells  int    numberOfUnknownPackageVersionSmells  int    numberOfUntaggedImageSmells  int    numberOfImproperSudoSmells  int    numberOfImproperCopySmells  int    numberOfImproperFromSmells  int    numberOfImproperCmdSmells  int    numberOfMeaninglessSmells  int    numberOfInvalidPortsSmells  int    numberOfImproperShellSmells  int    numberOfImproperEntrypointSmells  int    numberOfDeprecatedInstructionSmells  int       Visualisation Output Information  :   DockerDependenciesHistoricMetric :  id   docker.smells  id   docker.smells.improperUpgradeSmells  id   docker.smells.unknownPackageVersionSmells  id   docker.smells.untaggedImageSmells  id   docker.smells.improperSudoSmells  id   docker.smells.improperCopySmells  id   docker.smells.improperFromSmells  id   docker.smells.improperCmdSmells  id   docker.smells.meaninglessSmells  id   docker.smells.invalidPortsSmells  id   docker.smells.improperShellSmells  id   docker.smells.improperEntrypointSmells  id   docker.smells.deprecatedInstructionSmells     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.configuration.docker.smells"
        },
        {
            "location": "/user-guide/metrics/#historic-metric-providers-for-puppet-smells",
            "text": "The following Historic Metric Providers are associated with Puppet Smells  Back to top",
            "title": "Historic Metric Providers for Puppet Smells"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationpuppetdesignsmells",
            "text": "Short name : historic.configuration.puppet.designsmells  Friendly name : Number of design smells detected in Puppet manifests per day   This metric computes the number of the design smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell.   Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells   Returns  :   PuppetDesignSmellsHistoricMetric  which contains:     Variable  Type      cumulativeNumberOfDesignSmells  int    numberOfMultifacetedSmells  int    numberOfUnnecessarySmells  int    numberOfImperativeSmells  int    numberOfMissAbSmells  int    numberOfInsufficientSmells  int    numberOfUnstructuredSmells  int    numberOfTightSmells  int    numberOfBrokenSmells  int    numberOfMissingDepSmells  int    numberOfHairballSmells  int    numberOfDeficientSmells  int    numberOfWeakenSmells  int       Visualisation Output Information  :   DockerDependenciesHistoricMetric :  id   puppet.design.smells  id   puppet.design.multifacetedSmells  id   puppet.design.unnecessarySmells  id   puppet.design.imperativeSmells  id   puppet.design.missAbSmells  id   puppet.design.insufficientSmells  id   puppet.design.unstructuredSmells  id   puppet.design.tightSmells  id   puppet.design.brokenSmells  id   puppet.design.missingDepSmells  id   puppet.design.hairballSmells  id   puppet.design.deficientSmells  id   puppet.design.weakenSmells     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.configuration.puppet.designsmells"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationpuppetimplementationsmells",
            "text": "Short name : historic.configuration.puppet.implementationsmells  Friendly name : Number of implementation smells detected in Puppet manifests per day   This metric computes the number of the implementation smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell.   Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells   Returns  :   PuppetImplementationSmellsHistoricMetric  which contains:     Variable  Type      cumulativeNumberOfImplementationSmells  int    numberOfMissingDefaultCaseSmells  int    numberOfInconsistentNamingSmells  int    numberOfDuplicateEntitySmells  int    numberOfMisplacedAttributeSmells  int    numberOfImproperAlignment  int    numberOfInvalidPropertySmells  int    numberOfImproperQuoteSmells  int    numberOfLongStatementsSmells  int    numberOfUnguardedVariableSmells  int    numberOfMissingDocSmells  int    numberOfDeprecatedStatementsSmells  int    numberOfIncompleteTasksSmells  int    numberOfComplexExpressionSmells  int    numberOfMissingElseSmells  int       Visualisation Output Information  :   DockerDependenciesHistoricMetric :  id   puppet.implementation.smells  id   puppet.implementation.missingDefaultCaseSmells  id   puppet.implementation.inconsistentNamingSmells  id   puppet.implementation.duplicateEntitySmells  id   puppet.implementation.misplacedAttributeSmells  id   puppet.implementation.improperAlignmentSmells  id   puppet.implementation.invalidPropertySmells  id   puppet.implementation.improperQuoteSmells  id   puppet.implementation.longStatementsSmells  id   puppet.implementation.unguardedVariableSmells  id   puppet.implementation.missingDocSmells  id   puppet.implementation.deprecatedStatementsSmells  id   puppet.implementation.incompleteTasksSmells  id   puppet.implementation.complexExpressionSmells  id   puppet.implementation.missingElseSmells     Back to top",
            "title": "org.eclipse.scava.metricprovider.historic.configuration.puppet.implementationsmells"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers",
            "text": "Transient metrics are used to calculate heuristics that are associated with a particular period in time, i.e. a single day. Transient Metrics are stored temporarily within the knowledge base and their output is passed as parameters in the calculation of other transient and historic metrics. Depending on the complexity, a transient metric can depend on the output from other tools, other transient metircs or have no dependencies at all.  Back to top",
            "title": "Transient Metric Providers"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-bug-trackers",
            "text": "The following Transient Metric Providers are associated with Issue trackers.  Back to top",
            "title": "Transient Metric Providers for Bug Trackers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsactiveusers",
            "text": "Short name : trans.bugs.activeusers  Friendly name : Number of users with new bug comment in the last 15 days   This metric computes the number of users that submitted new bug comments in the last 15 days, for each bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :  BugsActiveUsersTransMetric  which contains:     Variable  Type      bugs  List<BugsData>    users  List<User>       Additional Information  :   BugData :  String   bugTrackerId  int  activeUsers  int    inactiveUsers  int  previousUsers  int  users  int  days    User :  String   bugTrackerId  String   userId  String   lastActivityDate  int  comments  int  requests  int  replies     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.activeusers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsbugmetadata",
            "text": "Short name : trans.bugs.bugmetadata  Friendly name : Bug header metadata   This metric computes various metadata in bug header, i.e. priority, status, operation system and resolution. Other values computed by this metric includes average sentiment, content class and requests/replies.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification , org.eclipse.scava.metricprovider.trans.sentimentclassification ,  org.eclipse.scava.metricprovider.trans.detectingcode    Returns  :   BugsBugMetadataTransMetric  which contains:     Variable  Type      BugData  List<BugData>    CommentData  List<CommentData>       Additional Information  :    BugData :   String   bugTrackerId  String   bugId  String   status  List<String>     resolution  String   operatingSystem  String   priority  String   creationTime  String   lastClosedTime  String   startSentiment  String   endSentiment  float    averageSentiment  int      commentSum  int      sentimentSum  String   firstCommentId  String   lastCommentId     CommentData :   String   bugTrackerId  String   bugId  String     commentId  String     creationTime  String     creator  String     contentClass  String     requestReplyPrediction     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.bugmetadata"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugscomments",
            "text": "Short name : trans.bugs.comments  Friendly name : Number of bug comments   This metric computes the number of bug comments, per bug tracker.    Depends-on  :  None    Returns  :   BugsCommentsTransMetric  which contains:     Variable  Type      bugTrackerData  List<BugTrackerData>       Additional Information  :   BugTrackerData:  String   bugTrackerId  int  numberOfComments  int  cumulativeNumberOfComments     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.comments"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugscontentclasses",
            "text": "Short name : trans.bugs.contentclasses  Friendly name : Content classes in bug comments   This metric computes the frequency and percentage of content Classes in bug comments, per bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsContentClassesTransMetric  which contains:     Variable  Type      bugTrackerData  List<BugTrackerData>    contentClasses  List<ContentClass>       Additional Information  :   BugTrackerData :  String     bugTrackerId  int    numberOfComments  ContentClass :  String     bugTrackerId  String     classLabel  int    numberOfComments  float  percentage   Note  :  classLabel  could be one of the content classes shown in the hierarchical tree structure below. Where a node consists of sub-trees of children, only the child nodes are considered as  classLabel . For example, bug comments of type   1. Clarification  can be labelled as either  1.1  or  1.2 . A node without sub-trees such as  2. Suggestion of solution  is considered  classLabel  on its own.   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.contentclasses"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsdailyrequestsreplies",
            "text": "Short name : trans.bugs.dailyrequestsreplies  Friendly name : Number of bug comments, requests and replies per day   This metric computes the number of bug comments, including those regarded as requests and replies each day, per bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   BugsDailyRequestsRepliesTransMetric  which contains:     Variable  Type      dayComments  List<DayComments>       Additional Information  :   DayComments :  String   name  String   bugTrackerId  int  numberOfComments  int  numberOfRequests  int  numberOfReplies  float    percentageOfComments  float    percentageOfRequests  float    percentageOfReplies     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsemotions",
            "text": "Short name : trans.bugs.emotions  Friendly name : Emotions in bug comments   This metric computes the emotional dimensions in bug comments, per bug tracker. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise).    Depends-on  :  org.eclipse.scava.metricprovider.trans.emotionclassification    Returns  :   BugsEmotionsTransMetric  which contains:     Variable  Type      bugTrackerData  List<BugTrackerData>    dimensions  List<EmotionDimension>       Additional Information  :   BugTrackerData :  String   bugTrackerId  int  numberOfComments  int  cumulativeNumberOfComments    EmotionDimension :  String   bugTrackerId  String   emotionLabel ( anger ,  fear ,  joy ,  sadness ,  love ,  surprise )  int  numberOfComments  int  cumulativeNumberOfComments  float    percentage  float    cumulativePercentage     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.emotions"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugshourlyrequestsreplies",
            "text": "Short name : trans.bugs.hourlyrequestsreplies  Friendly name : Number of bug comments, requests and replies per hour   This metric computes the number of bug comments, including those regarded as requests and replies, every hour of the day, per bug tracker.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   BugsHourlyRequestsRepliesTransMetric  which contains:     Variable  Type      hourComments  List<HourComments>       Additional Information  :   HourComments :  String   bugTrackerId  String   hour  int  numberOfComments  int  numberOfRequests  int  numberOfReplies  float    percentageOfComments  float    percentageOfRequests  float    percentageOfReplies     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsmigrationissues",
            "text": "Short name : trans.bugs.migrationissues  Friendly name : Migration Issues Detection in Bug Trackers   This metric detects migration issues in Bug Tracking Systems.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.topics    Returns  :   BugTrackerMigrationIssueTransMetric  which contains:     Variable  Type      bugTrackerMigrationIssues  List<BugTrackerMigrationIssue>       Additional Information  :   BugTrackerMigrationIssue :  String   bugTrackerId  String   bugId  String   summary     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.migrationissues"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsmigrationissuesmaracas",
            "text": "Short name : trans.bugs.migrationissuesmaracas  Friendly name : Migration Issues Detection Maracas in Bug Trackers   This metric detects migration issues in Bug Tracking Systems along with data from Maracas.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.bugs.migrationissues ,  org.eclipse.scava.metricprovider.trans.migrationissuesmaracas ,  org.eclipse.scava.metricprovider.trans.plaintextprocessing    Returns  :   BugTrackerMigrationIssueMaracasTransMetric  which contains:     Variable  Type      bugTrackerMigrationIssuesMaracas  List<BugTrackerMigrationIssueMaracas>       Additional Information  :   BugTrackerMigrationIssueMaracas :  String   bugTrackerId  String   bugId  List<String>  changes  List<Double>  matchingPercentage     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsnewbugs",
            "text": "Short name : trans.bugs.newbugs  Friendly name : Number of new bugs   This metric computes the number of new bugs over time, per bug tracker.    Depends-on  :  None    Returns  :   BugsNewBugsTransMetric  which contains:     Variable  Type      bugTrackerData  type       Additional Information  :   BugTrackerData :  String   bugTrackerId  int  numberOfBugs  int  cumulativeNumberOfBugs     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.newbugs"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugspatches",
            "text": "Short name : trans.bugs.patches  Friendly name : Number of patches per bug   This metric computes the number of patches submitted by the community (users) for each bug.    Depends-on  :  None    Returns  :   BugsPatchesTransMetric  which contains:     Variable  Type      bugTrackerData  type       Additional Information  :   BugTrackerData :  String   bugTrackerId  int  numberOfPatches  int  cumulativeNumberOfPatches     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.patches"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsreferences",
            "text": "Short name : trans.bugs.references  Friendly name : Bugs References   This metrics search for references of commits or bugs within comments comming from bugs comments.    Depends-on  :  None    Returns  :   BugsReferenceTransMetric  which contains:     Variable  Type      bugs  List<BugReferringTo>       Additional Information  :   BugReferringTo :  String   bugTrackerId  String   bugId  String   commentId  List<String>     bugsReferred    (URLs)  List<String>     commitsReferred (URLs)     Note  :\n    When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues.  Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.references"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransbugsrequestsreplies",
            "text": "Short name : trans.bugs.requestreplies  Friendly name : Bug statistics (answered?, response time)   This metric computes for each bug, whether it was  answered. If so, it computes the time taken to respond.    Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.bugmetadata    Returns  :   BugsRequestsRepliesTransMetric  which contains:     Variable  Type      bugs  List<BugStatistics>       Additional Information  :   BugStatistics :  String   bugTrackerId  String   bugId  boolean  answered  long     responseDurationSec  String   responseDate     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.bugs.requestsreplies"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-newsgroups-and-forums",
            "text": "The following Transient Metric Providers are associated with communication channels in general, either newsgroups or forums.\nDespite the name of the metrics are newsgroups, all the metrics are valid for communication channels.  Back to top",
            "title": "Transient Metric Providers for Newsgroups and Forums"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsactiveusers",
            "text": "Short name : trans.newsgroups.activeusers  Friendly name : Number of users with new comment in the last 15 days   This metric computes the number of users that submitted news comments in the last 15 days, per newsgroup.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   NewsgroupsActiveUsersTransMetric  which contains:     Variable  Type      newsgroupData  List<NewsgroupData>    user  List<User>       Additional Information  :    NewsgroupData :   String   newsgroupName  int  activeUsers  int  inactiveUsers  int  previousUsers  int  users  int  days     User :   String   newsgroupName  String   userId  String   lastActiveDate  int  articles  int  requests  int  replies     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.activeusers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsarticles",
            "text": "Short name : trans.newsgroups.articles  Friendly name : Number of articles per newsgroup   This metric computes the number of articles, per newsgroup.    Depends-on  :  None    Returns  :   NewsgroupsArticlesTransMetric  which contains:     Variable  Type      newsgroupData  List<NewsgroupData>       Additional Information  :   NewsgroupData :  String   newsgroupName  int  numberOfArticles  int  cumulativeNumberOfArticles     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.articles"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupscontentclasses",
            "text": "Short name : trans.newsgroups.contentclasses  Friendly name : Content classes in newsgroup articles   This metric computes the content classes in newgroup articles, per newsgroup.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threads    Returns  :   NewsgroupsContentClassesTransMetric  which contains:     Variable  Type      newsGroupData  List<NewsgroupData>    contentClass  List< ContentClass>       Additional Information  :    NewsGroupData:   String   newsgroupName  int  numberOfArticles     ContentClass:   String   newsgroupName  String   classLabel  int  numberOfArticles  float    percentage     Note  :  classLabel  could be one of the content classes shown in the hierarchical tree structure below. Where a node consists of sub-trees of children, only the child nodes are considered as  classLabel . For example, articles of type   1. Clarification  can be labelled as either  1.1  or  1.2 . A node without sub-trees such as  2. Suggestion of solution  is considered  classLabel  on its own.   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.contentclasses"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsdailyrequestsreplies",
            "text": "Short name : trans.newsgroups.dailyrequestsreplies  Friendly name : Number of articles, requests and replies per day   This metric computes the number of articles, including those regarded as requests and replies for each day of the week, per newsgroup.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   NewsgroupsDailyRequestsRepliesTransMetric  which contains:     Variable  Type      dailyArticles  List<DailyArticles>       Additional Information  :   DailyArticles :  String   name  int  numberOfArticles  int  numberOfRequests  int  numberOfReplies  float    percentageOfArticles  float    percentageOfRequests  float    percentageOfReplies     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsemotions",
            "text": "Short name : trans.newsgroups.emotions  Friendly name : Emotions in newsgroup articles   This metric computes the emotional dimensions in newsgroup articles, per newsgroup. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise).    Depends-on  :  org.eclipse.scava.metricprovider.trans.emotionclassification    Returns  :   NewsgroupsEmotionsTransMetric  which contains:     Variable  Type      newsgroupData  List<NewsgroupData>    emotionDimension  List<EmotionDimension>       Additional Information  :    NewsgroupData :   String   newsgroupName  int  numberOfArticles  int  cumulativeNumberOfArticles     EmotionDimension :   String   newsgroupName  String   emotionLabel ( anger ,  fear ,  joy ,  sadness ,  love ,  surprise )  int  numberOfArticles  int  cumulativeNumberOfArticles  float    percentage  float    cumulativePercentage     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.emotions"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupshourlyrequestsreplies",
            "text": "Short name : trans.newsgroups.hourlyrequestsreplies  Friendly name : Number of articles, requests and replies per hour   This metric computes the number of articles, including those regarded as requests and replies for each hour of the day, per newsgroup.    Depends-on  :  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   NewsgroupsHourlyRequestsRepliesTransMetric  which contains:     Variable  Type      hourArticles  List<HourArticles>       Additional Information  :   HourArticles :  String   hour  int  numberOfArticles  int  numberOfRequests  int  numberOfReplies  float    percentageOfArticles  float    percentageOfRequests  float    percentageOfReplies     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.hourlyrequestsreplies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsmigrationissues",
            "text": "Short name : trans.newsgroups.migrationissues  Friendly name : Migration Issues Detection in Communication Channels   This metric detects migration issues in Communication Channels articles.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.topics ,  org.eclipse.scava.metricprovider.trans.newsgroups.threads    Returns  :   NewsgroupsMigrationIssueTransMetric  which contains:     Variable  Type      newsgroupsMigrationIssues  List<NewsgroupsMigrationIssue>       Additional Information  :   NewsgroupsMigrationIssue :  String   newsgroupName  int  threadId  long  articleId     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsmigrationissuesmaracas",
            "text": "Short name : trans.newsgroups.migrationissuesmaracas  Friendly name : Migration Issues Detection Maracas in Newsgroups   This metric detects migration issues in Newsgroups along with data from Maracas.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues ,  org.eclipse.scava.metricprovider.trans.migrationissuesmaracas ,  org.eclipse.scava.metricprovider.trans.plaintextprocessing    Returns  :   BugTrackerMigrationIssueMaracasTransMetric  which contains:     Variable  Type      newsgroupsMigrationIssuesMaracas  List<NewsgroupMigrationIssueMaracas>       Additional Information  :   NewsgroupMigrationIssueMaracas :  String   newsgrupName  int  threadId  List<String>  changes  List<Double>  matchingPercentage     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupssentiment",
            "text": "Short name : trans.newsgroups.sentiment  Friendly name : Average sentiment in newsgroup threads   The metric computes the average sentiment, including sentiment at the beginning and end of each thread, per newsgroup. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threads ,  org.eclipse.scava.metricprovider.trans.sentimentclassification    Returns  :   NewsgroupsSentimentTransMetric  which contains:     Variable  Type      threadStatistics  List<ThreadStatistics>       Additional Information  :   ThreadStatistics :  String     newsgroupName  int    threadId  float  averageSentiment  String     startSentiment  String     endSentiment   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.sentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsthreads",
            "text": "Short name : trans.newsgroups.threads  Friendly name : Assigns newsgroup articles to threads   This metric holds information for assigning newsgroup articles to threads. The threading algorithm is executed from scratch every time.    Depends-on  :  None ,    Returns  :   NewsgroupsThreadsTransMetric  which contains:     Variable  Type      articleData  List<ArticleData>    threadData  List<ThreadData>    newsgroupData  List<NewsgroupData>    currentDate  List<CurrentDate>       Additional Information  :    ArticleData :   String   newsgroupName  int  articleNumber  String   articlesId  String   date  String   from  String   subject  String   contentClass  String   references     ThreadData :   int  threadId     NewsgroupData :   String   newsgroupName  int  threads  int  previousThreads     CurrentDate :   String   date     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.threads"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsthreadsrequestsreplies",
            "text": "Short name : trans.newsgroups.threadsrequestsreplies  Friendly name : Thread statistics (answered?, response time)   The metric computes for each thread whether it is answered. If so, it computes the response time.    Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.threads ,  org.eclipse.scava.metricprovider.trans.requestreplyclassification    Returns  :   NewsgroupsThreadsRequestsRepliesTransMetric  which contains:     Variable  Type      threadStatistics  List<ThreadStatistics>       Additional Information  :   ThreadStatistics :  String   newsgroupName  int  threadId  boolean  firstRequest  boolean  answered  long     responseDurationSec  String   responseDate     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-documentation",
            "text": "The following Transient Metric Providers are associated with documentation analyses.  Back to top",
            "title": "Transient Metric Providers for Documentation"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdocumentation",
            "text": "Short name : trans.documentation  Friendly name : Documentation processing   This metric process the files returned from the documentation readers and extracts the body (in format HTML or text)    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   DocumentationTransMetric  which contains:     Variable  Type      documentationEntries  List<DocumentationEntry>    documentation  List<Documentation>       Additional Information  :    DocumentationEntry :   String   documentationId  String   entryId  String   body  String   originalFormatName  String   originalFormatMime  boolean  htmlFormatted     Documentation :   String   documentationId  List<String>     entriesId  List<String>     removedEntriesId  String   lastUpdateDate  String   lastRevisionAnalyzed  String   nextUpdateDate     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.documentation"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationclassification",
            "text": "Short name : trans.documentation.classification  Friendly name : Documentation classification   This metric determines which type of documentation is present. The possible types are:  API ,  Development ,  Installation ,  Started ,  User .    Depends-on  :  org.eclipse.scava.metricprovider.trans.documentation ,  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   DocumentationClassificationTransMetric  which contains:     Variable  Type      documentationEntriesClassification  List<DocumentationEntryClassification>       Additional Information  :   DocumentationEntryClassification :  String   documentationId  String   entryId  List<String>     types     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.documentation.classification"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationdetectingcode",
            "text": "Short name : trans.documentation.detectingcode  Friendly name : Documentation detection of code   This metric process the plain text from documentation and detects the portions corresponding to code and natural language    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.documentation.plaintext    Returns  :   DocumentationDetectingCodeTransMetric  which contains:     Variable  Type      documentationEntriesDetectingCode  List<DocumentationEntryDetectingCode>       Additional Information  :   DocumentationEntryDetectingCode :  String   documentationId  String   entryId  String   naturalLanguage  String   code     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.documentation.detectingcode"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationplaintext",
            "text": "Short name : trans.documentation.plaintext  Friendly name : Documentation plain text processor   This metric process the body of each documentation entry and extracts the plain text    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.documentation    Returns  :   DocumentationPlainTextTransMetric  which contains:     Variable  Type      documentationEntriesPlainText  List<DocumentationEntryPlainText>       Additional Information  :   DocumentationEntryPlainText :  String   documentationId  String   entryId  List<String>     plainText     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.documentation.plaintext"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationreadability",
            "text": "Short name : trans.documentation.readability  Friendly name : Documentation calculation of readability   This metric calculates the readability of each documentation entry. The higher the score, the more difficult to understand the text.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.documentation ,  org.eclipse.scava.metricprovider.trans.documentation.detectingcode    Returns  :   DocumentationReadabilityTransMetric  which contains:     Variable  Type      documentationEntriesReadability  List<DocumentationEntryReadability>       Additional Information  :   DocumentationEntryReadability :  String   documentationId  String   entryId  double   readability     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.documentation.readability"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationsentiment",
            "text": "Short name : trans.documentation.sentiment  Friendly name : Documentation Sentiment Analysis   This metric calculates the sentiment polarity of each documentation entry. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.documentation ,  org.eclipse.scava.metricprovider.trans.documentation.detectingcode    Returns  :   DocumentationSentimentTransMetric  which contains:     Variable  Type      documentationEntriesSentiment  List<DocumentationEntrySentiment>       Additional Information  :   DocumentationEntrySentiment :  String   documentationId  String   entryId  String   polarity     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.documentation.sentiment"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-natural-language-processing",
            "text": "The following Transient Metric Providers are associated with Natural Language Processing tools.  Back to top",
            "title": "Transient Metric Providers for Natural Language Processing"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransdetectingcode",
            "text": "Short name : trans.detectingcode  Friendly name : Distinguishes between code and natural language   This metric determines the parts of a bug comment or a newsgroup article that contains code or natural language.    Depends-on  :  org.eclipse.scava.metricprovider.trans.plaintextprocessing    Returns  :   DetectingCodeTransMetric  which contains:     Variable  Type      bugTrackerComments  List<BugTrackerCommentDetectingCode>    newsgroupArticles  List<NewsgroupArticleDetectingCode>    forumPosts  List<ForumPostsDetectingCode>       Additional Information  :    BugTrackerCommentDetectingCode :   String   bugTrackerId  String   bugId  String   commentId  String   naturalLanguage  String   code     NewsgroupArticleDetectingCode :   String   newsgroupName  String   articleNumber  String   naturalLanguage  String   code     ForumPostsDetectingCode :   String   forumId  String   topicId  String   postId  String   naturalLanguage  String   code     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.detectingcode"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransemotionclassification",
            "text": "Short name : trans.emotionclassification  Friendly name : Emotion classifier   This metric computes the emotions present in each bug comment, newsgroup article or forum post. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise).    Depends-on  :  org.eclipse.scava.metricprovider.trans.detectingcode    Returns  :   EmotionClassificationTransMetric  which contains:     Variable  Type      bugTrackerComments  List<BugTrackerCommentEmotionClassification>    newsgroupArticles  List<NewsgroupArticleEmotionClassification>    forumPosts  List<ForumPostsEmotionClassification>       Additional Information  :    BugTrackerCommentEmotionClassification :   String   bugTrackerId  String   bugId  String   commentId  String   emotions     NewsgroupArticleEmotionClassification :   String   newsgroupName  String   articleNumber  String   emotions     ForumPostsEmotionClassification :   String   forumId  String   topicId  String   postId  String   emotions     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.emotionclassification"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransplaintextprocessing",
            "text": "Short name : trans.plaintextprocessing  Friendly name : Plain text processing   This metric preprocess each bug comment, newsgroup article or forum post into a split plain text format.    Depends-on  :  None    Returns  :   PlainTextProcessingTransMetric  which contains:     Variable  Type      bugTrackerComments  List<BugTrackerCommentPlainTextProcessing>    newsgroupArticles  List<NewsgroupArticlePlainTextProcessing>    forumPosts  List<ForumPostsPlainTextProcessing>       Additional Information  :   BugTrackerCommentPlainTextProcessing :  String     bugTrackerId  String     bugId  String     commentId  String     plainText   boolean    hadReplies    NewsgroupArticlePlainTextProcessing :   String     newsgroupName  String     articleNumber  String     plainText   boolean    hadReplies    ForumPostsPlainTextProcessing :   String     forumId  String     topicId  String     postId  String     plainText  boolean    hadReplies   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.plaintextprocessing"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransrequestreplyclassification",
            "text": "Short name : trans.requestreplyclassification  Friendly name : Request/Reply classification   This metric computes if a bug comment, newsgroup article or forum post is a request of a reply.    Depends-on  :  org.eclipse.scava.metricprovider.trans.plaintextprocessing ,  org.eclipse.scava.metricprovider.trans.detectingcode    Returns  :   RequestReplyClassificationTransMetric  which contains:     Variable  Type      bugTrackerComments  List<BugTrackerComments>    newsgroupArticles  List<NewsgroupArticles>    forumPosts  List<ForumPosts>       Additional Information  :   BugTrackerComments :  String     bugTrackerId  String     bugId  String     commentId  String     classificationResult   String     date    NewsgroupArticles :   String     newsgroupName  String     articleNumber  String     classificationResult   String     date    ForumPosts :   String     forumId  String     topicId  String     postId  String     classificationResult  String     date   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.requestreplyclassification"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertranssentimentclassification",
            "text": "Short name : trans.sentimentclassification  Friendly name : Sentiment classification   This metric computes the sentiment of each bug comment, newsgroup article or forum post. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or 1 (positive sentiment).    Depends-on  :  org.eclipse.scava.metricprovider.trans.detectingcode    Returns  :   SentimentClassificationTransMetric  which contains:     Variable  Type      bugTrackerComments  List<BugTrackerCommentsSentimentClassification>    newsgroupArticles  List<NewsgroupArticlesSentimentClassification>    forumPosts  List<ForumPostSentimentClassification>       Additional Information  :   BugTrackerCommentsSentimentClassification :  String     bugTrackerId  String     bugId  String     commentId   String     polarity ( negative (-1) ,  neutral (0)  or  positive (1) )    NewsgroupArticlesSentimentClassification :   String     newsgroupName  String     articleNumber   String     polarity ( negative (-1) ,  neutral (0)  or  positive (1) )    ForumPostSentimentClassification :   String     forumId  String     topicId  String     postId  String     polarity ( negative (-1) ,  neutral (0)  or  positive (1) )   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.sentimentclassification"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransseverityclassification",
            "text": "Short name : trans.severityclassification  Friendly name : Severity classification   This metric computes the severity of each bug comment, newsgroup article or forum post. Severity could be blocker, critical, major, minor, enhancement,  normal). For bug comments, there is an additional severity level called  unknown . A bug severity is considered  unknown  if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse.    Depends-on  :  org.eclipse.scava.metricprovider.trans.detectingcode ,  org.eclipse.scava.metricprovider.trans.newsgroups.threads    Returns  :   SeverityClassificationTransMetric  which contains:     Variable  Type      bugTrackerBugs  List<BugTrackerBugsData>    newsgroupArticles  List<NewsgroupArticleData>    newsgroupThreads  List<NewsgroupThreadData>    forumPosts  ForumPostData>       Additional Information  :   BugTrackerBugsData :  String     bugTrackerId  String     bugId  String     severity  int    unigrams  int    bigrams  int    trigrams  int    quadgrams  int    charTrigrams  int    charQuadgrams   int    charFivegrams    NewsgroupArticleData :   String     NewsgroupName  long   articleNumber  int    unigrams  int    bigrams  int    trigrams  int    quadgrams  int    charTrigrams  int    charQuadgrams   int    charFivegrams    NewsgroupThreadData :   String     newsgroupName  int    threadId   String     severity    BugTrackerBugsData :   String     forumId  String     topicId  String     severity  int    unigrams  int    bigrams  int    trigrams  int    quadgrams  int    charTrigrams  int    charQuadgrams  int    charFivegrams   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.severityclassification"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertranstopics",
            "text": "Short name : trans.topics  Friendly name : Topic clustering   This metric computes topic clusters for each bug comment, newsgroup article or forum post in the last 30 days.    Depends-on  :  org.eclipse.scava.metricprovider.trans.detectingcode    Returns  :   TopicsTransMetric  which contains:     Variable  Type      bugTrackerComments  List<BugTrackerCommentsData>    bugTrackerTopics  List<BugTrackerTopic>    newsgroupArticles  List<NewsgroupArticlesData>    newsgroupTopics  List<NewsgroupTopic>       Additional Information  :   BugTrackerCommentsData :  String     bugTrackerId  String     bugId  String     commentId  String     subject  String     text   String     date    NewsgroupArticlesData :   String     newsgroupName  long   articleNumber  String     subject  String     text   String     date    BugTrackerTopic :   String     bugTrackerId  List<String>   labels  int    numberOfDocuments   List<String>  commentsId    NewsgroupTopic :   String     newsgroupName  List<String>   labels  int    numberOfDocuments  List<Long>  articlesId",
            "title": "org.eclipse.scava.metricprovider.trans.topics"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-commits-and-committers",
            "text": "These metrics are related to the commits and committers of a project.  Back to top",
            "title": "Transient Metric Providers for Commits and Committers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertranscommitsmessageplaintext",
            "text": "Short name : trans.commits.message.plaintext  Friendly name : Commits message plain text   This metric preprocess each commit message to get a split plain text version.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   CommitsMessagePlainTextTransMetric  which contains:     Variable  Type      commitsMessagesPlainText  List<CommitMessagePlainText>       Additional Information  :   CommitMessagePlainText :  String   repository (URL)  String   revision (Commit SHA)  List<String>     plainText     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.commits.message.plaintext"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertranscommitsmessagereferences",
            "text": "Short name : trans.commits.messagereferences  Friendly name : Commits Messages References   This metrics search for references of commits or bugs within the messages of commits. In order to detect bugs references, it is necessary to use at the same time one Bug Tracker, as the retrieval of references are based on patterns defined by bug trackers. If multiple or zero Bug Trackers are defined in the project, the metric will only search for commits (alphanumeric strings of 40 characters).    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   CommitsMessageReferenceTransMetric  which contains:     Variable  Type      commitsMessagesReferringTo  List<CommitMessageReferringTo>       Additional Information  :   BugReferringTo :  String   repository (URL)  String   revision (Commit SHA)  List<String>     bugsReferred    (URLs)  List<String>     commitsReferred (URLs)     Note  :\n    When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues.  Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.commits.messagereferences"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertranscommitsmessagetopics",
            "text": "Short name : trans.commits.message.topics  Friendly name : Commits Messages Topic Clustering   This metric computes topic clusters for each commit message.    Depends-on  :  org.eclipse.scava.metricprovider.trans.commits.message.plaintext    Returns  :   CommitsMessageTopicsTransMetric  which contains:     Variable  Type      commitsMessages  List<CommitMessage>    commitsTopics  List<CommitsTopic>       Additional Information  :    CommitMessage :   String   repository (URL)  String   revision (Commit SHA)  String   subject  String   message  Date     date     CommitsTopic :   String   repository (URL)  List<String>     labels  int  numberOfMessages  List<String>     commitsMessageId     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.commits.message.topics"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersactivecommitters",
            "text": "Short name : activeCommitters  Friendly name : Committers of last two weeks   A list of committers who have been active the last two weeks. This metric is meant for downstream processing.   Depends-on :  trans.rascal.activecommitters.committersToday  Returns :  rel[datetime,set[str]]   Back to top",
            "title": "trans.rascal.activecommitters.activeCommitters"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommittersoverfile",
            "text": "Short name : giniCommittersOverFile  Friendly name : Committers over file   Calculates the gini coefficient of committers per file   Depends-on :  trans.rascal.activecommitters.countCommittersPerFile  Returns :  real   Back to top",
            "title": "trans.rascal.activecommitters.committersoverfile"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscountcommittersperfile",
            "text": "Short name : countCommittersPerFile  Friendly name : Number of committers per file   Count the number of committers that have touched a file.   Depends-on :  trans.rascal.activecommitters.committersPerFile  Returns :  map[loc file, int numberOfCommitters]   Back to top",
            "title": "trans.rascal.activecommitters.countCommittersPerFile"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersfirstlastcommitdatesperdeveloper",
            "text": "Short name : firstLastCommitDates  Friendly name : First and last commit dates per developer   Collects per developer the first and last dates on which he or she contributed code. This basic metric is used downstream for other metrics, but\nit is also used to drill down on the membership of specific individuals of the development team.   Depends-on :  trans.rascal.activecommitters.committersToday  Returns :  map[str, tuple[datetime,datetime]]   Back to top",
            "title": "trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersdevelopmentteam",
            "text": "Short name : developmentTeam  Friendly name : Development team   Lists the names of people who have been contributing code at least once in the history of the project.   Depends-on :  trans.rascal.activecommitters.committersToday  Returns :  set[str]   Back to top",
            "title": "trans.rascal.activecommitters.developmentTeam"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterspercentageofweekendcommits",
            "text": "Short name : percentageOfWeekendCommits  Friendly name : Percentage of weekend commits   Percentage of commits made during the weekend   Depends-on :  trans.rascal.activecommitters.commitsPerWeekDay  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.percentageOfWeekendCommits"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersmaximumactivecommittersever",
            "text": "Short name : maximumActiveCommittersEver  Friendly name : Maximum active committers ever   What is the maximum number of committers who have been active together in any two week period?   Depends-on :  trans.rascal.activecommitters.numberOfActiveCommitters.historic  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.maximumActiveCommittersEver"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersdevelopmentteamemails",
            "text": "Short name : developmentTeamEmails  Friendly name : Development team   Lists the names of people who have been contributing code at least once in the history of the project.   Depends-on :  trans.rascal.activecommitters.committersEmailsToday  Returns :  set[str]   Back to top",
            "title": "trans.rascal.activecommitters.developmentTeamEmails"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersdevelopmentdomainnames",
            "text": "Short name : developmentDomainNames  Friendly name : Development team domain names   Lists the domain names of email addresses of developers if such information is present.   Depends-on :  trans.rascal.activecommitters.developmentTeamEmails  Returns :  set[str]   Back to top",
            "title": "trans.rascal.activecommitters.developmentDomainNames"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommittersperfile",
            "text": "Short name : committersPerFile  Friendly name : Committers per file   Register which committers have contributed to which files   Depends-on : -  Returns :  rel[loc,str]   Back to top",
            "title": "trans.rascal.activecommitters.committersPerFile"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterslongertermactivecommitters",
            "text": "Short name : longerTermActiveCommitters  Friendly name : Committers of last year   Committers who have been active the last 12 months. This metric is meant for downstream processing.   Depends-on :  trans.rascal.activecommitters.committersToday  Returns :  rel[datetime,set[str]]   Back to top",
            "title": "trans.rascal.activecommitters.longerTermActiveCommitters"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommitsperdeveloper",
            "text": "Short name : commitsPerDeveloper  Friendly name : Number of commits per developer   The number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits,\nwhen combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well.   Depends-on : -  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.activecommitters.commitsPerDeveloper"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommittersage",
            "text": "Short name : ageOfCommitters  Friendly name : Developer experience in project   Measures in days the amount of time between the first and last contribution of each developer.   Depends-on :  trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper  Returns :  rel[str,int]   Back to top",
            "title": "trans.rascal.activecommitters.committersAge"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommitterstoday",
            "text": "Short name : committersToday  Friendly name : Active committers   Who have been active today?   Depends-on : -  Returns :  set[str]   Back to top",
            "title": "trans.rascal.activecommitters.committersToday"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersprojectage",
            "text": "Short name : projectAge  Friendly name : Age of the project (nr of days between first and last commit)   Age of the project (nr of days between first and last commit)   Depends-on :  trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.projectAge"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommitsperweekday",
            "text": "Short name : commitsPerWeekDay  Friendly name : Commits per week day   On which day of the week do commits take place?   Depends-on : -  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.activecommitters.commitsPerWeekDay"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterscommittersemailstoday",
            "text": "Short name : committersEmailsToday  Friendly name : Active committers   Who have been active today?   Depends-on : -  Returns :  set[str]   Back to top",
            "title": "trans.rascal.activecommitters.committersEmailsToday"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommitterssizeofdevelopmentteam",
            "text": "Short name : sizeOfDevelopmentTeam  Friendly name : Size of development team   How many people have ever contributed code to this project?   Depends-on :  trans.rascal.activecommitters.developmentTeam  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.sizeOfDevelopmentTeam"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersnumberofactivecommitterslongterm",
            "text": "Short name : numberOfActiveCommittersLongTerm  Friendly name : Number of active committers long term   Number of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days.   Depends-on :  trans.rascal.activecommitters.longerTermActiveCommitters  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.numberOfActiveCommittersLongTerm"
        },
        {
            "location": "/user-guide/metrics/#transrascalactivecommittersnumberofactivecommitters",
            "text": "Short name : numberOfActiveCommitters  Friendly name : Number of active committers   Number of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days.   Depends-on :  trans.rascal.activecommitters.activeCommitters  Returns :  int   Back to top",
            "title": "trans.rascal.activecommitters.numberOfActiveCommitters"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurncommitstoday",
            "text": "Short name : commitsToday  Friendly name : Number of commits today   Counts the number of commits made today.   Depends-on : -  Returns :  int   Back to top",
            "title": "rascal.generic.churn.commitsToday"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurntoday",
            "text": "Short name : commitsToday  Friendly name : Churn of today   Counts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends.   Depends-on : -  Returns :  int   Back to top",
            "title": "rascal.generic.churn.churnToday"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnpercommitintwoweeks",
            "text": "Short name : churnPerCommitInTwoWeeks  Friendly name : Churn per commit in two weeks   The ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns.   Depends-on :  rascal.generic.churn.churnInTwoWeeks  rascal.generic.churn.commitsInTwoWeeks  Returns :  int   Back to top",
            "title": "rascal.generic.churn.churnPerCommitInTwoWeeks"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnactivity",
            "text": "Short name : churnActivity  Friendly name : Churn over the last two weeks   Churn in the last two weeks: collects the lines of code added and deleted over a 14-day sliding window.   Depends-on :  rascal.generic.churn.churnToday  Returns :  rel[datetime,int]   Back to top",
            "title": "rascal.generic.churn.churnActivity"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurncommitactivity",
            "text": "Short name : commitActivity  Friendly name : Commits in last two weeks   Number of commits in the last two weeks: collects commit activity over a 14-day sliding window.   Depends-on :  rascal.generic.churn.commitsToday  Returns :  rel[datetime,int]   Back to top",
            "title": "rascal.generic.churn.commitActivity"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurncorecommitterschurn",
            "text": "Short name : coreCommittersChurn  Friendly name : Churn per core committer   Find out about the committers what their total number of added and deleted lines for this system.   Depends-on :  rascal.generic.churn.churnPerCommitter  Returns :  map[str, int]   Back to top",
            "title": "rascal.generic.churn.coreCommittersChurn"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnfilespercommit",
            "text": "Short name : numberOfFilesPerCommit  Friendly name : Number of files per commit   Counts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "rascal.generic.churn.filesPerCommit"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnpercommit",
            "text": "Short name : churnPerCommit  Friendly name : Counts number of lines added and deleted per commit.   Count churn. Churn is the number lines added or deleted. We measure this per commit because the commit\nis a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "rascal.generic.churn.churnPerCommit"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnpercommitter",
            "text": "Short name : churnPerCommitter  Friendly name : Churn per committer   Count churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing.   Depends-on : -  Returns :  map[str author, int churn]   Back to top",
            "title": "rascal.generic.churn.churnPerCommitter"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnperfile",
            "text": "Short name : churnPerFile  Friendly name : Churn per file   Churn per file counts the number of files added and deleted for a single file. This is a basic metric to indicate hotspots in the design of the system which is changed often. This metric is used further downstream.   Depends-on : -  Returns :  map[loc file, int churn]   Back to top",
            "title": "rascal.generic.churn.churnPerFile"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurncommitsintwoweeks",
            "text": "Short name : commitsInTwoWeeks  Friendly name : Number of commits in the last two weeks   Churn in the last two weeks: aggregates the number of commits over a 14-day sliding window.   Depends-on :  rascal.generic.churn.commitActivity  Returns :  int   Back to top",
            "title": "rascal.generic.churn.commitsInTwoWeeks"
        },
        {
            "location": "/user-guide/metrics/#rascalgenericchurnchurnintwoweeks",
            "text": "Short name : churnInTwoWeeks  Friendly name : Sum of churn in the last two weeks   Churn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window.   Depends-on :  rascal.generic.churn.churnActivity  Returns :  int   Back to top",
            "title": "rascal.generic.churn.churnInTwoWeeks"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-generic-source-code",
            "text": "These metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in.  Back to top",
            "title": "Transient Metric Providers for Generic Source Code"
        },
        {
            "location": "/user-guide/metrics/#transrascalreadabilityfilereadability",
            "text": "Short name : fileReadability  Friendly name : File readability   Code readability per file, measured by use of whitespace measures deviations from common usage of whitespace in source code, such\nas spaces after commas. This is a basic collection metric which is used further downstream.   Depends-on : -  Returns :  map[loc, real]   Back to top",
            "title": "trans.rascal.readability.fileReadability"
        },
        {
            "location": "/user-guide/metrics/#transrascalreadabilityfilereadabilityquartiles",
            "text": "Short name : fileReadabilityQ  Friendly name : File readability quartiles   We measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles\nrepresent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a\nlack of attention to readability.   Depends-on :  trans.rascal.readability.fileReadability  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.readability.fileReadabilityQuartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentsheadercounts",
            "text": "Short name : headerCounts  Friendly name : Number of appearances of estimated unique headers   In principle it is expected for the files in a project to share the same license. The license text in the header of each file may differ slightly due to different copyright years and or lists of contributors. The heuristic allows for slight differences. The metric produces the number of different types of header files found. A high number is a contra-indicator, meaning either a confusing licensing scheme or the source code of many different projects is included in the code base of the analyzed system.   Depends-on : -  Returns :  list[int]   Back to top",
            "title": "trans.rascal.comments.headerCounts"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentscommentedoutcode",
            "text": "Short name : commentedOutCode  Friendly name : Lines of commented out code per file   Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how\nmuch source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.comments.commentedOutCode"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentscommentloc",
            "text": "Short name : commentLOC  Friendly name : Number of lines containing comments per file   Number of lines containing comments per file is a basic metric used for downstream processing. This metric does not consider the difference\nbetween natural language comments and commented out code.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.comments.commentLOC"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentscommentlinesperlanguage",
            "text": "Short name : commentLinesPerLanguage  Friendly name : Number of lines containing comments per language (excluding headers)   Number of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream.   Depends-on :  trans.rascal.comments.commentLOC  trans.rascal.comments.headerLOC  trans.rascal.comments.commentedOutCode  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.comments.commentLinesPerLanguage"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentscommentedoutcodeperlanguage",
            "text": "Short name : commentedOutCodePerLanguage  Friendly name : Lines of commented out code per language   Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how\nmuch source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator.   Depends-on :  trans.rascal.comments.commentedOutCode  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.comments.commentedOutCodePerLanguage"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentsheaderloc",
            "text": "Short name : headerLOC  Friendly name : Header size per file   Header size per file is a basic metric counting the size of the comment at the start of each file. It is used for further processing downstream.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.comments.headerLOC"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentsmatchinglicenses",
            "text": "Short name : matchingLicenses  Friendly name : Used licenses (from selected list of known licenses)   We match against a list of known licenses to find out which are used in the current project   Depends-on : -  Returns :  set[str]   Back to top",
            "title": "trans.rascal.comments.matchingLicenses"
        },
        {
            "location": "/user-guide/metrics/#transrascalcommentsheaderpercentage",
            "text": "Short name : headerPercentage  Friendly name : Percentage of files with headers.   Percentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling.   Depends-on :  trans.rascal.comments.headerLOC  Returns :  real   Back to top",
            "title": "trans.rascal.comments.headerPercentage"
        },
        {
            "location": "/user-guide/metrics/#transrascallocgenericloc",
            "text": "Short name : countLoc  Friendly name : Language independent physical lines of code   Physical lines of code simply counts the number of newline characters (OS independent) in a source code file.\nThe metric can be used to compare the volume between two systems.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.LOC.genericLOC"
        },
        {
            "location": "/user-guide/metrics/#transrascallocgenericlocoverfiles",
            "text": "Short name : giniLOCOverFiles  Friendly name : Spread of code over files   We find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality.   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.LOC.genericLOCoverFiles"
        },
        {
            "location": "/user-guide/metrics/#transrascalloclocperlanguage",
            "text": "Short name : locPerLanguage  Friendly name : Physical lines of code per language   Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language.\nThe metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written.   Depends-on :  trans.rascal.LOC.genericLOC  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.LOC.locPerLanguage"
        },
        {
            "location": "/user-guide/metrics/#transrascalclonesclonelocperlanguage",
            "text": "Short name : cloneLOCPerLanguage  Friendly name : Lines of code in Type I clones larger than 6 lines, per language   Lines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric.   Depends-on : -  Returns :  map[str, int]",
            "title": "trans.rascal.clones.cloneLOCPerLanguage"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-java-code",
            "text": "These metrics are related to the Java source code of analyzed projects.  Back to top",
            "title": "Transient Metric Providers for Java Code"
        },
        {
            "location": "/user-guide/metrics/#stylefileswitherrorproneness",
            "text": "Short name : filesWithErrorProneness  Friendly name : Files with style violations which make the code error prone. This is basic metric which can not be easily compared between projects.   Percentage of files with error proneness   Depends-on :  style.errorProneness  Returns :  int   Back to top",
            "title": "style.filesWithErrorProneness"
        },
        {
            "location": "/user-guide/metrics/#styleunderstandability",
            "text": "Short name : understandability  Friendly name : Inefficient code   Percentage of the projects files with coding style violations which indicate the code may be hard to read and understand,\nbut not necessarily more error prone.   Depends-on :  style.styleViolations  Returns :  Table   Back to top",
            "title": "style.understandability"
        },
        {
            "location": "/user-guide/metrics/#styleinefficiencies",
            "text": "Short name : inefficiencies  Friendly name : Inefficient code   Percentage of the projects files with coding style violations which indicate common inefficient ways of doing things in Java.   Depends-on :  style.styleViolations  Returns :  Table   Back to top",
            "title": "style.inefficiencies"
        },
        {
            "location": "/user-guide/metrics/#stylefileswithunderstandabilityissues",
            "text": "Short name : filesWithUnderstandabilityIssues  Friendly name : Files with style violations which make the code harder to understand   Percentage of files with understandability issues. This is a basic metric which can not be easily compared between projects.   Depends-on :  style.understandability  Returns :  int   Back to top",
            "title": "style.filesWithUnderstandabilityIssues"
        },
        {
            "location": "/user-guide/metrics/#styleerrorproneness",
            "text": "Short name : errorProneness  Friendly name : Error proneness   Percentage of the projects files with coding style violations which indicate error prone code. This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation.\n  Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects.\n  Other metrics further downstream do aggregate this information.   Depends-on :  style.styleViolations  Returns :  Table   Back to top",
            "title": "style.errorProneness"
        },
        {
            "location": "/user-guide/metrics/#stylespreadofstyleviolations",
            "text": "Short name : spreadOfStyleViolations  Friendly name : Spread of style violations over files   Between 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.styleViolations  Returns :  real   Back to top",
            "title": "style.spreadOfStyleViolations"
        },
        {
            "location": "/user-guide/metrics/#stylefileswithinefficiencies",
            "text": "Short name : filesWithInefficiencies  Friendly name : Files with style violations which indicate inefficiencies. This is a basic metric which can not be easily compared between projects.   Percentage of files with inefficiencies   Depends-on :  style.inefficiencies  Returns :  int   Back to top",
            "title": "style.filesWithInefficiencies"
        },
        {
            "location": "/user-guide/metrics/#stylefileswithstyleviolations",
            "text": "Short name : filesWithStyleViolations  Friendly name : Counts the number of files with any kind of style violation. This metric can not be easily compared between projects.   Percentage of files with style violations   Depends-on :  style.styleViolations  Returns :  int   Back to top",
            "title": "style.filesWithStyleViolations"
        },
        {
            "location": "/user-guide/metrics/#stylespreadofunderstandabilityissues",
            "text": "Short name : spreadOfUnderstandabilityIssues  Friendly name : Spread of understandability issues over files   Between 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.understandability  Returns :  real   Back to top",
            "title": "style.spreadOfUnderstandabilityIssues"
        },
        {
            "location": "/user-guide/metrics/#stylespreadofinefficiencies",
            "text": "Short name : spreadOfInefficiencies  Friendly name : Spread of inefficiencies over files   Between 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.inefficiencies  Returns :  real   Back to top",
            "title": "style.spreadOfInefficiencies"
        },
        {
            "location": "/user-guide/metrics/#stylestyleviolations",
            "text": "Short name : styleViolations  Friendly name : All style violations   This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation.\n  Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects.\n  Other metrics further downstream do aggregate this information.   Depends-on : -  Returns :  Table   Back to top",
            "title": "style.styleViolations"
        },
        {
            "location": "/user-guide/metrics/#stylespreadoferrorproneness",
            "text": "Short name : spreadOfErrorProneness  Friendly name : Spread of error proneness style violations over files   Between 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can\nbe compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed.   Depends-on :  style.errorProneness  Returns :  real   Back to top",
            "title": "style.spreadOfErrorProneness"
        },
        {
            "location": "/user-guide/metrics/#rascaltestabilityjavatestoverpublicmethods",
            "text": "Short name : percentageOfTestedPublicMethods  Friendly name : Number of JUnit tests averaged over the total number of public methods   Number of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we\ncompute how far from the ideal situation the project is.   Depends-on : -  Returns :  real   Back to top",
            "title": "rascal.testability.java.TestOverPublicMethods"
        },
        {
            "location": "/user-guide/metrics/#rascaltestabilityjavanumberoftestmethods",
            "text": "Short name : numberOfTestMethods  Friendly name : Number of JUnit test methods. This is an intermediate absolute metric used to compute others. The bare metric is hard to compare between projects.   Number of JUnit test methods   Depends-on : -  Returns :  int   Back to top",
            "title": "rascal.testability.java.NumberOfTestMethods"
        },
        {
            "location": "/user-guide/metrics/#rascaltestabilityjavatestcoverage",
            "text": "Short name : estimateTestCoverage  Friendly name : Static Estimation of test coverage   This is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate\nthis by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation,\nas compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator\nfor a lack in testing effort for the project.   Depends-on : -  Returns :  real   Back to top",
            "title": "rascal.testability.java.TestCoverage"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavamif-java",
            "text": "Short name : MIF_Java  Friendly name : Method inheritance factor (Java)   Method inheritance factor (Java)   Depends-on : -  Returns :  map[loc, real]   Back to top",
            "title": "trans.rascal.OO.java.MIF-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaca-java-quartiles",
            "text": "Short name : Ca_Java_Q  Friendly name : Afferent coupling quartiles (Java)   Afferent coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.Ca-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.Ca-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavadac-java",
            "text": "Short name : DAC_Java  Friendly name : Data abstraction coupling (Java)   Data abstraction coupling (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.DAC-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavacf-java",
            "text": "Short name : CF_Java  Friendly name : Coupling factor (Java)   Coupling factor (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.CF-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavai-java",
            "text": "Short name : I_Java  Friendly name : Instability (Java)   Instability (Java)   Depends-on :  trans.rascal.OO.java.Ce-Java  trans.rascal.OO.java.Ca-Java  Returns :  map[loc, real]   Back to top",
            "title": "trans.rascal.OO.java.I-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavadac-java-quartiles",
            "text": "Short name : DAC_Java_Q  Friendly name : Data abstraction coupling quartiles (Java)   Data abstraction coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.DAC-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.DAC-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavampc-java-quartiles",
            "text": "Short name : MPC_Java_Q  Friendly name : Message passing coupling quartiles (Java)   Message passing coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.MPC-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.MPC-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanom-java",
            "text": "Short name : NOM_Java  Friendly name : Number of methods (Java)   Number of methods (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.NOM-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcom-java",
            "text": "Short name : LCOM_Java  Friendly name : Lack of cohesion in methods (Java)   Lack of cohesion in methods (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.LCOM-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavacbo-java",
            "text": "Short name : CBO_Java  Friendly name : Coupling between objects (Java)   Coupling between objects (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.CBO-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavace-java",
            "text": "Short name : Ce_Java  Friendly name : Efferent coupling (Java)   Efferent coupling (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.Ce-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavapf-java",
            "text": "Short name : PF_Java  Friendly name : Polymorphism factor (Java)   Polymorphism factor (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.PF-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavarfc-java-quartiles",
            "text": "Short name : RFC_Java_Q  Friendly name : Response for class quartiles (Java)   Response for class quartiles (Java)   Depends-on :  trans.rascal.OO.java.RFC-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.RFC-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavai-java-quartiles",
            "text": "Short name : I_Java_Q  Friendly name : Instability quartiles (Java)   Instability quartiles (Java)   Depends-on :  trans.rascal.OO.java.I-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.I-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavarfc-java",
            "text": "Short name : RFC_Java  Friendly name : Response for class (Java)   Response for class (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.RFC-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcc-java",
            "text": "Short name : LCC_Java  Friendly name : Loose class cohesion (Java)   Loose class cohesion (Java)   Depends-on : -  Returns :  map[loc, real]   Back to top",
            "title": "trans.rascal.OO.java.LCC-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavamif-java-quartiles",
            "text": "Short name : MIF_Java_Q  Friendly name : Method inheritance factor quartiles (Java)   Method inheritance factor quartiles (Java)   Depends-on :  trans.rascal.OO.java.MIF-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.MIF-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavadit-java",
            "text": "Short name : DIT_Java  Friendly name : Depth of inheritance tree (Java)   Depth of inheritance tree (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.DIT-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavamhf-java",
            "text": "Short name : MHF_Java  Friendly name : Method hiding factor (Java)   Method hiding factor (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.MHF-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavatcc-java",
            "text": "Short name : TCC_Java  Friendly name : Tight class cohesion (Java)   Tight class cohesion (Java)   Depends-on : -  Returns :  map[loc, real]   Back to top",
            "title": "trans.rascal.OO.java.TCC-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaahf-java",
            "text": "Short name : AHF_Java  Friendly name : Attribute hiding factor (Java)   Attribute hiding factor (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.AHF-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcom-java-quartiles",
            "text": "Short name : LCOM_Java_Q  Friendly name : Lack of cohesion in methods quartiles (Java)   Lack of cohesion in methods quartiles (Java)   Depends-on :  trans.rascal.OO.java.LCOM-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.LCOM-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaca-java",
            "text": "Short name : Ca_Java  Friendly name : Afferent coupling (Java)   Afferent coupling (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.Ca-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaa-java",
            "text": "Short name : A_Java  Friendly name : Abstractness (Java)   Abstractness (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.A-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavadit-java-quartiles",
            "text": "Short name : DIT_Java_Q  Friendly name : Depth of inheritance tree quartiles (Java)   Depth of inheritance tree quartiles (Java)   Depends-on :  trans.rascal.OO.java.DIT-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.DIT-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavatcc-java-quartiles",
            "text": "Short name : TCC_Java_Q  Friendly name : Tight class cohesion quartiles (Java)   Tight class cohesion quartiles (Java)   Depends-on :  trans.rascal.OO.java.TCC-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.TCC-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcom4-java-quartiles",
            "text": "Short name : LCOM4_Java_Q  Friendly name : Lack of cohesion in methods 4 quartiles (Java)   Lack of cohesion in methods 4 quartiles (Java)   Depends-on :  trans.rascal.OO.java.LCOM4-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.LCOM4-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcom4-java",
            "text": "Short name : LCOM4_Java  Friendly name : Lack of cohesion in methods 4 (Java)   Lack of cohesion in methods 4 (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.LCOM4-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavasr-java",
            "text": "Short name : SR_Java  Friendly name : Specialization ratio (Java)   Specialization ratio (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.SR-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaaif-java-quartiles",
            "text": "Short name : AIF_Java_Q  Friendly name : Attribute inheritance factor quartiles (Java)   Attribute inheritance factor quartiles (Java)   Depends-on :  trans.rascal.OO.java.AIF-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.AIF-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanoc-java-quartiles",
            "text": "Short name : NOC_Java_Q  Friendly name : Number of children quartiles (Java)   Number of children quartiles (Java)   Depends-on :  trans.rascal.OO.java.NOC-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.NOC-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanoc-java",
            "text": "Short name : NOC_Java  Friendly name : Number of children (Java)   Number of children (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.NOC-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavaaif-java",
            "text": "Short name : AIF_Java  Friendly name : Attribute inheritance factor (Java)   Attribute inheritance factor (Java)   Depends-on : -  Returns :  map[loc, real]   Back to top",
            "title": "trans.rascal.OO.java.AIF-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavarr-java",
            "text": "Short name : RR_Java  Friendly name : Reuse ratio (Java)   Reuse ratio (Java)   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.OO.java.RR-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavalcc-java-quartiles",
            "text": "Short name : LCC_Java_Q  Friendly name : Loose class cohesion quartiles (Java)   Loose class cohesion quartiles (Java)   Depends-on :  trans.rascal.OO.java.LCC-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.LCC-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanoa-java",
            "text": "Short name : NOA_Java  Friendly name : Number of attributes (Java)   Number of attributes (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.NOA-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavace-java-quartiles",
            "text": "Short name : Ce_Java_Q  Friendly name : Efferent coupling quartiles (Java)   Efferent coupling quartiles (Java)   Depends-on :  trans.rascal.OO.java.Ce-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.Ce-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanom-java-quartiles",
            "text": "Short name : NOM_Java_Q  Friendly name : Number of methods quartiles (Java)   Number of methods quartiles (Java)   Depends-on :  trans.rascal.OO.java.NOM-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.NOM-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavanoa-java-quartiles",
            "text": "Short name : NOA_Java_Q  Friendly name : Number of attributes quartiles (Java)   Number of attributes quartiles (Java)   Depends-on :  trans.rascal.OO.java.NOA-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.NOA-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavacbo-java-quartiles",
            "text": "Short name : CBO_Java_Q  Friendly name : Coupling between objects quartiles (Java)   Coupling between objects quartiles (Java)   Depends-on :  trans.rascal.OO.java.CBO-Java  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.OO.java.CBO-Java-Quartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaloojavampc-java",
            "text": "Short name : MPC_Java  Friendly name : Message passing coupling (Java)   Message passing coupling (Java)   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.OO.java.MPC-Java"
        },
        {
            "location": "/user-guide/metrics/#transrascallocjavalocoverjavaclass",
            "text": "Short name : giniLOCOverClassJava  Friendly name : Distribution of physical lines of code over Java classes, interfaces and enums   The distribution of physical lines of code over Java classes, interfaces and enums explains how complexity is distributed over the design elements of a system.   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.LOC.java.LOCoverJavaClass"
        },
        {
            "location": "/user-guide/metrics/#transrascaladvancedfeaturesjavaadvancedlanguagefeaturesjavaquartiles",
            "text": "Short name : countUsesOfAdvancedLanguageFeaturesQ  Friendly name : Usage of advanced Java features quartiles   Quartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values.   Depends-on :  trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava  Returns :  map[str, real]   Back to top",
            "title": "trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles"
        },
        {
            "location": "/user-guide/metrics/#transrascaladvancedfeaturesjavaadvancedlanguagefeaturesjava",
            "text": "Short name : countUsesOfAdvancedLanguageFeatures  Friendly name : Usage of advanced Java features   Usage of advanced Java features (wildcards, union types and anonymous classes), reported per file and line number of the occurrence. This metric is for downstream processing by other metrics.   Depends-on : -  Returns :  map[loc file, int count]   Back to top",
            "title": "trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava"
        },
        {
            "location": "/user-guide/metrics/#transrascalccjavacchistogramjava",
            "text": "Short name : CCHistogramJava  Friendly name : Number of Java methods per CC risk factor   Number of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis.   Depends-on :  trans.rascal.CC.java.CCJava  Returns :  map[str, int]   Back to top",
            "title": "trans.rascal.CC.java.CCHistogramJava"
        },
        {
            "location": "/user-guide/metrics/#transrascalccjavaccoverjavamethods",
            "text": "Short name : giniCCOverMethodsJava  Friendly name : CC over Java methods   Calculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects.   Depends-on :  trans.rascal.CC.java.CCJava  Returns :  real   Back to top",
            "title": "trans.rascal.CC.java.CCOverJavaMethods"
        },
        {
            "location": "/user-guide/metrics/#transrascalccjavaccjava",
            "text": "Short name : getCC  Friendly name : McCabe's Cyclomatic Complexity Metric (Java)   Cyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases\nyou would need to test the method. A high number indicates also a lot of work to understand the method.  This metric is a basic metric for further processing downstream. It is not easily compared between projects.   Depends-on : -  Returns :  map[loc, int]   Back to top",
            "title": "trans.rascal.CC.java.CCJava"
        },
        {
            "location": "/user-guide/metrics/#transrascalccjavawmcjava",
            "text": "Short name : getWMC  Friendly name : Weighted Method Count (Java)   Cyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases\nyou would need to test the method. A high number indicates also a lot of work to understand the method. The weighted method count for a class is the sum\nof the cyclomatic complexity measures of all methods in the class. This metric is a basic metric for further processing downstream. It is not easily compared between projects.   Depends-on :  trans.rascal.CC.java.CCJava  Returns :  map[loc class, int wmcCount]   Back to top",
            "title": "trans.rascal.CC.java.WMCJava"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-osgi-dependencies",
            "text": "These metrics are related to OSGi dependencies declared in  MANIFEST.MF  files.  Back to top",
            "title": "Transient Metric Providers for OSGi Dependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencynumberrequiredpackagesinsourcecode",
            "text": "Short name : numberRequiredPackagesInSourceCode  Friendly name : Number required packages in source code   Retrieves the number of required packages found in the project source code.   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.numberRequiredPackagesInSourceCode"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiallosgipackagedependencies",
            "text": "Short name : allOSGiPackageDependencies  Friendly name : All OSGi package dependencies   Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.allOSGiPackageDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiunversionedosgirequiredbundles",
            "text": "Short name : unversionedOSGiRequiredBundles  Friendly name : Unversioned OSGi required bundles   Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header).\nIf returned value != {} there is a smell in the Manifest.   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.unversionedOSGiRequiredBundles"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiunusedosgiimportedpackages",
            "text": "Short name : unusedOSGiImportedPackages  Friendly name : Unused OSGi imported packages   Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell).   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.unusedOSGiImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosginumberosgisplitimportedpackages",
            "text": "Short name : numberOSGiSplitImportedPackages  Friendly name : Number OSGi split imported packages   Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest.   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.osgi.numberOSGiSplitImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiratiounusedosgiimportedpackages",
            "text": "Short name : ratioUnusedOSGiImportedPackages  Friendly name : Ratio of unused OSGi imported packages   Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages.   Depends-on :  trans.rascal.dependency.osgi.unusedOSGiImportedPackages  Returns :  real   Back to top",
            "title": "trans.rascal.dependency.osgi.ratioUnusedOSGiImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiallosgibundledependencies",
            "text": "Short name : allOSGiBundleDependencies  Friendly name : All OSGi bundle dependencies   Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies).   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.allOSGiBundleDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiunversionedosgiexportedpackages",
            "text": "Short name : unversionedOSGiExportedPackages  Friendly name : Unversioned OSGi exported packages   Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header).\nIf returned value != {} there is a smell in the Manifest.   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.unversionedOSGiExportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosginumberosgisplitexportedpackages",
            "text": "Short name : numberOSGiSplitExportedPackages  Friendly name : Number OSGi split exported packages   Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest.   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.osgi.numberOSGiSplitExportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiallosgidynamicimportedpackages",
            "text": "Short name : allOSGiDynamicImportedPackages  Friendly name : All OSGi dynamically imported packages   Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file.   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.allOSGiDynamicImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosginumberosgibundledependencies",
            "text": "Short name : numberOSGiBundleDependencies  Friendly name : Number all OSGi bundle dependencies   Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.osgi.numberOSGiBundleDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiratiounversionedosgiimportedpackages",
            "text": "Short name : ratioUnversionedOSGiImportedPackages  Friendly name : Ratio unversioned OSGi imported packages   Retrieves the ratio of unversioned OSGi imported packages.   Depends-on :  trans.rascal.dependency.osgi.unversionedOSGiImportedPackages  Returns :  real   Back to top",
            "title": "trans.rascal.dependency.osgi.ratioUnversionedOSGiImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiunversionedosgiimportedpackages",
            "text": "Short name : unversionedOSGiImportedPackages  Friendly name : Unversioned OSGi imported packages   Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header).\nIf returned value != {} there is a smell in the Manifest.   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.unversionedOSGiImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosginumberosgipackagedependencies",
            "text": "Short name : numberOSGiPackageDependencies  Friendly name : Number of all OSGi package dependencies   Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.osgi.numberOSGiPackageDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiratiounversionedosgirequiredbundles",
            "text": "Short name : ratioUnversionedOSGiRequiredBundles  Friendly name : Ratio unversioned OSGi required bundles   Retrieves the ratio of unversioned OSGi required bundles.   Depends-on :  trans.rascal.dependency.osgi.unversionedOSGiRequiredBundles  Returns :  real   Back to top",
            "title": "trans.rascal.dependency.osgi.ratioUnversionedOSGiRequiredBundles"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiusedosgiunimportedpackages",
            "text": "Short name : usedOSGiUnimportedPackages  Friendly name : Used OSGi unimported packages   Retrieves the set of used but unimported packages. This metric does not consider packages implicitly imported through the Bundle-Require header.\nIf set != {} then developers may be depending on the execution environment (smell).   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.osgi.usedOSGiUnimportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiratiounversionedosgiexportedpackages",
            "text": "Short name : ratioUnversionedOSGiExportedPackages  Friendly name : Ratio of unversioned OSGi exported packages   Retrieves the ratio of unversioned OSGi exported packages.   Depends-on :  trans.rascal.dependency.osgi.unversionedOSGiExportedPackages  Returns :  real   Back to top",
            "title": "trans.rascal.dependency.osgi.ratioUnversionedOSGiExportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencyosgiratiousedosgiimportedpackages",
            "text": "Short name : ratioUsedOSGiImportedPackages  Friendly name : Ratio of used OSGi imported packages   Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code.   Depends-on : -  Returns :  real   Back to top",
            "title": "trans.rascal.dependency.osgi.ratioUsedOSGiImportedPackages"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-maven-dependencies",
            "text": "These metrics are related to Maven dependencies declared in  pom.xml  files.  Back to top",
            "title": "Transient Metric Providers for Maven dependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencynumberrequiredpackagesinsourcecode_1",
            "text": "Short name : numberRequiredPackagesInSourceCode  Friendly name : Number required packages in source code   Retrieves the number of required packages found in the project source code.   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.numberRequiredPackagesInSourceCode"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavenratiooptionalmavendependencies",
            "text": "Short name : ratioOptionalMavenDependencies  Friendly name : Ratio optional Maven dependencies   Retrieves the ratio of optional Maven dependencies.   Depends-on :  trans.rascal.dependency.maven.allOptionalMavenDependencies  Returns :  real   Back to top",
            "title": "trans.rascal.dependency.maven.ratioOptionalMavenDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavennumberuniquemavendependencies",
            "text": "Short name : numberUniqueMavenDependencies  Friendly name : Number unique Maven dependencies   Retrieves the number of unique Maven dependencies.   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.maven.numberUniqueMavenDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavenalloptionalmavendependencies",
            "text": "Short name : allOptionalMavenDependencies  Friendly name : All optional Maven dependencies   Retrieves all the optional Maven dependencies.   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.maven.allOptionalMavenDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavenisusingtycho",
            "text": "Short name : isUsingTycho  Friendly name : Is using Tycho   Checks if the current project is a Tycho project.   Depends-on : -  Returns :  bool   Back to top",
            "title": "trans.rascal.dependency.maven.isUsingTycho"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavennumbermavendependencies",
            "text": "Short name : numberMavenDependencies  Friendly name : Number Maven dependencies   Retrieves the number of Maven dependencies.   Depends-on : -  Returns :  int   Back to top",
            "title": "trans.rascal.dependency.maven.numberMavenDependencies"
        },
        {
            "location": "/user-guide/metrics/#transrascaldependencymavenallmavendependencies",
            "text": "Short name : allMavenDependencies  Friendly name : All Maven dependencies   Retrieves all the Maven dependencies.   Depends-on : -  Returns :  set[loc]   Back to top",
            "title": "trans.rascal.dependency.maven.allMavenDependencies"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-docker-dependencies",
            "text": "This metric is related to Docker dependencies declared in Dockerfiles.  Back to top",
            "title": "Transient Metric Providers for Docker Dependencies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationdockerdependencies",
            "text": "Short name : trans.configuration.docker.dependencies  Friendly name : Dependencies declared in Dockerfiles   Retrieves the names of the dependencies that are declared in the Dockerfiles of a project and additional information such as their version and type.    Depends-on :  None    Returns :  DockerDependency  which contains:     Variable  Type      dependencyName  String    dependencyVersion  String    type  String    subType  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-puppet-dependencies",
            "text": "This metric is related to Puppet dependencies declared in Puppet manifests.  Back to top",
            "title": "Transient Metric Providers for Puppet Dependencies"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetdependencies",
            "text": "Short name : trans.configuration.puppet.dependencies  Friendly name : Dependencies declared in Puppet manifests   Retrieves the names of the dependencies that are declared in the Puppet manifests of a project and additional information such as their version and type.    Depends-on :  None    Returns :  PuppetDependency  which contains:     Variable  Type      dependencyName  String    dependencyVersion  String    type  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-docker-smells",
            "text": "This metric is related to Docker smells detected in Dockerfiles.  Back to top",
            "title": "Transient Metric Providers for Docker Smells"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationdockersmells",
            "text": "Short name : trans.configuration.docker.smells  Friendly name : Smells detected in Dockerfiles   Detects the smells in the Dockerfiles of a project and additional information such as their reason, the file and the line that each smells is detected.    Depends-on :  None    Returns :  Smell  which contains:     Variable  Type      smellName  String    reason  String    code  String    fileName  String    line  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.docker.smells"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-puppet-smells",
            "text": "These metrics are related to Puppet smells detected in Puppet manifests.  Back to top",
            "title": "Transient Metric Providers for Puppet Smells"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetdesignsmells",
            "text": "Short name : trans.configuration.puppet.designsmells  Friendly name : Design smells detected in Puppet manifests   Detects the design smells in the Puppet manifests of a project and additional information such as their reason and the file that each smells is detected.    Depends-on :  None    Returns :  Smell  which contains:     Variable  Type      smellName  String    reason  String    fileName  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetimplementationsmells",
            "text": "Short name : trans.configuration.puppet.implementationsmells  Friendly name : Implementation smells detected in Puppet manifests   Detects the implementation smells in the Puppet manifests of a project and additional information such as their reason, the file and the line that each smells is detected.    Depends-on :  None    Returns :  Smell  which contains:     Variable  Type      smellName  String    reason  String    fileName  String    line  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-docker-antipatterns",
            "text": "This metric is related to Docker antipatterns detected in Dockerfiles.  Back to top",
            "title": "Transient Metric Providers for Docker Antipatterns"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationdockerantipatterns",
            "text": "Short name : trans.configuration.docker.antipatterns  Friendly name : Antipatterns detected in Dockerfiles   Detects the antipatterns in the Dockerfiles of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related.    Depends-on :  None    Returns :  DockerAntipattern  which contains:     Variable  Type      smellName  String    reason  String    code  String    fileName  String    line  String    commit  String    date  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.docker.antipatterns"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-puppet-antipatterns",
            "text": "These metrics are related to Puppet antipatterns detected in Puppet manifests.  Back to top",
            "title": "Transient Metric Providers for Puppet Antipatterns"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetdesignantipatterns",
            "text": "Short name : trans.configuration.puppet.designantipatterns  Friendly name : Design antipatterns detected in Puppet manifests   Detects the design antipatterns in the Puppet manifests of a project and additional information such as their reason, the file that each antipattern is detected and the commit and date that this antipattern is related.    Depends-on :  None    Returns :  DesignAntipattern  which contains:     Variable  Type      smellName  String    reason  String    fileName  String    commit  String    date  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.puppet.designantipatterns"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetimplementationantipatterns",
            "text": "Short name : trans.configuration.puppet.implementationantipatterns  Friendly name : Implementation antipatterns detected in Puppet manifests   Detects the implementation antipatterns in the Puppet manifests of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related.    Depends-on :  None    Returns :  Smell  which contains:     Variable  Type      smellName  String    reason  String    fileName  String    line  String    commit  String    date  String    org.eclipse.scava.metricprovider.trans.configuration.projects.relations     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationantipatterns"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-indexing",
            "text": "These metrics facilitate data indexing unto the platform.  Back to top",
            "title": "Transient Metric Providers for Indexing"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-projects-relations",
            "text": "This metric is related to the relations between projects that are analysed at the platform.  Back to top",
            "title": "Transient Metric Providers for Projects Relations"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationprojectsrelations",
            "text": "Short name : trans.configuration.projects.relations  Friendly name : Relations between projects   Detects the relations between projects that are already analysed at the platform by determining if a project is used as dependency by another project.    Depends-on :  None    Returns :  ProjectRelation  which contains:     Variable  Type      relationName  String    dependencyType  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.configuration.projects.relations"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-new-versions",
            "text": "These metrics are related to the new version of the dependencies of the projects that are analysed at the platform.  Back to top",
            "title": "Transient Metric Providers for New Versions"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewversiondocker",
            "text": "Short name : trans.newversion.docker  Friendly name : New versions of Docker dependencies   Detects the new versions of dependencies of Docker based projects.    Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies    Returns :  NewDockerVersion  which contains:     Variable  Type      packageName  String    oldVersion  String    newVersion  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newversion.docker"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewversionpuppet",
            "text": "Short name : trans.newversion.puppet  Friendly name : New versions of Puppet dependencies   Detects the new versions of dependencies of Puppet based projects.    Depends-on :  org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies    Returns :  NewPuppetVersion  which contains:     Variable  Type      packageName  String    oldVersion  String    newVersion  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newversion.puppet"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewversionosgi",
            "text": "Short name : trans.newversion.osgi  Friendly name : New versions of OSGi dependencies   Detects the new versions of dependencies of OSGi based projects.    Depends-on :  None    Returns :  NewOsgiVersion  which contains:     Variable  Type      packageName  String    oldVersion  String    newVersion  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newversion.osgi"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransnewversionmaven",
            "text": "Short name : trans.newversion.maven  Friendly name : New versions of Maven dependencies   Detects the new versions of dependencies of Maven based projects.    Depends-on :  None    Returns :  NewMavenVersion  which contains:     Variable  Type      packageName  String    oldVersion  String    newVersion  String       Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.newversion.maven"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransindexingpreparation",
            "text": "Short name : index preparation transmetric  Friendly name : index preparation   This identifies the metric(s) that have been chosen to be executed by the user in preparation for indexing (note: This is required to enable the indexing capabilities of the platform to be dynamic.    Depends-on  :  None    Returns  :   IndexPrepTransMetric  which contains:     Variable  Type      executedMetricProviders  List<ExecutedMetricProviders>       Additional Information  :   ExecutedMetricProviders :  List<String>     metricIdentifiers     Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.indexing.preparation"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderindexingbugs",
            "text": "Short name : bug indexing metric  Friendly name : bug tracking system indexer   This metric prepares and indexes documents relating to bug tracking systems.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   BugsIndexingMetric    Back to top",
            "title": "org.eclipse.scava.metricprovider.indexing.bugs"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderindexingcommits",
            "text": "Short name : metricprovider.indexing.commits  Friendly name : Commits indexer   This metric prepares and indexes documents relating to commits.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   CommitsIndexingMetric    Back to top",
            "title": "org.eclipse.scava.metricprovider.indexing.commits"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderindexingcommunicationchannels",
            "text": "Short name : communication channels indexing metric  Friendly name : communication channels indexer   This metric prepares and indexes documents relating to communication channels.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation    Returns  :   CommunicationChannelsIndexingMetric    Back to top",
            "title": "org.eclipse.scava.metricprovider.indexing.communicationchannels"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricproviderindexingdocumentation",
            "text": "Short name : metricprovider.indexing.documentation  Friendly name : Documentation indexer   This metric prepares and indexes documents relating to documentation.    Depends-on  :  org.eclipse.scava.metricprovider.trans.indexing.preparation ,  org.eclipse.scava.metricprovider.trans.documentation    Returns  :   DocumentationIndexingMetric    Back to top",
            "title": "org.eclipse.scava.metricprovider.indexing.documentation"
        },
        {
            "location": "/user-guide/metrics/#transient-metric-providers-for-api",
            "text": "These transient metrics are related to the analysis and evolution of API  Back to top",
            "title": "Transient Metric Providers for API"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavametricprovidertransmigrationissuesmaracas",
            "text": "Short name : trans.migrationissuesmaracas  Friendly name : Migration Issues Detection using Maracas   This metric convert the changes found by Maracas into Regex useful for other metrics.    Depends-on  :  trans.rascal.api.changedMethods    Returns  :   MigrationIssueMaracasTransMetric  which contains:     Variable  Type      maracasMeasurements  List<MaracasMeasurement>       Additional Information  :   MaracasMeasurement :  List<String>   regex  String     change  int    lastUpdateDate   Back to top",
            "title": "org.eclipse.scava.metricprovider.trans.migrationissuesmaracas"
        },
        {
            "location": "/user-guide/metrics/#factoids",
            "text": "Factoids are plugins used to present data that has been mined and analysed using one or more historic and/or transient metric providers.  Back to top",
            "title": "Factoids"
        },
        {
            "location": "/user-guide/metrics/#factoids-for-bug-trackers",
            "text": "These factoids are related to bug tracking systems.  Back to top",
            "title": "Factoids for Bug Trackers"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugschannelusage",
            "text": "Short name : factoid.bugs.channelusage  Friendly name : Bug Tracker Usage data   This plugin generates the factoid regarding usage data for bug trackers. For example, the total number of new bugs, comments or patches per year.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.newbugs ,  org.eclipse.scava.metricprovider.historic.bugs.comments ,  org.eclipse.scava.metricprovider.historic.bugs.patches   Additional Information  :    Star rating information :   4 star    number of bugs or comments  >  working days in a year (250) .  3 star    2  x  number of bugs or comments  >  working days in a year (250) .  2 star    4  x  number of bugs or comments  >  working days in a year (250) .  1 star    otherwise     Note :   If the analyses is not up to a year ( 365  days), the proportion of working days is used. We consider that  250  working days exist within a year.     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.channelusage"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsemotion",
            "text": "Short name : factoid.bugs.emotion  Friendly name : Bug Tracker Emotions   This plugin generates the factoid regarding emotions for bug trackers. For example, the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.newbugs ,  org.eclipse.scava.metricprovider.historic.bugs.comments ,  org.eclipse.scava.metricprovider.historic.bugs.patches   Additional Information  :   Star rating information :  4 star    positive emotion percentage  >  80  OR  negative emotion percentage  <  35 .  3 star    positive emotion percentage  >  65  OR  negative emotion percentage  <  50 .  2 star    positive emotion percentage  >  50  OR  negative emotion percentage  <  65 .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.emotion"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugshourly",
            "text": "Short name : factoid.bugs.hourly  Friendly name : Bug Tracker hourly data   This plugin generates the factoid regarding hourly statistics for bug trackers. For example, the percentage of bugs, comments etc.   Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies   Additional Information  :   Star rating information :  4 star    maximum percentage of hourly comments  >  2  x  uniform percentage of comments per hour (100/24) .  3 star    maximum percentage of hourly comments  >  4  x  uniform percentage of comments per hour (100/24) .  2 star    maximum percentage of hourly comments  >  6  x  uniform percentage of comments per hour (100/24) .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.hourly"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsresponsetime",
            "text": "Short name : factoid.bugs.responsetime  Friendly name : Bug Tracker Response Time   This plugin generates the factoid regarding response time for bug trackers. This could be a cummulative average, yearly average etc.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.responsetime   Additional Information  :   Star rating information :  4 star    Zero(0)  <  yearly average response time  <  eight hours milliseconds (8 x 60 x 60 x 1000) .  3 star    Zero(0)  <  yearly average response time  <  day milliseconds (3 x eight hour milliseconds) .  2 star    Zero(0)  <  yearly average response time  <  week milliseconds (7 x week milliseconds) .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.responsetime"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugssentiment",
            "text": "Short name : factoid.bugs.sentiment  Friendly name : Bug Tracker Sentiment   This plugin generates the factoid regarding sentiment for bug trackers. For example, the average sentiment in all bug trackers associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.sentiment   Additional Information  :   Star rating information :  4 star    average sentiment  >  0.5  OR  thread end sentiment  -  thread begining sentiment  >  0.25  AND  thread begining sentiment  >  0.15 .  3 star    average sentiment  >  0.25  OR  thread end sentiment  -  thread begining sentiment  >  0.125  AND  thread begining sentiment  >  0.0 .  2 star    average sentiment  >  0  OR  thread end sentiment  -  thread begining sentiment  >  0 .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.sentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsseverity",
            "text": "Short name : factoid.bugs.severity  Friendly name : Bug Tracker Severity   This plugin generates the factoid regarding severity for bug trackers. For example, the number of bugs per severity level, the average sentiment for each severity etc. There are 8 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered  unknown  if there is not enough information for the classifier to make a decision. Also,   blocker ,  critical  and  major  are regarded as serious bugs.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.severity ,  org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus ,  org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime ,  org.eclipse.scava.metricprovider.historic.bugs.severitysentiment   Additional Information  :   Star rating information :  1 star      percentage of serious bugs  >  50 .  2 star      percentage of serious bugs  >  25 .  3 star      percentage of serious bugs  >  12.5 .  4 star      otherwise  (i.e., fewer percentage of serious bugs).   Back to top",
            "title": "org.eclipse.scava.factoid.bugs.severity"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugssize",
            "text": "Short name : factoid.bugs.size  Friendly name : Bug Tracker Size   This plugin generates the factoid regarding bug size for bug trackers. For example, the cumulative number of bug comments or patches.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.newbugs ,  org.eclipse.scava.metricprovider.historic.bugs.comments ,  org.eclipse.scava.metricprovider.historic.bugs.patches   Additional Information  :   Star rating information :  4 star    number of bugs or parches  >  1000  OR  number of comments  >  10000 .  3 star    2  x  number of bugs or parches  >  1000  OR  2  x  number of comments  >  10000 .  2 star    4  x  number of bugs or parches  >  1000  OR  4  x  number of comments  >  10000 .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.size"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsstatus",
            "text": "Short name : factoid.bugs.status  Friendly name : Bug Tracker Status   This plugin generates the factoid regarding bug status for bug trackers. For example, the number of fixed bugs, duplicate bugs etc. There are 7 bug status labels (resolved, nonResolved, fixed, worksForMe, wontFix, invalid and duplicate).   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.status   Additional Information  :   Star rating information :  4 star    perventage of resolved bug  >  75 .  3 star    perventage of resolved bug  >  50 .  2 star    perventage of resolved bug  >  25 .  1 star    otherwise  (i.e., very few resolved bugs)     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.status"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsthreadlength",
            "text": "Short name : factoid.bugs.threadlength  Friendly name : Bug Tracker Thread Length   This plugin generates the factoid regarding bug thread length for bug trackers. For example, the average length of discussion associated to bugs.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.bugs   Additional Information  :   Star rating information :  4 star    Zero(0)  <  average comments  <  5 .  3 star    Zero(0)  <  average comments  <  10 .  2 star    Zero(0)  <  average comments  <  20 .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.threadlength"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsusers",
            "text": "Short name : factoid.bugs.users  Friendly name : Bug Tracker Users   This plugin generates the factoid regarding users for bug trackers. For example, the average number of users associated to a project in a bug tracking system.   Depends-on  :  org.eclipse.scava.metricprovider.historic.bugs.users ,  org.eclipse.scava.metricprovider.historic.bugs.bugs   Additional Information  :   Star rating information :  4 star    daily new users in last month  >  8  x  0.25  OR  daily active users in last month  >  8  x  2.5  OR  daily new users in last year  >  4  x  0.25  OR daily active users in last year  >  4  x  2.5*.  3 star    daily new users in last month  >  4  x  0.25  OR  daily active users in last month  >  4  x  2.5  OR  daily new users in last year  >  2  x  0.25  OR daily active users in last year  >  2  x  2.5*.  2 star    daily new users in last month  >  2  x  0.25  OR  daily active users in last month  >  2  x  2.5  OR  daily new users in last year  >  0.25  OR  daily active users in last year  >  2.5 .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.users"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidbugsweekly",
            "text": "Short name : factoid.bugs.weekly  Friendly name : Bug Tracker Weekly   This plugin generates the factoid regarding weekly user engagements for bug trackers. For example, the average number of bug comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project.   Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies   Additional Information  :   Star rating information :  4 star    maximum percentage of weekly comments  <  2  x  uniform percentage of comments per week (100/7) .  3 star    maximum percentage of weekly comments  <  3  x  uniform percentage of comments per week (100/7) .  2 star    maximum percentage of weekly comments  <  4  x  uniform percentage of comments per week (100/7) .  1 star    otherwise     Back to top",
            "title": "org.eclipse.scava.factoid.bugs.weekly"
        },
        {
            "location": "/user-guide/metrics/#factoids-for-newsgroups-and-forums",
            "text": "These factoids are related to communication channels.  Back to top",
            "title": "Factoids for Newsgroups and Forums"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupschannelusage",
            "text": "Short name : factoid.newsgroups.channelusage  Friendly name : Newsgroup Channel Usage   This plugin generates the factoid regarding usage data for newsgroups. For example, the total number of new articles or threads per year.   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.articles ,  org.eclipse.scava.metricprovider.historic.newsgroups.newthreads   Additional Information  :   Star rating information :  4 star      number of articles  OR  threads  >  working days in a year (250 ).  3 star      2  x  number of articles  OR  threads  >  working days in a year (250) .  2 star      4  x  number of articles  OR  threads  >  working days in a year (250) .   1 star      otherwise    Note :   If the analyses is not up to a year ( 365  days), the proportion of working days is used. We consider that  250  working days exist within a year.     Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.channelusage"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsemotion",
            "text": "Short name : factoid.newsgroups.emotion  Friendly name : Newsgroup Channel Emotion   This plugin generates the factoid regarding emotions for newsgroups, such as the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive.   Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.emotions   Additional Information  :   Star rating information :  4 star      positive emotion percentage  >  80  OR  negative emotion percentage  <  35 .  3 star      positive emotion percentage  >  65  OR  negative emotion percentage  <  50 .  2 star      positive emotion percentage  >  50  OR  negative emotion percentage  <  65 .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.emotion"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupshourly",
            "text": "Short name : factoid.newsgroups.hourly  Friendly name : Newsgroup Channel hourly data   This plugin generates the factoid regarding hourly data for newsgroups, such as the percentage of articles, threads etc.   Depends-on  :  org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies   Additional Information  :   Star rating information :  4 star      maximum percentage of hourly articles  >  2  x  uniform percentage of articles per hour (100/24) .  3 star      maximum percentage of hourly articles  >  4  x  uniform percentage of articles per hour (100/24) .  2 star      maximum percentage of hourly articles  >  6  x  uniform percentage of articles per hour (100/24) .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.hourly"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsresponsetime",
            "text": "Short name : factoid.newsgroups.responsetime  Friendly name : Newsgroup Channel Response Time   This plugin generates the factoid regarding response time for newsgroups. This could be a cummulative average, yearly average etc.   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.responsetime   Additional Information  :   Star rating information :  4 star      Zero(0)  <  yearly average response time  <  eight hours in milliseconds (8 x 60 x 60 x 1000) .  3 star      Zero(0)  <  yearly average response time  <  day in milliseconds (3 x eight hours in milliseconds) .  2 star      Zero(0)  <  yearly average response time  <  week in milliseconds (7 x day in milliseconds) .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.responsetime"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupssentiment",
            "text": "Short name : factoid.newsgroups.sentiment  Friendly name : Newsgroup Channel Sentiment   This plugin generates the factoid regarding sentiments for newsgroups. For example, the average sentiment in all newsgroup channel associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment)   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.sentiment   Additional Information  :   Star rating information :  4 star      average sentiment  >  0.5  OR  thread end sentiment  -  thread begining sentiment  >  0.25  AND  thread begining sentiment  >  0.15 .  3 star      average sentiment  >  0.25  OR  thread end sentiment  -  thread begining sentiment  >  0.125  AND  thread begining sentiment  >  0.0 .  2 star      average sentiment  >  0  OR  thread end sentiment  -  thread begining sentiment  >  0 .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.sentiment"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsseverity",
            "text": "Short name : factoid.newsgroups.severity  Friendly name : Newsgroup Channel Severity   This plugin generates the factoid regarding severity for newsgroups. For example, the number of articles per severity level, the average sentiment for each severity etc. There are 7 severity  levels (blocker, critical, major, minor, enhancement, normal, trivial). Note:   blocker ,  critical  and  major  are regarded as serious bugs.   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime ,  org.eclipse.scava.metricprovider.historic.newsgroups.severity ,  org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment   Additional Information  :   Star rating information :  1 star      percentage of serious bugs  >  50 .  2 star      percentage of serious bugs  >  25 .  3 star      percentage of serious bugs  >  12.5 .  4 star      otherwise  (i.e., fewer percentage of serious bugs).   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.severity"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupssize",
            "text": "Short name : factoid.newsgroups.size  Friendly name : Newsgroup Channel Size   This plugin generates the factoid regarding thread or article size for newsgroups. For example, the cummulative number of threads.   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.articles ,  org.eclipse.scava.metricprovider.historic.newsgroups.newthreads   Additional Information  :   Star rating information :  4 star      number of threads  >  1000  OR  number of articles  >  10000 .  3 star      2  x  number of threads  >  1000  OR  2  x  number of articles  >  10000 .  2 star      4  x  number of threads  >  1000  OR  4  x  number of articles  >  10000 .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.size"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsstatus",
            "text": "Short name : factoid.newsgroups.status  Friendly name : Newsgroup Channel Status   This plugin generates the factoid regarding thread or article status for newsgroups. For example, the number of requests and replies, unanswered threads etc.     Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads ,  org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies ,  org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average   Additional Information  :   Star rating information :  4 star      perventage of replies  >  75 .  3 star      perventage of replies  >  50 .  2 star      perventage of replies  >  25 .  1 star      otherwise  (i.e., very few replies)   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.status"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsthreadlength",
            "text": "Short name : factoid.newsgroups.threadlength  Friendly name : Newsgroup Channel Thread Length   This plugin generates the factoid regarding thread length for newsgroups. For example, the average length of discussion per day, month etc.   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.threads   Additional Information  :   Star rating information :  4 star      Zero(0)  <  average comments  <  5 .  3 star      Zero(0)  <  average comments  <  10 .  2 star      Zero(0)  <  average comments  <  20 .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.threadlength"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsusers",
            "text": "Short name : factoid.newsgroups.users  Friendly name : Newsgroup Channel Users   This plugin generates the factoid regarding users for newsgroups. For example, the average number of users associated to a project in a newsgroup channel.   Depends-on  :  org.eclipse.scava.metricprovider.historic.newsgroups.users ,  org.eclipse.scava.metricprovider.historic.newsgroups.threads   Additional Information  :   Star rating information :  4 star      daily new users in last month  >  8  x  0.25  OR  daily active users in last month  >  8  x  2.5  OR  daily new users in last year  >  4  x  0.25  OR  daily active users in last year  >  4  x  2.5 .  3 star      daily new users in last month  >  4  x  0.25  OR  daily active users in last month  >  4  x  2.5  OR  daily new users in last year  >  2  x  0.25  OR  daily active users in last year  >  2  x  2.5 .  2 star      daily new users in last month  >  2  x  0.25  OR  daily active users in last month > 2 x 2.5  OR  daily new users in last year  >  0.25  OR  daily active users in last year  >  2.5 .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.users"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoidnewsgroupsweekly",
            "text": "Short name : factoid.newsgroups.weekly  Friendly name : Newsgroup Channel Weekly   This plugin generates the factoid regarding weekly user engagement for newsgroups. For example, the average number of comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project.   Depends-on  :  org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies   Additional Information  :   Star rating information :  4 star      maximum percentage of weekly articles  <  2  x  uniform percentage of articles per week (100/7) .  3 star      maximum percentage of weekly articles  <  3  x  uniform percentage of articles per week (100/7) .  2 star      maximum percentage of weekly articles  <  4  x  uniform percentage of articles per week (100/7) .  1 star      otherwise   Back to top",
            "title": "org.eclipse.scava.factoid.newsgroups.weekly"
        },
        {
            "location": "/user-guide/metrics/#factoids-for-documentation",
            "text": "These factoids are related to documentation.  Back to top",
            "title": "Factoids for Documentation"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoiddocumentationentries",
            "text": "Short name : factoid.documentation.entries  Friendly name : Documentation Entries   This plugin generates the factoid regarding which sections have been found and which are missing in the documentation. This can help understanding which sections should be added or better indicated to have a better documentation.   Depends-on  :  org.eclipse.scava.metricprovider.trans.documentation.classification   Additional Information  :   Star rating information :  4 star      All five types of documentation sections have been found .  3 star      Three or four types of documentation sections have been found .  2 star      Two or one types of documentation sections have been found .  1 star      No sections have been found   Back to top",
            "title": "org.eclipse.scava.factoid.documentation.entries"
        },
        {
            "location": "/user-guide/metrics/#orgeclipsescavafactoiddocumentationsentiment",
            "text": "Short name : factoid.documentation.sentiment  Friendly name : Documentation Sentiment   This plugin generates the factoid regarding sentiment for documentation.   Depends-on  :  org.eclipse.scava.metricprovider.historic.documentation.sentiment.DocumentationSentimentHistoricMetricProvider   Additional Information  :   Star rating information :  4 star      Average sentiment greater than 0.5 .  3 star      Average sentiment between 0.25 and 0.5 .  2 star      Average sentiment between 0 and 0.25 .  1 star      Average sentiment equal or lesser than 0   Back to top",
            "title": "org.eclipse.scava.factoid.documentation.sentiment"
        },
        {
            "location": "/user-guide/plugin/",
            "text": "Eclipse Plugin",
            "title": "Home"
        },
        {
            "location": "/user-guide/plugin/#eclipse-plugin",
            "text": "",
            "title": "Eclipse Plugin"
        },
        {
            "location": "/user-guide/plugin/getting-started/",
            "text": "Install the CROSSMINER plug-in\n\n\nInstall the latest version of the CROSSMINER plug-in from the update site:\n\n\nhttp://ci5.castalia.camp:8080/job/scava-ide/job/dev/lastSuccessfulBuild/artifact/eclipse-based-ide/org.eclipse.scava.root/releng/org.eclipse.scava.update/target/repository\n\n\nConfiguration\n\n\n\n\nOnce the plug-in is installed and the workbench restarted, open the Eclipse Preferences wizard and select the left item \nCROSSMINER > Remote settings\n\n\nFor the knowledge base server specify either  http://83.212.75.210 or http://scava-dev.ow2.org\n\n\nFor the port, for both servers, indicate: 8080\n\n\n\n\n\n\nGetting started scenario\n\n\nA professor requests his students to extract the current weather for a given city from the \nOpenWeather\n \nhttps://openweathermap.org\n open data site. The data provided are in a JSON format. For example, for \"Paris, FR\"\n\nhttp://api.openweathermap.org/data/2.5/weather?q=Paris&appid=abbcea2020f75409af198b98de40e3a6\n.\n\n\nTask 0: Import a library\n\n\nLook for a JSON parser library\n\n\nLook for using CROSSMINER > Project search > Find JSON parser projects.\n\n\n\nThe developer can choose how to import the selected library in the project that he is implementing once a possible library has been selected. We presented two ways to import a library as:\n- an Eclipse project and\n- a Maven dependency.\n\n\nWe suggest importing the libraries as maven dependencies because it enables more recommendations. For instance, a recommender provides the list of update library. Moreover, the recommender system suggests useful missing libraries.\n\n\nImport it as an Eclipse project\n\n\nSelect the json-simple projects and clone it.\n\n\nPlease manually remove \u201c\\\u201d from the project folder name after it has been cloned. FEA is going to push a fix that solves this issue.\n\n\n\n\n\n\n\n\n\n\nAdd the downloaded project into the classpath.\n\n\n\n\n\n\nImport it as a Maven dependency\n\n\nAn interested developer can discover that JSON-simple is available as a Maven dependency by inspecting the GitHub pages.\n\n\n\n\n\n\nIn this case json-simple library can be imported as a Maven dependency.\n- Create a new Eclipse project named \nopenweather.json-simple\n\n- \nConfigure > Convert to Maven project\n the created project\n\n\n\n- Open the \npom.xml\n file\n- From the \nDependencies\n tab, add a new dependency on \njson-simple\n:\n\n\n\n- The \npom.xml\n file should look like that after the changes have been accepted:\n\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>openweather</groupId>\n  <artifactId>openweather.json-simple</artifactId>\n  <version>0.0.1-SNAPSHOT</version>\n  <build>\n    <sourceDirectory>src</sourceDirectory>\n    <plugins>\n      <plugin>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.8.0</version>\n        <configuration>\n          <source>1.8</source>\n          <target>1.8</target>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n  </properties>\n  <dependencies>\n    <dependency>\n        <groupId>com.googlecode.json-simple</groupId>\n        <artifactId>json-simple</artifactId>\n        <version>1.1.1</version>\n    </dependency>\n  </dependencies>\n</project>\n\n\n\nTask 1: Extract the current weather from the OpenWeather API\n\n\nLook for JSON API StackOverFlow recommendations\n\n\n\n\nCreate a Class with its main method.\n\n\n\n\nAdd the code instantiating the JSON parser\n\n\npackage openweather.json_simple;\n\n\nimport org.json.simple.parser.JSONParser;\n\n\n/\n\n * Parse Open Weather API\n \n\n \n/\npublic class App {\n    public static void main(String[] args) {\n        JSONParser parser = new JSONParser();\n    }\n}\n\n\n\n\n\n\nSelect the whole code and, in the contextual menu, ask for relevant Q&A post recommendations.\n\n\n\n\n\n\n\n\n\nThe recommendation query is based on the selected code and more particularly on:\n\n\n\n\nImport Declaration: The directives used to invoke libraries\n\n\nMethod Declaration: Method declarations with parameters\n\n\nMethod Invocation: API function calls\n\n\nVariable Type: Types of all declared variables\n\n\nVariable Dec: All declared variables\n\n\nClass Instance: Class declarations\n\n\n\n\n\n\n\n\nThe recommender provides related StackOverflow posts. The recommendations are useful to learn how to use the selected json-simple recommendation. A new view is displayed below the code with a list of related posts:\n\n\n\n\n\n\nThese posts come from a filtering (only questions with answers, only question with java code,...) and indexing of more than 18 millions StackOverFlow posts. \n\n\n\n\n\n\n\n\n\n\nThe provided links are: \n\n\n\n\nJava. Praoblem with org.json\n\n\nReading json from java application with python\n\n\nHow to access JSONArray in a JSONObject using Java?\n\n\n[how to convert stringified JSONObject[] to JSONObj[] in java](https://stackoverflow.com/questions/52753162/how-to-convert-stringified-jsonobject-to-jsonobj-in-java)\n\n\nValue of type java.lang.String cannot be converted to JSONArray\n\n\n\n\n\n\n\n\nThese different posts, e.g. \nHow to access JSONArray in a JSONObject using Java?\n, provides a good example which can help understand the JSONParser core APIs:\n\n\n\n\n\n\n\n\nAt this point the code looks like that:\n\n\npackage openweather.json_simple;\n\nimport java.io.BufferedReader;\n\nimport org.json.simple.JSONArray;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\n\n/**\n * Parse Open Weather API\n */\npublic class App {\n\n    public static void main(String[] args) throws IOException, ParseException {\n        BufferedReader reader = null;\n        // Parsing of the API result\n        JSONParser parser = new JSONParser();\n        JSONObject jsonObject = (JSONObject) parser.parse(reader);\n        // Access the JSON object\n        String cityName = (String) jsonObject.get(\"name\");\n        JSONArray array = (JSONArray) jsonObject.get(\"weather\");\n    }\n}\n\n\n\nLook for JSON API snippet code recommendations\n\n\nIf we look again for StackOverFlow recommendations to understand how to handle a JSONArray, we don\u2019t get useful post.\nSo let's try with another recommender.\n\n\n\n\nSelect the whole code and, in the contextual menu, ask for code recommendation.\n\n\n\n\n\n\n\n\nThis new feature opens a view named Code Recommendation and lists some examples of code snippets extracted from the knowledge base. \n\n\nFor the current code, we obtain one single code snippet. This snippet is quite useful, it shows a good example of how parsing the JSONArray\n\n\n\n\n\n\n\n\n\n\nBased on this, we can conclude the parsing of the JSON result by iterating across the JSONArray:\n\n\n// loop array \nJSONArray array = (JSONArray) jsonObject.get(\"weather\");\nIterator\n iterator = array.iterator(); \nJSONObject weather;\nwhile (iterator.hasNext()) { \n        weather = (JSONObject)iterator.next();\n        String description = (String)weather.get(\"description\");\n        System.out.println(cityName+\": \"+description);\n    }\nFinally, at this point, our code looks like that:\n\n\npackage openweather.json_simple;\n\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.util.Iterator;\n\n\nimport org.json.simple.JSONArray;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\nimport org.json.simple.parser.ParseException;\n\n\n/*\n\n * Parse Open Weather API\n \n/\npublic class App {\n\n\npublic static void main(String[] args) throws IOException, ParseException {\n    BufferedReader reader = null;\n    // Parsing of the API result\n    JSONParser parser = new JSONParser();\n    JSONObject jsonObject = (JSONObject) parser.parse(reader);\n    // Access the JSON object\n    String cityName = (String) jsonObject.get(\"name\");\n    // loop array \n    JSONArray array = (JSONArray) jsonObject.get(\"weather\");\n    Iterator<JSONObject> iterator = array.iterator(); \n    JSONObject weather;\n    while (iterator.hasNext()) { \n        weather = (JSONObject)iterator.next();\n        String description = (String)weather.get(\"description\");\n        System.out.println(cityName+\": \"+description);\n    }\n}\n\n\n\n}\n\n\n\n\n\n\nAccess REST API result\n\n\nIt\u2019s time to implement the getCityWeatherReader method accessing to the the Open Weather REST API.\n- Let\u2019s start with the creation of the REST API URL. Before calling any recommender, the code looks like that:\n\n\npublic class App {\n    static String OpenWeathermapAPI = \"http://api.openweathermap.org/data/2.5/weather?q={city}&appid= {appid}\";\n    static String AppId = \"abbcea2020f75409af198b98de40e3a6\";\n\n    public static void main(String[] args) throws IOException, ParseException {\n        // Creation of the REST API\n        String city = (args.length != 0)? args[0]:\"Toulouse\";\n        String restAPI = OpenWeathermapAPI.replace(\"{city}\", city);\n        restAPI = restAPI.replace(\"{appid}\", AppId);\n        URL url = new URL(restAPI);\n        // Call the  REST API\n        HttpURLConnection connection = (HttpURLConnection)url.openConnection();\n\n\n\n\n\nSelect the code for the top of the file to the openConnection call\n\n\n\n\n\n\n\n\nAgain, the StackOverFlow recommender doesn\u2019t provide pertinent post. On the other hand, the Code Recommender provides an interesting code snippet:\n\n\n\n\n\n\nBased on this, we can complete our code. Finally the code looks like that:\n\n\npackage openweather.json_simple;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\nimport java.util.Iterator;\n\nimport org.json.simple.JSONArray;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\nimport org.json.simple.parser.ParseException;\n\n\n/**\n * Parse Open Weather API\n */\npublic class App {\n    static String OpenWeathermapAPI = \"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={appid}\";\n    static String AppId = \"abbcea2020f75409af198b98de40e3a6\";\n\n    public static void main(String[] args) throws IOException, ParseException {\n        // Creation of the REST API\n        String city = (args.length != 0)? args[0]:\"Toulouse\";\n        String restAPI = OpenWeathermapAPI.replace(\"{city}\", city);\n        restAPI = restAPI.replace(\"{appid}\", AppId);\n        URL url = new URL(restAPI);\n        // Call the  REST API\n        HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n        int status = connection.getResponseCode(); \n        if (status == HttpURLConnection.HTTP_OK) { \n        BufferedReader reader = null;\n        try {\n                reader = new BufferedReader(new InputStreamReader(connection.getInputStream()));\n                // Parsing of the API result\n                JSONParser parser = new JSONParser();\n                JSONObject jsonObject = (JSONObject) parser.parse(reader);\n                // Access the JSON object\n                String cityName = (String) jsonObject.get(\"name\");\n                // loop array \n                JSONArray array = (JSONArray) jsonObject.get(\"weather\");\n                Iterator<JSONObject> iterator = array.iterator(); \n                JSONObject weather;\n                while (iterator.hasNext()) { \n                    weather = (JSONObject)iterator.next();\n                    String description = (String)weather.get(\"description\");\n                    System.out.println(cityName+\": \"+description);\n                }\n        } finally {\n            if (reader != null) reader.close();\n        }\n        }\n    }\n}\n\n\n\nIf we call the application without argument, we obtain:\n\n\nToulouse: light rain\n\n\n\nIf we call it with, let's say, Rome:\n\n\nRome: light rain\n\n\n\nand with London\n\n\nLondon: mist\n\n\n\nand Rio?\n\n\nRio: clear sky\n\n\n\nAt least we know where we should spend end of fall! ;-)\n\n\nTask 2: Migrate from an abandoned library to a better-supported one.\n\n\nOnce the task is done, the teacher asks the student to migrate from JSON-simple to Jackson-core library because the former is better supported and a better performance is guaranteed. \n\n\nSimilarities\n\n\nSimilar projects can be recommended from the Project search wizard. \n- Search for the json-simple library with the CROSSMINER Search project\n- Select the json-simple library in the list provided by the search dialog\n\n\n\n\n\nThe dialog offers the possibility to find similar projects using different methods:\n\n\n\n\n\n\n\n\nCompound Similarity\n: It is a weighted combination of Readme Similarity and Dependency Similarity\n\n\nCrossSim Similarity\n: computing similarities among all imported projects by using the star events and project dependencies;\n\n\nDependency Similarity\n: using the Jaccard distance on project dependencies to calculate the similarity between two projects;\n\n\nCrossRec Similarity\n: a lightweight version of CrossSim. It uses project dependencies to provide similarities among all imported projects;\n\n\nReadme Similarity\n: The similarity between a project p1 and a project p2 is calculated as the similarity between their readme files with the corresponding features\n\n\nRepoPalCompound Similarity\n: being inspired by the approach presented in [];\n\n\nRepoPalCompoundV2 Similarity\n: an evolved version of RepoPalCompound similarity;\n\n\n\n\n\n\nThe \nRepoPalCompound\n method provides, in this case, the most accurate result suggesting a set of libraries dealing with JSON. Select the library \njackson-databind\n.\n\n\n\nAt this point, you can either import this library as an Eclipse project (see at the previous task) or you can import it as a Maven dependency.\n\n\nImport it as an Eclipse project\n\n\nPlease look at previous task.\n\n\nImport it as a Maven dependency\n\n\nThe GitHub readme file suggests how to import Jackson-core library as a Maven dependency (the GitHub page link is available in the details view of the project). \n\n\n\n\nConvert the example against the new library\n\n\nIn this particular case, we could use the readme file of this library because it provides some good examples which can be used to convert the previous example against the Jackson-core library. However, let\u2019s assume we don\u2019t have access to this information and we need to figure it out by ourselves.\n\n\n\n\nCreate a new Eclipse project named openweather.jackson-core\n\n\nConfigure > Convert to Maven project the created project\n\n\nOpen the pom.xml file\n\n\nFrom the Dependencies tab, add a new dependency on \njackson-core\n library:\n\n\n\n\n\n\n\n\n\n\nThe pom.xml file should look like that after the changes have been accepted:\n\n\n\n  \n4.0.0\n\n  \nopenweather.jackson-core\n\n  \nopenweather.jackson-core\n\n  \n0.0.1-SNAPSHOT\n\n  \n\n    \nsrc\n\n    \n\n      \n\n        \nmaven-compiler-plugin\n\n        \n3.8.0\n\n        \n\n          \n1.8\n\n          \n1.8\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nUTF-8\n\n  \n\n  \n\n    \n\n        \ncom.fasterxml.jackson.core\n\n        \njackson-databind\n\n        \n2.10.0\n\n    \n\n  \n\n\n\n\n\n\n\n\nCreate a class with its main method. Defines the list of objects that we would like to use in the import section of the class\n\n\npackage openweather.jackson_core;\n\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\n\npublic class App {\n    public static void main(String[] args) {\n\n\n}\n\n\n\n}\n\n\n\n\n\n\nSelect the whole code and, in the contextual menu, ask for relevant Q&A post recommendations. \n\n\n\n\n\n\n\n\nThe first recommendation provides enough information to understand how to create and parse a JSON file using teh jackson-core library\n\n\n\n\nBased on this, we can initiate our code accordingly and complete it by iterating through the ArrayNode weather\n\n\npackage openweather.jackson_core;\n\nimport java.io.IOException;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\n\npublic class App {\n    static String OpenWeathermapAPI = \"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={appid}\";\n    static String AppId = \"abbcea2020f75409af198b98de40e3a6\";\n\n    public static void main(String[] args) throws IOException {\n        // Creation of the REST API\n        String city = (args.length != 0)? args[0]:\"Toulouse\";\n        String restAPI = OpenWeathermapAPI.replace(\"{city}\", city);\n        restAPI = restAPI.replace(\"{appid}\", AppId);\n        URL url = new URL(restAPI);\n        // Call the  REST API\n        ObjectMapper objectMapper = new ObjectMapper();\n        JsonNode rootNode = objectMapper.readTree(url);\n        String cityName = rootNode.get(\"name\").asText();\n        ArrayNode weather = (ArrayNode)rootNode.get(\"weather\");\n        for (JsonNode jsonNode : weather) {\n            String description = jsonNode.get(\"description\").asText();\n            System.out.println(cityName+\": \"+description);\n        }\n    }\n}\n\n\n\nOther recommendations\n\n\nLibrary versions\n\n\nThe developer can check the library releases. Foreach imported library the system provides the list of versions.\n- Select the Eclipse project to check for library release, for example the project \nopenweather.jackson-core\n\n- From the context menu, select the item \nCROSSMINER > Search updates for libraries\n\n\n\n\n\n\nThe menu item opens a dialog listing all the jackson-core library versions:\n\n\n\n\n\n\n\n\nIn our case, we already had the latest release selected, otherwise we could select a more recent one and have our pom.xml file updated.\n\n\n\n\nSuggested library\n\n\nThe KB assists open source software developers in selecting suitable third-party libraries. CrossRec feature aims at supporting software developers who have already included some libraries in the new projects being developed, and expect to get recommendations on which additional libraries should be further incorporated (if any).\n- Select the Eclipse project to check for library release, for example the project \nopenweather.jackson-core\n\n- From the context menu, select the item \nCROSSMINER > Search libraries for this project\n\n\n\n\n\n\nThe menu item opens a dialog listing the libraries that may be useful in our project because they are often used by other developers:\n\n\n\n\n\n\n\n\nAdd, for example, the \nlog4j\n logging library by clicking on the \n button\n\n\nPress the Install button\n\n\nAccept the Confirmation box\n\n\nThe selected library is inserted into the pom.xml dependency list\n\n\n\n\n\n\n\n\nUnfortunately, the \nlast.release\n keyword is not recognized anymore by Maven 3.+. It is for this reason it is shown as an error in the \npom.xml\n file\n\n\n\n\n\n\n\n\n\n\n\n\nTo fix it we must replace this keyword \nlatest.release\n by the latest release version number. \nThe portal \nhttps://mvnrepository.com/artifact/log4j/log4j\n indicates the version \n2.1.17\n\n\n\n\n\n\n\n\n\n\nAt this point, the \npom.xml\n should look like that:\n\n\n<?xml version=\"1.0\"?>\n\n\n  \n4.0.0\n\n  \nopenweather.jackson-core\n\n  \nopenweather.jackson-core\n\n  \n0.0.1-SNAPSHOT\n\n  \n\n    \nUTF-8\n\n  \n\n  \n\n    \n\n      \ncom.fasterxml.jackson.core\n\n      \njackson-databind\n\n      \n2.9.7\n\n    \n\n    \n\n      \nlog4j\n\n      \nlog4j\n\n      \n1.2.17\n\n    \n\n  \n\n  \n\n    \nsrc\n\n    \n\n      \n\n        \nmaven-compiler-plugin\n\n        \n3.8.0\n\n        \n\n          \n1.8\n\n          \n1.8\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\nNow, the developer can adapt his code to use the logging library\n\n\nSome pointers\n\n\n\n\nThe implemented tasks are available at: \nhttps://github.com/md2manoppello/CROSSMINER_Tasks",
            "title": "Home"
        },
        {
            "location": "/user-guide/plugin/getting-started/#install-the-crossminer-plug-in",
            "text": "Install the latest version of the CROSSMINER plug-in from the update site:  http://ci5.castalia.camp:8080/job/scava-ide/job/dev/lastSuccessfulBuild/artifact/eclipse-based-ide/org.eclipse.scava.root/releng/org.eclipse.scava.update/target/repository",
            "title": "Install the CROSSMINER plug-in"
        },
        {
            "location": "/user-guide/plugin/getting-started/#configuration",
            "text": "Once the plug-in is installed and the workbench restarted, open the Eclipse Preferences wizard and select the left item  CROSSMINER > Remote settings  For the knowledge base server specify either  http://83.212.75.210 or http://scava-dev.ow2.org  For the port, for both servers, indicate: 8080",
            "title": "Configuration"
        },
        {
            "location": "/user-guide/plugin/getting-started/#getting-started-scenario",
            "text": "A professor requests his students to extract the current weather for a given city from the  OpenWeather   https://openweathermap.org  open data site. The data provided are in a JSON format. For example, for \"Paris, FR\" http://api.openweathermap.org/data/2.5/weather?q=Paris&appid=abbcea2020f75409af198b98de40e3a6 .",
            "title": "Getting started scenario"
        },
        {
            "location": "/user-guide/plugin/getting-started/#task-0-import-a-library",
            "text": "",
            "title": "Task 0: Import a library"
        },
        {
            "location": "/user-guide/plugin/getting-started/#look-for-a-json-parser-library",
            "text": "Look for using CROSSMINER > Project search > Find JSON parser projects.  The developer can choose how to import the selected library in the project that he is implementing once a possible library has been selected. We presented two ways to import a library as:\n- an Eclipse project and\n- a Maven dependency.  We suggest importing the libraries as maven dependencies because it enables more recommendations. For instance, a recommender provides the list of update library. Moreover, the recommender system suggests useful missing libraries.",
            "title": "Look for a JSON parser library"
        },
        {
            "location": "/user-guide/plugin/getting-started/#import-it-as-an-eclipse-project",
            "text": "Select the json-simple projects and clone it.  Please manually remove \u201c\\\u201d from the project folder name after it has been cloned. FEA is going to push a fix that solves this issue.      Add the downloaded project into the classpath.",
            "title": "Import it as an Eclipse project"
        },
        {
            "location": "/user-guide/plugin/getting-started/#import-it-as-a-maven-dependency",
            "text": "An interested developer can discover that JSON-simple is available as a Maven dependency by inspecting the GitHub pages.    In this case json-simple library can be imported as a Maven dependency.\n- Create a new Eclipse project named  openweather.json-simple \n-  Configure > Convert to Maven project  the created project  \n- Open the  pom.xml  file\n- From the  Dependencies  tab, add a new dependency on  json-simple :  \n- The  pom.xml  file should look like that after the changes have been accepted:  <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>openweather</groupId>\n  <artifactId>openweather.json-simple</artifactId>\n  <version>0.0.1-SNAPSHOT</version>\n  <build>\n    <sourceDirectory>src</sourceDirectory>\n    <plugins>\n      <plugin>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.8.0</version>\n        <configuration>\n          <source>1.8</source>\n          <target>1.8</target>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n  </properties>\n  <dependencies>\n    <dependency>\n        <groupId>com.googlecode.json-simple</groupId>\n        <artifactId>json-simple</artifactId>\n        <version>1.1.1</version>\n    </dependency>\n  </dependencies>\n</project>",
            "title": "Import it as a Maven dependency"
        },
        {
            "location": "/user-guide/plugin/getting-started/#task-1-extract-the-current-weather-from-the-openweather-api",
            "text": "",
            "title": "Task 1: Extract the current weather from the OpenWeather API"
        },
        {
            "location": "/user-guide/plugin/getting-started/#look-for-json-api-stackoverflow-recommendations",
            "text": "Create a Class with its main method.   Add the code instantiating the JSON parser  package openweather.json_simple;  import org.json.simple.parser.JSONParser;  / \n * Parse Open Weather API\n  \n  /\npublic class App {\n    public static void main(String[] args) {\n        JSONParser parser = new JSONParser();\n    }\n}    Select the whole code and, in the contextual menu, ask for relevant Q&A post recommendations.     The recommendation query is based on the selected code and more particularly on:   Import Declaration: The directives used to invoke libraries  Method Declaration: Method declarations with parameters  Method Invocation: API function calls  Variable Type: Types of all declared variables  Variable Dec: All declared variables  Class Instance: Class declarations     The recommender provides related StackOverflow posts. The recommendations are useful to learn how to use the selected json-simple recommendation. A new view is displayed below the code with a list of related posts:    These posts come from a filtering (only questions with answers, only question with java code,...) and indexing of more than 18 millions StackOverFlow posts.       The provided links are:    Java. Praoblem with org.json  Reading json from java application with python  How to access JSONArray in a JSONObject using Java?  [how to convert stringified JSONObject[] to JSONObj[] in java](https://stackoverflow.com/questions/52753162/how-to-convert-stringified-jsonobject-to-jsonobj-in-java)  Value of type java.lang.String cannot be converted to JSONArray     These different posts, e.g.  How to access JSONArray in a JSONObject using Java? , provides a good example which can help understand the JSONParser core APIs:     At this point the code looks like that:  package openweather.json_simple;\n\nimport java.io.BufferedReader;\n\nimport org.json.simple.JSONArray;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\n\n/**\n * Parse Open Weather API\n */\npublic class App {\n\n    public static void main(String[] args) throws IOException, ParseException {\n        BufferedReader reader = null;\n        // Parsing of the API result\n        JSONParser parser = new JSONParser();\n        JSONObject jsonObject = (JSONObject) parser.parse(reader);\n        // Access the JSON object\n        String cityName = (String) jsonObject.get(\"name\");\n        JSONArray array = (JSONArray) jsonObject.get(\"weather\");\n    }\n}",
            "title": "Look for JSON API StackOverFlow recommendations"
        },
        {
            "location": "/user-guide/plugin/getting-started/#look-for-json-api-snippet-code-recommendations",
            "text": "If we look again for StackOverFlow recommendations to understand how to handle a JSONArray, we don\u2019t get useful post.\nSo let's try with another recommender.   Select the whole code and, in the contextual menu, ask for code recommendation.     This new feature opens a view named Code Recommendation and lists some examples of code snippets extracted from the knowledge base.   For the current code, we obtain one single code snippet. This snippet is quite useful, it shows a good example of how parsing the JSONArray      Based on this, we can conclude the parsing of the JSON result by iterating across the JSONArray:  // loop array \nJSONArray array = (JSONArray) jsonObject.get(\"weather\");\nIterator  iterator = array.iterator(); \nJSONObject weather;\nwhile (iterator.hasNext()) { \n        weather = (JSONObject)iterator.next();\n        String description = (String)weather.get(\"description\");\n        System.out.println(cityName+\": \"+description);\n    }\nFinally, at this point, our code looks like that:  package openweather.json_simple;  import java.io.BufferedReader;\nimport java.io.IOException;\nimport java.util.Iterator;  import org.json.simple.JSONArray;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\nimport org.json.simple.parser.ParseException;  /* \n * Parse Open Weather API\n  /\npublic class App {  public static void main(String[] args) throws IOException, ParseException {\n    BufferedReader reader = null;\n    // Parsing of the API result\n    JSONParser parser = new JSONParser();\n    JSONObject jsonObject = (JSONObject) parser.parse(reader);\n    // Access the JSON object\n    String cityName = (String) jsonObject.get(\"name\");\n    // loop array \n    JSONArray array = (JSONArray) jsonObject.get(\"weather\");\n    Iterator<JSONObject> iterator = array.iterator(); \n    JSONObject weather;\n    while (iterator.hasNext()) { \n        weather = (JSONObject)iterator.next();\n        String description = (String)weather.get(\"description\");\n        System.out.println(cityName+\": \"+description);\n    }\n}  }",
            "title": "Look for JSON API snippet code recommendations"
        },
        {
            "location": "/user-guide/plugin/getting-started/#access-rest-api-result",
            "text": "It\u2019s time to implement the getCityWeatherReader method accessing to the the Open Weather REST API.\n- Let\u2019s start with the creation of the REST API URL. Before calling any recommender, the code looks like that:  public class App {\n    static String OpenWeathermapAPI = \"http://api.openweathermap.org/data/2.5/weather?q={city}&appid= {appid}\";\n    static String AppId = \"abbcea2020f75409af198b98de40e3a6\";\n\n    public static void main(String[] args) throws IOException, ParseException {\n        // Creation of the REST API\n        String city = (args.length != 0)? args[0]:\"Toulouse\";\n        String restAPI = OpenWeathermapAPI.replace(\"{city}\", city);\n        restAPI = restAPI.replace(\"{appid}\", AppId);\n        URL url = new URL(restAPI);\n        // Call the  REST API\n        HttpURLConnection connection = (HttpURLConnection)url.openConnection();   Select the code for the top of the file to the openConnection call     Again, the StackOverFlow recommender doesn\u2019t provide pertinent post. On the other hand, the Code Recommender provides an interesting code snippet:    Based on this, we can complete our code. Finally the code looks like that:  package openweather.json_simple;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\nimport java.util.Iterator;\n\nimport org.json.simple.JSONArray;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\nimport org.json.simple.parser.ParseException;\n\n\n/**\n * Parse Open Weather API\n */\npublic class App {\n    static String OpenWeathermapAPI = \"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={appid}\";\n    static String AppId = \"abbcea2020f75409af198b98de40e3a6\";\n\n    public static void main(String[] args) throws IOException, ParseException {\n        // Creation of the REST API\n        String city = (args.length != 0)? args[0]:\"Toulouse\";\n        String restAPI = OpenWeathermapAPI.replace(\"{city}\", city);\n        restAPI = restAPI.replace(\"{appid}\", AppId);\n        URL url = new URL(restAPI);\n        // Call the  REST API\n        HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n        int status = connection.getResponseCode(); \n        if (status == HttpURLConnection.HTTP_OK) { \n        BufferedReader reader = null;\n        try {\n                reader = new BufferedReader(new InputStreamReader(connection.getInputStream()));\n                // Parsing of the API result\n                JSONParser parser = new JSONParser();\n                JSONObject jsonObject = (JSONObject) parser.parse(reader);\n                // Access the JSON object\n                String cityName = (String) jsonObject.get(\"name\");\n                // loop array \n                JSONArray array = (JSONArray) jsonObject.get(\"weather\");\n                Iterator<JSONObject> iterator = array.iterator(); \n                JSONObject weather;\n                while (iterator.hasNext()) { \n                    weather = (JSONObject)iterator.next();\n                    String description = (String)weather.get(\"description\");\n                    System.out.println(cityName+\": \"+description);\n                }\n        } finally {\n            if (reader != null) reader.close();\n        }\n        }\n    }\n}  If we call the application without argument, we obtain:  Toulouse: light rain  If we call it with, let's say, Rome:  Rome: light rain  and with London  London: mist  and Rio?  Rio: clear sky  At least we know where we should spend end of fall! ;-)",
            "title": "Access REST API result"
        },
        {
            "location": "/user-guide/plugin/getting-started/#task-2-migrate-from-an-abandoned-library-to-a-better-supported-one",
            "text": "Once the task is done, the teacher asks the student to migrate from JSON-simple to Jackson-core library because the former is better supported and a better performance is guaranteed.",
            "title": "Task 2: Migrate from an abandoned library to a better-supported one."
        },
        {
            "location": "/user-guide/plugin/getting-started/#similarities",
            "text": "Similar projects can be recommended from the Project search wizard. \n- Search for the json-simple library with the CROSSMINER Search project\n- Select the json-simple library in the list provided by the search dialog   The dialog offers the possibility to find similar projects using different methods:     Compound Similarity : It is a weighted combination of Readme Similarity and Dependency Similarity  CrossSim Similarity : computing similarities among all imported projects by using the star events and project dependencies;  Dependency Similarity : using the Jaccard distance on project dependencies to calculate the similarity between two projects;  CrossRec Similarity : a lightweight version of CrossSim. It uses project dependencies to provide similarities among all imported projects;  Readme Similarity : The similarity between a project p1 and a project p2 is calculated as the similarity between their readme files with the corresponding features  RepoPalCompound Similarity : being inspired by the approach presented in [];  RepoPalCompoundV2 Similarity : an evolved version of RepoPalCompound similarity;    The  RepoPalCompound  method provides, in this case, the most accurate result suggesting a set of libraries dealing with JSON. Select the library  jackson-databind .  At this point, you can either import this library as an Eclipse project (see at the previous task) or you can import it as a Maven dependency.",
            "title": "Similarities"
        },
        {
            "location": "/user-guide/plugin/getting-started/#import-it-as-an-eclipse-project_1",
            "text": "Please look at previous task.",
            "title": "Import it as an Eclipse project"
        },
        {
            "location": "/user-guide/plugin/getting-started/#import-it-as-a-maven-dependency_1",
            "text": "The GitHub readme file suggests how to import Jackson-core library as a Maven dependency (the GitHub page link is available in the details view of the project).",
            "title": "Import it as a Maven dependency"
        },
        {
            "location": "/user-guide/plugin/getting-started/#convert-the-example-against-the-new-library",
            "text": "In this particular case, we could use the readme file of this library because it provides some good examples which can be used to convert the previous example against the Jackson-core library. However, let\u2019s assume we don\u2019t have access to this information and we need to figure it out by ourselves.   Create a new Eclipse project named openweather.jackson-core  Configure > Convert to Maven project the created project  Open the pom.xml file  From the Dependencies tab, add a new dependency on  jackson-core  library:      The pom.xml file should look like that after the changes have been accepted:  \n   4.0.0 \n   openweather.jackson-core \n   openweather.jackson-core \n   0.0.1-SNAPSHOT \n   \n     src \n     \n       \n         maven-compiler-plugin \n         3.8.0 \n         \n           1.8 \n           1.8 \n         \n       \n     \n   \n   \n     UTF-8 \n   \n   \n     \n         com.fasterxml.jackson.core \n         jackson-databind \n         2.10.0 \n     \n       Create a class with its main method. Defines the list of objects that we would like to use in the import section of the class  package openweather.jackson_core;  import com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;  public class App {\n    public static void main(String[] args) {  }  }    Select the whole code and, in the contextual menu, ask for relevant Q&A post recommendations.      The first recommendation provides enough information to understand how to create and parse a JSON file using teh jackson-core library   Based on this, we can initiate our code accordingly and complete it by iterating through the ArrayNode weather  package openweather.jackson_core;\n\nimport java.io.IOException;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\n\npublic class App {\n    static String OpenWeathermapAPI = \"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={appid}\";\n    static String AppId = \"abbcea2020f75409af198b98de40e3a6\";\n\n    public static void main(String[] args) throws IOException {\n        // Creation of the REST API\n        String city = (args.length != 0)? args[0]:\"Toulouse\";\n        String restAPI = OpenWeathermapAPI.replace(\"{city}\", city);\n        restAPI = restAPI.replace(\"{appid}\", AppId);\n        URL url = new URL(restAPI);\n        // Call the  REST API\n        ObjectMapper objectMapper = new ObjectMapper();\n        JsonNode rootNode = objectMapper.readTree(url);\n        String cityName = rootNode.get(\"name\").asText();\n        ArrayNode weather = (ArrayNode)rootNode.get(\"weather\");\n        for (JsonNode jsonNode : weather) {\n            String description = jsonNode.get(\"description\").asText();\n            System.out.println(cityName+\": \"+description);\n        }\n    }\n}",
            "title": "Convert the example against the new library"
        },
        {
            "location": "/user-guide/plugin/getting-started/#other-recommendations",
            "text": "",
            "title": "Other recommendations"
        },
        {
            "location": "/user-guide/plugin/getting-started/#library-versions",
            "text": "The developer can check the library releases. Foreach imported library the system provides the list of versions.\n- Select the Eclipse project to check for library release, for example the project  openweather.jackson-core \n- From the context menu, select the item  CROSSMINER > Search updates for libraries    The menu item opens a dialog listing all the jackson-core library versions:     In our case, we already had the latest release selected, otherwise we could select a more recent one and have our pom.xml file updated.",
            "title": "Library versions"
        },
        {
            "location": "/user-guide/plugin/getting-started/#suggested-library",
            "text": "The KB assists open source software developers in selecting suitable third-party libraries. CrossRec feature aims at supporting software developers who have already included some libraries in the new projects being developed, and expect to get recommendations on which additional libraries should be further incorporated (if any).\n- Select the Eclipse project to check for library release, for example the project  openweather.jackson-core \n- From the context menu, select the item  CROSSMINER > Search libraries for this project    The menu item opens a dialog listing the libraries that may be useful in our project because they are often used by other developers:     Add, for example, the  log4j  logging library by clicking on the   button  Press the Install button  Accept the Confirmation box  The selected library is inserted into the pom.xml dependency list     Unfortunately, the  last.release  keyword is not recognized anymore by Maven 3.+. It is for this reason it is shown as an error in the  pom.xml  file       To fix it we must replace this keyword  latest.release  by the latest release version number. \nThe portal  https://mvnrepository.com/artifact/log4j/log4j  indicates the version  2.1.17      At this point, the  pom.xml  should look like that:  <?xml version=\"1.0\"?> \n   4.0.0 \n   openweather.jackson-core \n   openweather.jackson-core \n   0.0.1-SNAPSHOT \n   \n     UTF-8 \n   \n   \n     \n       com.fasterxml.jackson.core \n       jackson-databind \n       2.9.7 \n     \n     \n       log4j \n       log4j \n       1.2.17 \n     \n   \n   \n     src \n     \n       \n         maven-compiler-plugin \n         3.8.0 \n         \n           1.8 \n           1.8 \n         \n       \n     \n       Now, the developer can adapt his code to use the logging library",
            "title": "Suggested library"
        },
        {
            "location": "/user-guide/plugin/getting-started/#some-pointers",
            "text": "The implemented tasks are available at:  https://github.com/md2manoppello/CROSSMINER_Tasks",
            "title": "Some pointers"
        },
        {
            "location": "/user-guide/quickstart/",
            "text": "Quick Start Guide\n\n\n\u201cHello world example\u201d with the aim of presenting an overall picture consisting of the steps presented here https://docs.google.com/presentation/d/1mugPrIqs6JqJhMmVt4x2cjKfpSxP-wwQFOCL9aWyV5I/edit#slide=id.g3a59249d20_2_211",
            "title": "Home"
        },
        {
            "location": "/user-guide/quickstart/#quick-start-guide",
            "text": "\u201cHello world example\u201d with the aim of presenting an overall picture consisting of the steps presented here https://docs.google.com/presentation/d/1mugPrIqs6JqJhMmVt4x2cjKfpSxP-wwQFOCL9aWyV5I/edit#slide=id.g3a59249d20_2_211",
            "title": "Quick Start Guide"
        },
        {
            "location": "/user-guide/readers/",
            "text": "Readers\n\n\nThis document describes the readers used to provide data for the Scava platform.\n\n\n\n\nBug Trackers\n\n\nCommunication Channels\n\n\nDocumentation\n\n\n\n\nBug Trackers\n\n\nThe following readers retrieve data from bug tracking systems.\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.bitbucket\n\n\nThis reader retrieves data from Bitbucket through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://bitbucket.org/fenics-project/dolfin\n\n\nYes\n\n\n\n\n\n\nLogin\n\n\nString\n\n\nUsername used to log into Bitbucket\n\n\nadmin\n\n\nYes\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nPassword used to log into Bitbucket\n\n\nadmin101\n\n\nYes\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.bugzilla\n\n\nThis reader retrieves data from Bugzilla through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://bugzilla.redhat.com/xmlrpc.cgi\n\n\nYes\n\n\n\n\n\n\nProduct\n\n\nString\n\n\nAn existing project in Bugzilla\n\n\nRed Hat Enterprise Linux 8\n\n\nYes\n\n\n\n\n\n\nComponent\n\n\nString\n\n\nAn existing component of the project\n\n\n389-ds-base\n\n\nYes\n\n\n\n\n\n\nProject\n\n\nString\n\n\nThe Project name\n\n\nBugzilla\n\n\nYes\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.github\n\n\nThis reader retrieves data from GitHub through a REST API.  To register a project on the platform the user must use the \nImport Project\n option and provide the following parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://github.com/Adrian-Berrigan/adriantest\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.gitlab\n\n\nThis reader retrieves data from GitLab through a REST API.  To register a project on the platform the user must use the \nImport Project\n option and provide the following parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://github.com/Adrian-Berrigan/adriantest\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.jira\n\n\nThis reader retrieves data from Jira through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://jira.xwiki.org\n\n\nYes\n\n\n\n\n\n\nLogin\n\n\nString\n\n\nUsername used to log into Jira\n\n\nadmin\n\n\nNo\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nPassword used to log into Jira\n\n\nadmin101\n\n\nNo\n\n\n\n\n\n\nProject\n\n\nString\n\n\nName of the project\n\n\nXWIKI\n\n\nYes\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.mantis\n\n\nThis reader retrieves data from Mantis through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://www.mantisbt.org/\n\n\n\n\n\n\nToken\n\n\nString\n\n\nA unique token used to make authentication request\n\n\n6aefdre554675bfgtrhgfy77567\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nTo retrieve issues with \nCLOSED\n status from Mantis, the user must ensure that the \n$g_hide_status_default\n filter parameter located in the config file \n.../config/config_inc.php\n is set to \nMETA_FILTER_NONE\n. It seems this parameter is set to \nCLOSED\n by default as discussed in \nthis\n forum post.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.redmine\n\n\nThis reader retrieves data from Redmine through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttp://forge.modelio.org\n\n\n\n\n\n\nName\n\n\nString\n\n\nName of the user\n\n\nDave\n\n\n\n\n\n\nProject\n\n\nString\n\n\nName of the project\n\n\nintocps\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.bugtrackingsystem.sourceforge\n\n\nThis reader retrieves data from SOURCEFORGE through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the project repository\n\n\nhttps://sourceforge.net/rest/p/vice-emu/bugs\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nCommunication Channels\n\n\nThe following readers retrieve data from communication channels.\n\n\n\n\norg.eclipse.scava.platform.communicationchannel.eclipseforums\n\n\nThis reader retrieves data from Eclipse Forums through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nForum Id\n\n\nString\n\n\nThe unique ID of the eclipse forum\n\n\n305\n\n\nYes\n\n\n\n\n\n\nForum Name\n\n\nString\n\n\nName of the forum\n\n\nAndmore\n\n\nYes\n\n\n\n\n\n\nClient Id\n\n\nString\n\n\nUnique ID provided by eclipse for authenticated access\n\n\nG12DrqtW86745tTre65476\n\n\nNo\n\n\n\n\n\n\nClient Secret\n\n\nString\n\n\nAccess token provided by eclipse for authenticated access\n\n\nwp199NB564Frt5R43Ghy87\n\n\nNo\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThe \nForum Id\n can be found within the forum URL. For example, the \nAndmore\n forum URL is https://www.eclipse.org/forums/index.php/f/305/ and the \nForum Id\n is \n305\n shown in the last directory of the path.\n\n\nEclipse enforces a rate limit of 1,000 requests per hour for both authenticated and anonymous requests. However, authenticated users may request to increase the call limit, subject to Eclipse approval.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.communicationchannel.irc\n\n\nThe Irc reader supports data retrieval from log archive. To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nThe URL location of the archive\n\n\nhttp://localhost/Downloads/\n\n\nYes\n\n\n\n\n\n\nName\n\n\nString\n\n\nName of the Irc\n\n\nkubuntu\n\n\nYes\n\n\n\n\n\n\nDescription\n\n\nString\n\n\nBrief description of the Irc\n\n\nThe kubuntu archive contains ....\n\n\nYes\n\n\n\n\n\n\nCompressed File Ext.\n\n\nString\n\n\nFile extension of the archive\n\n\ntar.gzip\n\n\nYes\n\n\n\n\n\n\nUsername\n\n\nString\n\n\nUsername (for protected archive)\n\n\nadmin\n\n\nNo\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nPassword (for protected archive)\n\n\nadmin101\n\n\nNo\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThe reader currently supports \"tarballs\" (i.e., tar.gzip, tgz).\n\n\nEach archive must be stored in the following name format \nIrcName-yyyymmdd.ext\n , to specify the date of analysis (e.g., \nkubuntu-20040306.tar\n). The archive may contain one or more files stored in a folder, and each file represents interaction within a single chat room.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.communicationchannel.mbox\n\n\nThe Mbox reader uses dumps to parse the data. To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nThe URL location of the archive\n\n\nhttps://mail.gnome.org/archives/gtk-list/\n\n\nYes\n\n\n\n\n\n\nName\n\n\nString\n\n\nName of the Mbox\n\n\nmboxes\n\n\nNo\n\n\n\n\n\n\nDescription\n\n\nString\n\n\nBrief description of the Mbox\n\n\nThe mboxes archive contains\n\n\nNo\n\n\n\n\n\n\nCompressed File Ext.\n\n\nString\n\n\nFile extension of the archive\n\n\ntar.gzip\n\n\nYes\n\n\n\n\n\n\nUsername\n\n\nString\n\n\nUsername (for protected archive)\n\n\nadmin\n\n\nNo\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nPassword (for protected archive)\n\n\nadmin101\n\n\nNo\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThe reader supports compressed tarballs (i.e., \n.tar.gzip\n, \n.tgz\n) and specific compressed gz files (\n.txt.gz\n and \n.mbox.gz\n).\n\n\nIf the format is \n.tar.gzip\n, \n.tgz\n or \n.txt.gz\n the platform supports month based files, following this file format name convention: \nYYYY-Month.ext\n, e.g. \n2019-Janaury.txt.gz\n.\nThis is similar to the one used in https://mail.gnome.org/archives/gtk-list/\n\n\nIf the format is \n.mbox.gz\n, the platform supports one file containing all the emails from the mailing list. In this case the URL must contain the name of the mailing list, like in:\nhttps://download.eclipse.org/scava/datasets/eclipse_mls/mboxes/actf-dev . This follows the format used in https://download.eclipse.org/scava/datasets/eclipse_mls/eclipse_mls.html to store dumps of mboxes.\n\n\n\n\n\n\nThe reader will manage intelligently the dumps and will store, as a temporal file, only one file. However, the current implementation have some particular characteristics and limitations:\n\n\nIf the format is month-year based, the platform will download the files when the month or year of the delta changes. Do not analyze a mailing list that has not generated yet a dump, as the platform is not aware of the date when the dumps are updated. For example, if we are in December 2019, do not ask for analyzing December 2019 until the dump has been made. Otherwise, the platform will create empty deltas and the analysis will not be accurate.\n\n\nIf the format is \n.mbox.gz\n which contains all the emails sent, the dump will update the files only if the platform need to analyze a delta that the dump does not cover. This is done by calculating when was the last update of the dump; the calculation is based on the fact that Eclipse Mailing lists are updated each Sunday at 2am. The platform will pause until Monday and then get a dump that covers the expected date.\n\n\n\n\n\n\nCurrently, the reader does not support a user-defined time for updating dumps.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.communicationchannel.nntp\n\n\nThis reader retrieves data from NNTP through a REST API.  To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the project repository\n\n\nnews.mozilla.org\n\n\nYes\n\n\n\n\n\n\nNewsGroupName\n\n\nString\n\n\nThe newsgroup name\n\n\nmozilla.activity-stream\n\n\nYes\n\n\n\n\n\n\nUsername\n\n\nString\n\n\nUsername used to log into the newsgroup channel\n\n\nadmin\n\n\nNo\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nPassword used to log into the newsgroup channel\n\n\nadmin101\n\n\nNo\n\n\n\n\n\n\nPort\n\n\nint\n\n\nThe port number\n\n\n119 by default\n\n\nYes\n\n\n\n\n\n\nInterval\n\n\nint\n\n\nThe frequency of calls\n\n\n10000 by default\n\n\nNo\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThe port number is mandatory and defaults to 119. However, the user has the option to change this value, if their port number is different from 119.\n\n\nThe Interval is not mandatory but defaults to 10000. However, the user has the option to lower or increase this value.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.communicationchannel.sympa\n\n\nThe Sympa reader supports data retrieval from log archive. To register a project on the platform the user must use the \nCreate Project\n option and provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nThe URL location of the archive\n\n\nhttp://localhost/Downloads/\n\n\nYes\n\n\n\n\n\n\nName\n\n\nString\n\n\nName of the Sympa\n\n\nsympa\n\n\nYes\n\n\n\n\n\n\nDescription\n\n\nString\n\n\nBrief description of the Sympa\n\n\nThe sympa archive contains ....\n\n\nYes\n\n\n\n\n\n\nCompressed File Ext.\n\n\nString\n\n\nFile extension of the archive\n\n\ntar.gzip\n\n\nYes\n\n\n\n\n\n\nUsername\n\n\nString\n\n\nUsername (for protected archive)\n\n\nadmin\n\n\nNo\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nPassword (for protected archive)\n\n\nadmin101\n\n\nNo\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThe reader currently supports \"tarballs\" (i.e., tar.gzip, tgz).\n\n\nThe storage location may contain one or more Sympa archive(s), each stored in the following name format \nSympaName-yyyymmdd.ext\n , to specify the date of analysis. Each Sympa archive must contain one or more email messages (of the same analysis date), stored in a folder. \n\n\n\n\nBack to top\n\n\nDocumentation\n\n\nThe following readers retrieve project documentation from relevant sources.\n\n\n\n\norg.eclipse.scava.platform.documentation.gitbased\n\n\nThis reader is Git-Based and thus retrieves data through a REST API.  To register a project on the platform the user must use the \nDocumentation Git-Based\n option and provide the following parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://github.com/linhr/pongo-pongo/wiki\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nThis reader cannot be used along with a Git project, because internally they use the same model and the \ndocumentation.Git-Based\n reader extends the Git project reader.\n\n\n\n\nBack to top\n\n\n\n\norg.eclipse.scava.platform.documentation.systematic\n\n\nThis reader uses web crawler to retrieve data from websites.  To register a project on the platform the user must use the \nDocumentation Systematic\n option and provide the relevant parameter:\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nURL\n\n\nString\n\n\nURL of the repository\n\n\nhttps://wiki.eclipse.org/Trace_Compass#User_Guides\n\n\n\n\n\n\nExecutionFrequency*\n\n\nint\n\n\nCrawling frequency defined in days, e.g., 1 represents a day\n\n\n1\n\n\n\n\n\n\nUserName\n\n\nString\n\n\nThe \nusername\n used to log into the website\n\n\nname@domain.com\n\n\n\n\n\n\nPassword\n\n\nString\n\n\nThe \npassword\n used to log into the website\n\n\np$%7876\n\n\n\n\n\n\nLoginURL\n\n\nString\n\n\nThe URL of the login page\n\n\nhttps://accounts.eclipse.org/user/login?destination=user/login%3Ftakemeback%3Dhttp%253A//www.eclipse.org/forums/index.php%253Ft%253Dlogin\n\n\n\n\n\n\nUsernameFieldName\n\n\nString\n\n\nThe name used to define the \nusername\n field on the website\n\n\nname\n\n\n\n\n\n\nPasswordFieldName\n\n\nString\n\n\nThe name used to define the \npassword\n field on the website\n\n\npass\n\n\n\n\n\n\n\n\nAdditional Information\n :\n\n\n\n\nExecutionFrequency. By default the value is zero, which means that the documentation will be retrieved only once at the beginning of the analysis. Furthermore, we recommend a execution frequency greater than 5 in order to prevent the crawler to be banned from accessing the data.\n\n\nFor clarity, all examples used in the above table are derived from the Eclipse Foundation website. For example, the \nusername\n field is defined as name.\n\n\nThe \nDocumentation Systematic\n reader supports both password protected and un-protected sources. \nURL\n and \nExecutionFrequency\n are mandatory for both sources. However, only the password protected sources require the additional paramenters.\n\n\nDates preceeding the current date are processed once because the reader does not keep track of the website evolution up to the current date.\n\n\nThe reader assumes that all textual information within the specified URL location are related to documentation.\n\n\nThe crawler is configured to comply with the Robots Exclusion protocol and thus follows the rules in the robots.txt file during crawling.\n\n\nThe crawler does not support websites that are based mostly on JavaScript, such as websites based on AngularJS. The reason behind is that the crawler cannot execute JavaScript.\n\n\n\n\nBack to top",
            "title": "Home"
        },
        {
            "location": "/user-guide/readers/#readers",
            "text": "This document describes the readers used to provide data for the Scava platform.   Bug Trackers  Communication Channels  Documentation",
            "title": "Readers"
        },
        {
            "location": "/user-guide/readers/#bug-trackers",
            "text": "The following readers retrieve data from bug tracking systems.",
            "title": "Bug Trackers"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystembitbucket",
            "text": "This reader retrieves data from Bitbucket through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example  Mandatory      URL  String  URL of the repository  https://bitbucket.org/fenics-project/dolfin  Yes    Login  String  Username used to log into Bitbucket  admin  Yes    Password  String  Password used to log into Bitbucket  admin101  Yes     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.bitbucket"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystembugzilla",
            "text": "This reader retrieves data from Bugzilla through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example  Mandatory      URL  String  URL of the repository  https://bugzilla.redhat.com/xmlrpc.cgi  Yes    Product  String  An existing project in Bugzilla  Red Hat Enterprise Linux 8  Yes    Component  String  An existing component of the project  389-ds-base  Yes    Project  String  The Project name  Bugzilla  Yes     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.bugzilla"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystemgithub",
            "text": "This reader retrieves data from GitHub through a REST API.  To register a project on the platform the user must use the  Import Project  option and provide the following parameter:     Parameter  Type  Description  Example      URL  String  URL of the repository  https://github.com/Adrian-Berrigan/adriantest     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.github"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystemgitlab",
            "text": "This reader retrieves data from GitLab through a REST API.  To register a project on the platform the user must use the  Import Project  option and provide the following parameter:     Parameter  Type  Description  Example      URL  String  URL of the repository  https://github.com/Adrian-Berrigan/adriantest     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.gitlab"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystemjira",
            "text": "This reader retrieves data from Jira through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example  Mandatory      URL  String  URL of the repository  https://jira.xwiki.org  Yes    Login  String  Username used to log into Jira  admin  No    Password  String  Password used to log into Jira  admin101  No    Project  String  Name of the project  XWIKI  Yes     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.jira"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystemmantis",
            "text": "This reader retrieves data from Mantis through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example      URL  String  URL of the repository  https://www.mantisbt.org/    Token  String  A unique token used to make authentication request  6aefdre554675bfgtrhgfy77567     Additional Information  :   To retrieve issues with  CLOSED  status from Mantis, the user must ensure that the  $g_hide_status_default  filter parameter located in the config file  .../config/config_inc.php  is set to  META_FILTER_NONE . It seems this parameter is set to  CLOSED  by default as discussed in  this  forum post.   Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.mantis"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystemredmine",
            "text": "This reader retrieves data from Redmine through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example      URL  String  URL of the repository  http://forge.modelio.org    Name  String  Name of the user  Dave    Project  String  Name of the project  intocps     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.redmine"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformbugtrackingsystemsourceforge",
            "text": "This reader retrieves data from SOURCEFORGE through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameter:     Parameter  Type  Description  Example      URL  String  URL of the project repository  https://sourceforge.net/rest/p/vice-emu/bugs     Back to top",
            "title": "org.eclipse.scava.platform.bugtrackingsystem.sourceforge"
        },
        {
            "location": "/user-guide/readers/#communication-channels",
            "text": "The following readers retrieve data from communication channels.",
            "title": "Communication Channels"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformcommunicationchanneleclipseforums",
            "text": "This reader retrieves data from Eclipse Forums through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameter:     Parameter  Type  Description  Example  Mandatory      Forum Id  String  The unique ID of the eclipse forum  305  Yes    Forum Name  String  Name of the forum  Andmore  Yes    Client Id  String  Unique ID provided by eclipse for authenticated access  G12DrqtW86745tTre65476  No    Client Secret  String  Access token provided by eclipse for authenticated access  wp199NB564Frt5R43Ghy87  No     Additional Information  :   The  Forum Id  can be found within the forum URL. For example, the  Andmore  forum URL is https://www.eclipse.org/forums/index.php/f/305/ and the  Forum Id  is  305  shown in the last directory of the path.  Eclipse enforces a rate limit of 1,000 requests per hour for both authenticated and anonymous requests. However, authenticated users may request to increase the call limit, subject to Eclipse approval.   Back to top",
            "title": "org.eclipse.scava.platform.communicationchannel.eclipseforums"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformcommunicationchannelirc",
            "text": "The Irc reader supports data retrieval from log archive. To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example  Mandatory      URL  String  The URL location of the archive  http://localhost/Downloads/  Yes    Name  String  Name of the Irc  kubuntu  Yes    Description  String  Brief description of the Irc  The kubuntu archive contains ....  Yes    Compressed File Ext.  String  File extension of the archive  tar.gzip  Yes    Username  String  Username (for protected archive)  admin  No    Password  String  Password (for protected archive)  admin101  No     Additional Information  :   The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz).  Each archive must be stored in the following name format  IrcName-yyyymmdd.ext  , to specify the date of analysis (e.g.,  kubuntu-20040306.tar ). The archive may contain one or more files stored in a folder, and each file represents interaction within a single chat room.   Back to top",
            "title": "org.eclipse.scava.platform.communicationchannel.irc"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformcommunicationchannelmbox",
            "text": "The Mbox reader uses dumps to parse the data. To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example  Mandatory      URL  String  The URL location of the archive  https://mail.gnome.org/archives/gtk-list/  Yes    Name  String  Name of the Mbox  mboxes  No    Description  String  Brief description of the Mbox  The mboxes archive contains  No    Compressed File Ext.  String  File extension of the archive  tar.gzip  Yes    Username  String  Username (for protected archive)  admin  No    Password  String  Password (for protected archive)  admin101  No     Additional Information  :   The reader supports compressed tarballs (i.e.,  .tar.gzip ,  .tgz ) and specific compressed gz files ( .txt.gz  and  .mbox.gz ).  If the format is  .tar.gzip ,  .tgz  or  .txt.gz  the platform supports month based files, following this file format name convention:  YYYY-Month.ext , e.g.  2019-Janaury.txt.gz .\nThis is similar to the one used in https://mail.gnome.org/archives/gtk-list/  If the format is  .mbox.gz , the platform supports one file containing all the emails from the mailing list. In this case the URL must contain the name of the mailing list, like in:\nhttps://download.eclipse.org/scava/datasets/eclipse_mls/mboxes/actf-dev . This follows the format used in https://download.eclipse.org/scava/datasets/eclipse_mls/eclipse_mls.html to store dumps of mboxes.    The reader will manage intelligently the dumps and will store, as a temporal file, only one file. However, the current implementation have some particular characteristics and limitations:  If the format is month-year based, the platform will download the files when the month or year of the delta changes. Do not analyze a mailing list that has not generated yet a dump, as the platform is not aware of the date when the dumps are updated. For example, if we are in December 2019, do not ask for analyzing December 2019 until the dump has been made. Otherwise, the platform will create empty deltas and the analysis will not be accurate.  If the format is  .mbox.gz  which contains all the emails sent, the dump will update the files only if the platform need to analyze a delta that the dump does not cover. This is done by calculating when was the last update of the dump; the calculation is based on the fact that Eclipse Mailing lists are updated each Sunday at 2am. The platform will pause until Monday and then get a dump that covers the expected date.    Currently, the reader does not support a user-defined time for updating dumps.   Back to top",
            "title": "org.eclipse.scava.platform.communicationchannel.mbox"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformcommunicationchannelnntp",
            "text": "This reader retrieves data from NNTP through a REST API.  To register a project on the platform the user must use the  Create Project  option and provide the following parameter:     Parameter  Type  Description  Example  Mandatory      URL  String  URL of the project repository  news.mozilla.org  Yes    NewsGroupName  String  The newsgroup name  mozilla.activity-stream  Yes    Username  String  Username used to log into the newsgroup channel  admin  No    Password  String  Password used to log into the newsgroup channel  admin101  No    Port  int  The port number  119 by default  Yes    Interval  int  The frequency of calls  10000 by default  No     Additional Information  :   The port number is mandatory and defaults to 119. However, the user has the option to change this value, if their port number is different from 119.  The Interval is not mandatory but defaults to 10000. However, the user has the option to lower or increase this value.   Back to top",
            "title": "org.eclipse.scava.platform.communicationchannel.nntp"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformcommunicationchannelsympa",
            "text": "The Sympa reader supports data retrieval from log archive. To register a project on the platform the user must use the  Create Project  option and provide the following parameters:     Parameter  Type  Description  Example  Mandatory      URL  String  The URL location of the archive  http://localhost/Downloads/  Yes    Name  String  Name of the Sympa  sympa  Yes    Description  String  Brief description of the Sympa  The sympa archive contains ....  Yes    Compressed File Ext.  String  File extension of the archive  tar.gzip  Yes    Username  String  Username (for protected archive)  admin  No    Password  String  Password (for protected archive)  admin101  No     Additional Information  :   The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz).  The storage location may contain one or more Sympa archive(s), each stored in the following name format  SympaName-yyyymmdd.ext  , to specify the date of analysis. Each Sympa archive must contain one or more email messages (of the same analysis date), stored in a folder.    Back to top",
            "title": "org.eclipse.scava.platform.communicationchannel.sympa"
        },
        {
            "location": "/user-guide/readers/#documentation",
            "text": "The following readers retrieve project documentation from relevant sources.",
            "title": "Documentation"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformdocumentationgitbased",
            "text": "This reader is Git-Based and thus retrieves data through a REST API.  To register a project on the platform the user must use the  Documentation Git-Based  option and provide the following parameter:     Parameter  Type  Description  Example      URL  String  URL of the repository  https://github.com/linhr/pongo-pongo/wiki     Additional Information  :   This reader cannot be used along with a Git project, because internally they use the same model and the  documentation.Git-Based  reader extends the Git project reader.   Back to top",
            "title": "org.eclipse.scava.platform.documentation.gitbased"
        },
        {
            "location": "/user-guide/readers/#orgeclipsescavaplatformdocumentationsystematic",
            "text": "This reader uses web crawler to retrieve data from websites.  To register a project on the platform the user must use the  Documentation Systematic  option and provide the relevant parameter:     Parameter  Type  Description  Example      URL  String  URL of the repository  https://wiki.eclipse.org/Trace_Compass#User_Guides    ExecutionFrequency*  int  Crawling frequency defined in days, e.g., 1 represents a day  1    UserName  String  The  username  used to log into the website  name@domain.com    Password  String  The  password  used to log into the website  p$%7876    LoginURL  String  The URL of the login page  https://accounts.eclipse.org/user/login?destination=user/login%3Ftakemeback%3Dhttp%253A//www.eclipse.org/forums/index.php%253Ft%253Dlogin    UsernameFieldName  String  The name used to define the  username  field on the website  name    PasswordFieldName  String  The name used to define the  password  field on the website  pass     Additional Information  :   ExecutionFrequency. By default the value is zero, which means that the documentation will be retrieved only once at the beginning of the analysis. Furthermore, we recommend a execution frequency greater than 5 in order to prevent the crawler to be banned from accessing the data.  For clarity, all examples used in the above table are derived from the Eclipse Foundation website. For example, the  username  field is defined as name.  The  Documentation Systematic  reader supports both password protected and un-protected sources.  URL  and  ExecutionFrequency  are mandatory for both sources. However, only the password protected sources require the additional paramenters.  Dates preceeding the current date are processed once because the reader does not keep track of the website evolution up to the current date.  The reader assumes that all textual information within the specified URL location are related to documentation.  The crawler is configured to comply with the Robots Exclusion protocol and thus follows the rules in the robots.txt file during crawling.  The crawler does not support websites that are based mostly on JavaScript, such as websites based on AngularJS. The reason behind is that the crawler cannot execute JavaScript.   Back to top",
            "title": "org.eclipse.scava.platform.documentation.systematic"
        },
        {
            "location": "/user-guide/workflow/",
            "text": "Workflow Engine\n\n\n\n\nCrossflow is a distributed data processing framework that supports dispensation of work across multiple opinionated and low-commitment workers.\n\n\nDocker Quick Start\n\n\nPull container image from Docker Hub:\n\n\ndocker pull crossminer/crossflow\n\n\nStartup container:\n\n\ndocker run -it --rm -d --name crossflow -p 80:8080 -p 61616:61616 -p 61614:61614 -p 5672:5672 -p 61613:61613 -p 1883:1883 -p 8161:8161 -p 1099:1099 crossminer/crossflow:latest\n\n\nAccess Crossflow web application:\nhttp://localhost/org.eclipse.scava.crossflow.web/\n\n\nMore details on running Crossflow with Docker are available \nhere\n.\n\n\nRunning from source\n\n\nTo run Crossflow from source you will need Eclipse, Apache Tomcat and Apache Thrift. Brief instructions are provided below.\n\n\nEclipse\n\n\n\n\nStart with a J2EE distribution from https://www.eclipse.org/downloads/packages/release/2018-09/r/eclipse-ide-java-ee-developers\n\n\nInstall Emfatic from http://download.eclipse.org/emfatic/update/ (Untick the \"Group items by category\" check box)\n\n\nInstall the Graphical Modelling Framework (GMF) Tooling SDK from http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/\n\n\nInstall the following features from http://download.eclipse.org/epsilon/interim/\n\n\nEpsilon Core\n\n\nEpsilon Core Develoment Tools\n\n\nEpsilon EMF Integration\n\n\nEpsilon GMF Integration\n\n\n\n\n\n\nInstall Web Tools Platform SDK (WTP SDK) from http://download.eclipse.org/webtools/repository/photon\n\n\n\n\nTomcat\n\n\n\n\nDownload a copy of Tomcat from http://archive.apache.org/dist/tomcat/tomcat-9/v9.0.14/bin/apache-tomcat-9.0.14.zip\n\n\nSet up Tomcat in your Eclipse through the Servers view\n\n\n\n\nThrift\n\n\n\n\nInstall Apache Thrift (http://thrift.apache.org/)\n\n\nStandalone executable for Windows\n\n\nHomebrew for Mac\n\n\n\n\n\n\n\n\nGit\n\n\n\n\nClone the https://github.com/crossminer/scava/ repository\n\n\nSwitch to the crossflow branch\n\n\nImport all projects from the crossflow and the restmule folders\n\n\n\n\nIvy\n\n\nWe're using Apache Ivy for dependency management (i.e. so that we don't need to store jars in the repo)\n- Install the Ivy Eclipse plugin: http://www.apache.org/dist/ant/ivyde/updatesite\n- If Ivy doesn't run automatically, look for any projects that contain an ivy.xml, right-click and select Ivy -> Resolve\n\n\nGenerating stuff\n\n\nYou will need to run the ANT build-files below to generate stuff after you import all the crossflow and restmule projects.\n\n\n\n\norg.eclipse.scava.crossflow.tests/generate-all-tests.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.tests/models\n\n\norg.eclipse.scava.crossflow.web/run-thrift.xml runs the Thrift code generator against crossflow.thrift to produce Java and JavaScript source code\n\n\norg.eclipse.scava.crossflow.web/build-war.xml builds a Tomcat WAR file from org.eclipse.scava.crossflow.web\n\n\norg.eclipse.scava.crossflow.examples/generate-all-examples.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.examples/models\n\n\n\n\nTests\n\n\n\n\nJUnit tests can be ran through the CrossflowTests class in org.eclipse.scava.crossflow.tests\n\n\n\n\nWeb application\n\n\n\n\nTo run the web application (port: 8080) right-click on org.eclipse.scava.crossflow.web and select Run as -> Run on Server\n\n\nThe web app should be running on http://localhost:8080/org.eclipse.scava.crossflow.web/\n\n\n\n\nScreenshots\n\n\n\n\nFigure\n: Main page listing available workflows and \nUpload New Workflow\n button.\n\n\n\n\nFigure\n: Calculator experiment page \nAdvanced\n tab listing Calculator workflow configuration.\n\n\n\n\nFigure\n: Calculator experiment page \nCalculations\n tab listing Calculator workflow input calculations obtained from CSV source.\n\n\n\n\nFigure\n: Calculator experiment page \nModel\n tab listing Calculator workflow model.\n\n\n\n\nFigure\n: Calculator experiment page \nLog\n tab listing Calculator workflow log after experiment completion.\n\n\n\n\nFigure\n: Word Count experiment page \nModel\n tab listing Word Count workflow model before execution.\n\n\n\n\nFigure\n: Word Count experiment page \nModel\n tab listing Word Count workflow model during execution visualizing task status and queue size by means of color and rounded number, respectively. \nTask status (color)\n: STARTED (lightcyan), WAITING (skyblue), INPROGRESS (palegreen), BLOCKED (salmon), and FINISHED (slategrey).  \n\n\n\n\nFigure\n: Word Count experiment page \nModel\n tab listing Word Count workflow model during execution with mouse hovering over initial queue depicting (queue) size, in-flight count, and subscriber count.\n\n\n\n\nFigure\n: Word Count experiment page \nModel\n tab listing Word Count workflow model during execution with mouse click inside empty model area, i.e. not on a particular task or queue, displaying context menu popup to clear the cache of all queues involved in the Word Count workflow.\n\n\n\n\nFigure\n: Word Count experiment page \nModel\n tab listing Word Count workflow model during execution with mouse click inside boundaries of \nWordFrequencies\n queue displaying context menu popup to clear the cache of all queues involved in the Word Count workflow.\n\n\n\n\nFigure\n: Upload New Workflow page allowing the upload and deployment of new experiments.",
            "title": "Home"
        },
        {
            "location": "/user-guide/workflow/#workflow-engine",
            "text": "Crossflow is a distributed data processing framework that supports dispensation of work across multiple opinionated and low-commitment workers.",
            "title": "Workflow Engine"
        },
        {
            "location": "/user-guide/workflow/#docker-quick-start",
            "text": "Pull container image from Docker Hub:  docker pull crossminer/crossflow  Startup container:  docker run -it --rm -d --name crossflow -p 80:8080 -p 61616:61616 -p 61614:61614 -p 5672:5672 -p 61613:61613 -p 1883:1883 -p 8161:8161 -p 1099:1099 crossminer/crossflow:latest  Access Crossflow web application:\nhttp://localhost/org.eclipse.scava.crossflow.web/  More details on running Crossflow with Docker are available  here .",
            "title": "Docker Quick Start"
        },
        {
            "location": "/user-guide/workflow/#running-from-source",
            "text": "To run Crossflow from source you will need Eclipse, Apache Tomcat and Apache Thrift. Brief instructions are provided below.",
            "title": "Running from source"
        },
        {
            "location": "/user-guide/workflow/#eclipse",
            "text": "Start with a J2EE distribution from https://www.eclipse.org/downloads/packages/release/2018-09/r/eclipse-ide-java-ee-developers  Install Emfatic from http://download.eclipse.org/emfatic/update/ (Untick the \"Group items by category\" check box)  Install the Graphical Modelling Framework (GMF) Tooling SDK from http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/  Install the following features from http://download.eclipse.org/epsilon/interim/  Epsilon Core  Epsilon Core Develoment Tools  Epsilon EMF Integration  Epsilon GMF Integration    Install Web Tools Platform SDK (WTP SDK) from http://download.eclipse.org/webtools/repository/photon",
            "title": "Eclipse"
        },
        {
            "location": "/user-guide/workflow/#tomcat",
            "text": "Download a copy of Tomcat from http://archive.apache.org/dist/tomcat/tomcat-9/v9.0.14/bin/apache-tomcat-9.0.14.zip  Set up Tomcat in your Eclipse through the Servers view",
            "title": "Tomcat"
        },
        {
            "location": "/user-guide/workflow/#thrift",
            "text": "Install Apache Thrift (http://thrift.apache.org/)  Standalone executable for Windows  Homebrew for Mac",
            "title": "Thrift"
        },
        {
            "location": "/user-guide/workflow/#git",
            "text": "Clone the https://github.com/crossminer/scava/ repository  Switch to the crossflow branch  Import all projects from the crossflow and the restmule folders",
            "title": "Git"
        },
        {
            "location": "/user-guide/workflow/#ivy",
            "text": "We're using Apache Ivy for dependency management (i.e. so that we don't need to store jars in the repo)\n- Install the Ivy Eclipse plugin: http://www.apache.org/dist/ant/ivyde/updatesite\n- If Ivy doesn't run automatically, look for any projects that contain an ivy.xml, right-click and select Ivy -> Resolve",
            "title": "Ivy"
        },
        {
            "location": "/user-guide/workflow/#generating-stuff",
            "text": "You will need to run the ANT build-files below to generate stuff after you import all the crossflow and restmule projects.   org.eclipse.scava.crossflow.tests/generate-all-tests.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.tests/models  org.eclipse.scava.crossflow.web/run-thrift.xml runs the Thrift code generator against crossflow.thrift to produce Java and JavaScript source code  org.eclipse.scava.crossflow.web/build-war.xml builds a Tomcat WAR file from org.eclipse.scava.crossflow.web  org.eclipse.scava.crossflow.examples/generate-all-examples.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.examples/models",
            "title": "Generating stuff"
        },
        {
            "location": "/user-guide/workflow/#tests",
            "text": "JUnit tests can be ran through the CrossflowTests class in org.eclipse.scava.crossflow.tests",
            "title": "Tests"
        },
        {
            "location": "/user-guide/workflow/#web-application",
            "text": "To run the web application (port: 8080) right-click on org.eclipse.scava.crossflow.web and select Run as -> Run on Server  The web app should be running on http://localhost:8080/org.eclipse.scava.crossflow.web/",
            "title": "Web application"
        },
        {
            "location": "/user-guide/workflow/#screenshots",
            "text": "Figure : Main page listing available workflows and  Upload New Workflow  button.   Figure : Calculator experiment page  Advanced  tab listing Calculator workflow configuration.   Figure : Calculator experiment page  Calculations  tab listing Calculator workflow input calculations obtained from CSV source.   Figure : Calculator experiment page  Model  tab listing Calculator workflow model.   Figure : Calculator experiment page  Log  tab listing Calculator workflow log after experiment completion.   Figure : Word Count experiment page  Model  tab listing Word Count workflow model before execution.   Figure : Word Count experiment page  Model  tab listing Word Count workflow model during execution visualizing task status and queue size by means of color and rounded number, respectively.  Task status (color) : STARTED (lightcyan), WAITING (skyblue), INPROGRESS (palegreen), BLOCKED (salmon), and FINISHED (slategrey).     Figure : Word Count experiment page  Model  tab listing Word Count workflow model during execution with mouse hovering over initial queue depicting (queue) size, in-flight count, and subscriber count.   Figure : Word Count experiment page  Model  tab listing Word Count workflow model during execution with mouse click inside empty model area, i.e. not on a particular task or queue, displaying context menu popup to clear the cache of all queues involved in the Word Count workflow.   Figure : Word Count experiment page  Model  tab listing Word Count workflow model during execution with mouse click inside boundaries of  WordFrequencies  queue displaying context menu popup to clear the cache of all queues involved in the Word Count workflow.   Figure : Upload New Workflow page allowing the upload and deployment of new experiments.",
            "title": "Screenshots"
        }
    ]
}